{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_audio_classifier_example_for_pico.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m174xspUl7LC"
      },
      "source": [
        "# ML Audio Classifier Example for Pico\n",
        "\n",
        "```\n",
        "Copyright (c) 2021 Arm Limited and Contributors. All rights reserved.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "```\n",
        "\n",
        "Authors: [Sandeep Mistry](https://twitter.com/sandeepmistry), [Henri Woodcock](https://twitter.com/henriwoodcock) from the [Arm Software Developers team](https://twitter.com/armsoftwaredev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h5zXsutmdCS"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This tutorial will guide you through how to train a TensorFlow based audio classification Machine Learning (ML) model to detect a fire alarm sound. We’ll show you how to use [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers) with Arm [CMSIS-NN](https://arm-software.github.io/CMSIS_5/NN/html/index.html) accelerated kernels to deploy the ML model to an [Arm Cortex-M0+](https://developer.arm.com/ip-products/processors/cortex-m/cortex-m0-plus) based microcontroller (MCU) board for local on-device ML interferencing. Arm’s [CMSIS-DSP](https://arm-software.github.io/CMSIS_5/DSP/html/index.html) library, which provides optimized Digital Signal Processing (DSP) function implementations for [Arm Cortex-M](https://developer.arm.com/ip-products/processors/cortex-m) processors, will also be used to extract features from the real-time audio data prior to inference.\n",
        "\n",
        "While this guide focuses on detecting a fire alarm sound, it can be adapted for other sound classification tasks. You may also need to adapt the feature extraction stages and/or adjust ML model architecture for your use case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w60BKbnzzrP"
      },
      "source": [
        "## What you need to to get started\n",
        "\n",
        "### Development Environment\n",
        "\n",
        " * [Google Chrome](https://www.google.com/intl/en_ca/chrome/)\n",
        " * [Google Colab](https://colab.research.google.com/notebooks/)\n",
        " * A [Google Account](https://www.google.com/account/about/)\n",
        "\n",
        "### Hardware\n",
        "\n",
        "You’ll need one of the following development boards that are based on [Raspberry Pi’s RP2040 MCU chip](https://www.raspberrypi.org/products/rp2040/) that was released early in 2021.\n",
        "\n",
        "\n",
        "#### SparkFun RP2040 MicroMod and MicroMod ML Carrier\n",
        "\n",
        "This is recommended for people who are new to electronics and microcontrollers. While it does cost a bit more than the option below, it is easier to assemble and does not require a soldering iron, knowing how to solder and how to wire up breadboards.\n",
        "\n",
        " * [SparkFun MicroMod RP2040 Processor](https://www.sparkfun.com/products/17720)\n",
        "  * For the brains of the operation! Contains Raspberry Pi’s RP2040 MCU and 16MB of flash storage\n",
        " * [SparkFun MicroMod Machine Learning Carrier Board](https://www.sparkfun.com/products/16400)\n",
        "Enables USB connectivity, and provides a built-in microphone, IMU and camera connector\n",
        " * A USB-C cable to connect the board to your computer\n",
        " * A Phillips screwdriver\n",
        "\n",
        "#### Raspberry Pi Pico and PDM microphone board\n",
        "\n",
        "This option is slightly lower in cost, however it requires a soldering iron and knowledge of how to wire a breadboard with electronic components.\n",
        "\n",
        " * [Raspberry Pi Pico](https://www.raspberrypi.org/products/raspberry-pi-pico/)\n",
        " * [Adafruit PDM MEMS Microphone Breakout](https://www.adafruit.com/product/3492)\n",
        " * Half size or full size breadboard\n",
        " * Jumper wires\n",
        " * A USB-B micro cable to connect the board to your computer\n",
        " * Soldering iron\n",
        "\n",
        "#### More information\n",
        "\n",
        "Both of the options above will allow you to collect real-time 16 kHz audio from a digital microphone and process the audio signal in real-time on the development board’s Arm Cortex-M0+ processor which operates at 125 MHz. The application running on the Arm Cortex-M0+ will have a Digital Signal Processing (DSP) stage to extract features from the audio signal, the extracted features will then be fed into a neural network to perform a classification task to determine if a fire alarm sound is present in the board’s environment.\n",
        "\n",
        "### Hardware Setup\n",
        "\n",
        "#### SparkFun MicroMod RP2040\n",
        "\n",
        "For assembly, remove the screw on the carrier board, at an angle, slide in the MicroMod RP2040 Processor board into the socket and secure it in place with the screw. See the [MicroMod Machine Learning Carrier Board Hookup Guide](https://learn.sparkfun.com/tutorials/micromod-machine-learning-carrier-board-hookup-guide?_ga=2.90268890.1509654996.1628608170-268367655.1627493370#hardware-hookup) for more details.\n",
        "\n",
        "\n",
        "#### Raspberry Pi Pico\n",
        "\n",
        "Follow the instructions from the [Hardware Setup section of the \"Create a USB Microphone with the Raspberry Pi Pico\"](https://www.hackster.io/sandeep-mistry/create-a-usb-microphone-with-the-raspberry-pi-pico-cc9bd5#toc-hardware-setup-5) guide for assembly instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojuc2yoIrA8G"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t4hpGoHoUR6"
      },
      "source": [
        "### Python Libraries\n",
        "\n",
        "Let's start by installing the Python library dependencies:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J1XVmXyAINvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GZ1A4IQiIOAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ARM-software/CMSIS_5.git@5.8.0#egg=CMSISDSP\\&subdirectory=CMSIS/DSP/PythonWrapper"
      ],
      "metadata": {
        "id": "ypV18H2NKdEy",
        "outputId": "d5dcbd3d-64cd-41f6-d658-256aade00cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting CMSISDSP\n",
            "  Cloning https://github.com/ARM-software/CMSIS_5.git (to revision 5.8.0) to /tmp/pip-install-ptxt1s3u/cmsisdsp_f518686d513441a59995c44e47b970f5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ARM-software/CMSIS_5.git /tmp/pip-install-ptxt1s3u/cmsisdsp_f518686d513441a59995c44e47b970f5\n",
            "  Running command git checkout -q 649bd8aa41ed7d86b416c89cdb4b820b899a4cbc\n",
            "  Resolved https://github.com/ARM-software/CMSIS_5.git to commit 649bd8aa41ed7d86b416c89cdb4b820b899a4cbc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: CMSISDSP\n",
            "  Building wheel for CMSISDSP (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for CMSISDSP: filename=CMSISDSP-1.0.0-cp310-cp310-linux_x86_64.whl size=1352838 sha256=020d5c01ed3587e7ed4adb0764a49d05cf8bb580d959498803e6d4bd37d306e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w91ljpb3/wheels/91/e0/e0/a9f82222f5d1eac8dca08f231ea3bffa47d79f51a2da4355f1\n",
            "Successfully built CMSISDSP\n",
            "Installing collected packages: CMSISDSP\n",
            "Successfully installed CMSISDSP-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6CQatks4z4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681e6cb9-59c0-47b3-be21-d111bad142ec"
      },
      "source": [
        "!pip install librosa matplotlib pandas \"tensorflow==2.17.1\" \"tensorflow-io==0.37.1\" \"tensorflow-model-optimization==0.8.0\"\n",
        "\n",
        "!pip install git+https://github.com/ARM-software/CMSIS_5.gt@5.8.0#egg=CMSISDSP\\&subdirectory=CMSIS/DSP/PythonWrapper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow==2.17.1 in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow-io==0.37.1\n",
            "  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting tensorflow-model-optimization==0.8.0\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.26.4)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.8.0) (0.1.8)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.1) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.1) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.13.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2024.12.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.1) (0.1.2)\n",
            "Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-model-optimization, tensorflow-io\n",
            "Successfully installed tensorflow-io-0.37.1 tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMTfgPBoeRB"
      },
      "source": [
        "### Command line tools\n",
        "\n",
        "Now let's install the command line tools we will need to build applications for the Raspberry Pi RP2040:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb3DjP6lXyUf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "324aa46f-ce8b-4416-a958-c5572acde438"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.get_file('cmake-3.21.0-linux-x86_64.tar.gz',\n",
        "                        'https://github.com/Kitware/CMake/releases/download/v3.21.0/cmake-3.21.0-linux-x86_64.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='tools',\n",
        "                        extract=True)\n",
        "\n",
        "tf.keras.utils.get_file('gcc-arm-none-eabi-10-2020-q4-major-x86_64-linux.tar.bz2',\n",
        "                        'https://developer.arm.com/-/media/Files/downloads/gnu-rm/10-2020q4/gcc-arm-none-eabi-10-2020-q4-major-x86_64-linux.tar.bz2',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='tools',\n",
        "                        extract=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/Kitware/CMake/releases/download/v3.21.0/cmake-3.21.0-linux-x86_64.tar.gz\n",
            "\u001b[1m44660598/44660598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://developer.arm.com/-/media/Files/downloads/gnu-rm/10-2020q4/gcc-arm-none-eabi-10-2020-q4-major-x86_64-linux.tar.bz2\n",
            "\u001b[1m156882554/156882554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./tools/gcc-arm-none-eabi-10-2020-q4-major-x86_64-linux.tar.bz2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV4uI68XX_J0",
        "outputId": "3d691176-5bc0-46ac-c9a1-0084d7fef338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install -y xxd"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xxd is already the newest version (2:8.2.3995-1ubuntu2.21).\n",
            "xxd set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPi1R_THovN9"
      },
      "source": [
        "Now add the downloaded and extracted tools to the `PATH` environmental variable, so we can use them later on without specifying the full path to them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9evc84YJYz2P"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['PATH'] = f\"{os.getcwd()}/tools/cmake-3.21.0-linux-x86_64/bin:{os.environ['PATH']}\"\n",
        "os.environ['PATH'] = f\"{os.getcwd()}/tools/gcc-arm-none-eabi-10-2020-q4-major/bin:{os.environ['PATH']}\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT78ms3GpQIv"
      },
      "source": [
        "### Raspberry Pi Pico SDK\n",
        "\n",
        "We can use `git` to get the `v1.2.0` of the [Raspberry Pi Pico SDK](https://github.com/raspberrypi/pico-sdk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P02F36WY5uZ",
        "outputId": "f7379b75-0c23-41f7-dc03-7cb3cea728c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "git clone --branch 1.2.0 https://github.com/raspberrypi/pico-sdk.git\n",
        "cd pico-sdk\n",
        "git submodule init\n",
        "git submodule update"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pico-sdk'...\n",
            "remote: Enumerating objects: 11662, done.\u001b[K\n",
            "remote: Counting objects: 100% (533/533), done.\u001b[K\n",
            "remote: Compressing objects: 100% (315/315), done.\u001b[K\n",
            "remote: Total 11662 (delta 397), reused 218 (delta 218), pack-reused 11129 (from 2)\u001b[K\n",
            "Receiving objects: 100% (11662/11662), 5.18 MiB | 9.11 MiB/s, done.\n",
            "Resolving deltas: 100% (6094/6094), done.\n",
            "Note: switching to 'bfcbefafc5d2a210551a4d9d80b4303d4ae0adf7'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Submodule 'tinyusb' (https://github.com/hathach/tinyusb.git) registered for path 'lib/tinyusb'\n",
            "Cloning into '/content/pico-sdk/lib/tinyusb'...\n",
            "Submodule path 'lib/tinyusb': checked out 'd49938d0f5052bce70e55c652b657c0a6a7e84fe'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE_gX86hpk8h"
      },
      "source": [
        "Set the `PICO_SDK_PATH` environment variable to specify the location of the `pico-sdk`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXCHQtSWY_dS"
      },
      "source": [
        "os.environ['PICO_SDK_PATH'] = f\"{os.getcwd()}/pico-sdk\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzxhKhMQpzqr"
      },
      "source": [
        "**You will need to change the code cell below** to select the board you will be using for the remainder of the tutorial.\n",
        "\n",
        "By default the `PICO_BOARD` environment variable is set to `sparkfun_micromod` for the SparkFun RP2040 MicroMod. Set the value to `pico` if you are using a Raspberry Pi Pico board."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WVJZJFUTMEC",
        "outputId": "43477819-39a6-415e-c0f6-e0bd2bfc2766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "os.environ['PICO_BOARD'] = 'pico'\n",
        "\n",
        "print(f\"PICO_BOARD env. var. set to '{os.environ['PICO_BOARD']}'\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PICO_BOARD env. var. set to 'pico'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1-rsj4vchBC"
      },
      "source": [
        "### Project Files\n",
        "\n",
        "The source code for the inference application and Python utilities for Google Colab can also be cloned using `git`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jDNNonWcyAL",
        "outputId": "23f57ea3-5edf-474f-83ea-f59008f5df84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "git clone --recurse-submodules https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico.git"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ml-audio-classifier-example-for-pico'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 131 (delta 43), reused 118 (delta 40), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (131/131), 43.04 MiB | 28.12 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "Submodule 'inference-app/lib/CMSIS_5' (https://github.com/ARM-software/CMSIS_5.git) registered for path 'inference-app/lib/CMSIS_5'\n",
            "Submodule 'inference-app/lib/microphone-library-for-pico' (https://github.com/ArmDeveloperEcosystem/microphone-library-for-pico.git) registered for path 'inference-app/lib/microphone-library-for-pico'\n",
            "Submodule 'inference-app/lib/pico-tflmicro' (https://github.com/raspberrypi/pico-tflmicro.git) registered for path 'inference-app/lib/pico-tflmicro'\n",
            "Cloning into '/content/ml-audio-classifier-example-for-pico/inference-app/lib/CMSIS_5'...\n",
            "remote: Enumerating objects: 44155, done.        \n",
            "remote: Counting objects: 100% (44155/44155), done.        \n",
            "remote: Compressing objects: 100% (17298/17298), done.        \n",
            "remote: Total 44155 (delta 32317), reused 35534 (delta 25781), pack-reused 0 (from 0)        \n",
            "Receiving objects: 100% (44155/44155), 168.13 MiB | 20.90 MiB/s, done.\n",
            "Resolving deltas: 100% (32317/32317), done.\n",
            "Cloning into '/content/ml-audio-classifier-example-for-pico/inference-app/lib/microphone-library-for-pico'...\n",
            "remote: Enumerating objects: 211, done.        \n",
            "remote: Counting objects: 100% (90/90), done.        \n",
            "remote: Compressing objects: 100% (22/22), done.        \n",
            "remote: Total 211 (delta 74), reused 68 (delta 68), pack-reused 121 (from 1)        \n",
            "Receiving objects: 100% (211/211), 49.87 KiB | 6.23 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "Cloning into '/content/ml-audio-classifier-example-for-pico/inference-app/lib/pico-tflmicro'...\n",
            "remote: Enumerating objects: 3685, done.        \n",
            "remote: Counting objects: 100% (1134/1134), done.        \n",
            "remote: Compressing objects: 100% (607/607), done.        \n",
            "remote: Total 3685 (delta 670), reused 758 (delta 505), pack-reused 2551 (from 1)        \n",
            "Receiving objects: 100% (3685/3685), 4.07 MiB | 15.39 MiB/s, done.\n",
            "Resolving deltas: 100% (2140/2140), done.\n",
            "Submodule path 'inference-app/lib/CMSIS_5': checked out '13b9f72f212688d2306d0d085d87cbb4bf9e5d3f'\n",
            "Submodule path 'inference-app/lib/microphone-library-for-pico': checked out '7d398aa671a873230871e41d0fd79c8505888d7f'\n",
            "Submodule path 'inference-app/lib/pico-tflmicro': checked out '6ff6387ed1fb3b721b0996583c4af8872980833b'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWTsVPrgdL9R"
      },
      "source": [
        "For convenience we can create symbolic links for the project files that we've cloned to the root Google Colab folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1JjfZ15eZX9",
        "outputId": "ff2f1c23-d3e9-4266-8a12-dbc55699acf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "ln -s ml-audio-classifier-example-for-pico/colab_utils colab_utils\n",
        "ln -s ml-audio-classifier-example-for-pico/inference-app inference-app"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMuv2599z8hy"
      },
      "source": [
        "## Baseline model\n",
        "\n",
        "We will start by training a generic sound classifier model with TensorFlow using the [ESC-50: Dataset for Environmental Sound Classification](https://github.com/karolpiczak/ESC-50). This will allow us to create a more generic model that is trained on a broader dataset, and then use Transfer Learning later on to fine tune it for our specific audio classification task.\n",
        "\n",
        "This model will be trained on the ESC-50 dataset, which contains 50 types of sounds; each sound category has 40 audio files that are 5 seconds each in length. Each audio file will be split into 1 second soundbites, and any soundbites that contain pure silence will be discarded.\n",
        "\n",
        "### Prepare dataset\n",
        "\n",
        "#### Download and extract\n",
        "\n",
        "The ESC-50 dataset will be downloaded and extracted to the `datasets` folder using the [`tf.keras.utils.get_file(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVM5_CQdoEAZ",
        "outputId": "9f11e751-aaec-431b-eec8-0a7a23076da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/karoldvl/ESC-50/archive/master.zip\n",
            "644030464/Unknown - 40s 0us/step"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./datasets/esc-50.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH8NUBY90nlH"
      },
      "source": [
        "#### Load dataset metadata\n",
        "\n",
        "Now we will use the [pandas](https://pandas.pydata.org/) library to read the `datasets/ESC-50-master/meta/esc50.csv` file which contains the metadata for the audio files in the ESC-50 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJnoX0I70r2j"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "df = pd.read_csv(esc50_csv)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SMgChs01Jry"
      },
      "source": [
        "Then add new column with the `fullpath` of the wave files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mioGTM5M1TQN"
      },
      "source": [
        "from os import path\n",
        "\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "df['fullpath'] = df['filename'].map(lambda x: path.join(base_data_path, x))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqnpuM-G2jDr"
      },
      "source": [
        "#### Load wave file data\n",
        "\n",
        "We can then define a new function named `load_wav` to load audio samples from a wave file using TensorFlow's [`tf.io.read_file(...)`](https://www.tensorflow.org/api_docs/python/tf/io/read_file) and[`tf.audio.decode_wav(...)`](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav) API's. The [`tfio.audio.resample(...)`](https://www.tensorflow.org/io/api_docs/python/tfio/audio/resample) API will be used to resample the audio samples at the specified sampling rate.\n",
        "\n",
        "[librosa](https://librosa.org/)'s [`load(...)`](https://librosa.org/doc/main/generated/librosa.load.html) API will be used as a fallback if TensorFlow is unable to decode the wave file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70RwxZs12iPZ"
      },
      "source": [
        "import tensorflow_io as tfio\n",
        "import librosa\n",
        "\n",
        "def load_wav(filename, desired_sample_rate, desired_channels):\n",
        "  try:\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=desired_channels)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "  except:\n",
        "    # fallback to librosa if the wav file can be read with TF\n",
        "    filename = tf.cast(filename, tf.string)\n",
        "    wav, sample_rate = librosa.load(filename.numpy().decode('utf-8'), sr=None, mono=(desired_channels == 1))\n",
        "\n",
        "  wav = tfio.audio.resample(wav, rate_in=tf.cast(sample_rate, dtype=tf.int64), rate_out=tf.cast(desired_sample_rate, dtype=tf.int64))\n",
        "\n",
        "  return wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_2AYbxi5oIp"
      },
      "source": [
        "Now let's load the first wave file, which is a sound of a dog barking, from the pandas `DataFrame`, and plot it overtime using `matplotlib`. The [`IPython.display.Audio(...)`](https://ipython.org/ipython-doc/3/api/generated/IPython.display.html#IPython.display.Audio) API can be used to playback the audio samples inside the notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgLhGyi75bvI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "sample_rate = 16000\n",
        "channels = 1\n",
        "\n",
        "test_wav_file_path = df['fullpath'][0]\n",
        "test_wav_data = load_wav(test_wav_file_path, sample_rate, channels)\n",
        "\n",
        "plt.plot(test_wav_data)\n",
        "plt.show()\n",
        "\n",
        "display.Audio(test_wav_data, rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLVRrhvt7E9T"
      },
      "source": [
        "If we zoom in and only plot samples `32000` to `48000`, we can get a closer plot of the audio samples in the wave file in the 2 to 3 second span:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvKn45Pl6X4B"
      },
      "source": [
        "_ = plt.plot(test_wav_data[32000:48000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYg6G3ivfOPq"
      },
      "source": [
        "We can then use the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) TensorFlow API to create a pipeline that loads all wave file data from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDr615fNfLdp"
      },
      "source": [
        "fullpaths = df['fullpath']\n",
        "targets = df['target']\n",
        "folds = df['fold']\n",
        "\n",
        "fullpaths_ds = tf.data.Dataset.from_tensor_slices((fullpaths, targets, folds))\n",
        "fullpaths_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2IJbEHPf2ok"
      },
      "source": [
        "Map each `fullpath` value to wave file samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-A6jYUz6lfI"
      },
      "source": [
        "def load_wav_for_map(fullpath, label, fold):\n",
        "  wav = tf.py_function(load_wav, [fullpath, sample_rate, channels], tf.float32)\n",
        "\n",
        "  return wav, label, fold\n",
        "\n",
        "wav_ds = fullpaths_ds.map(load_wav_for_map)\n",
        "wav_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2RD4zXVRAJ2"
      },
      "source": [
        "#### Split Wave file data\n",
        "\n",
        "We would like to train the model on 1 secound soundbites, so we must split up the 5 seconds of audio per item in the ESC-50 dataset to slices of 16000 samples. We will also stride the original audio samples `4000` samples at a time, and filter out any sound chunks that contain pure silence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnDQ_PDERIJl"
      },
      "source": [
        "@tf.function\n",
        "def split_wav(wav, width, stride):\n",
        "  return tf.map_fn(fn=lambda t: wav[t * stride:t * stride + width], elems=tf.range((tf.shape(wav)[0] - width) // stride), fn_output_signature=tf.float32)\n",
        "\n",
        "@tf.function\n",
        "def wav_not_empty(wav):\n",
        "  return tf.experimental.numpy.any(wav)\n",
        "\n",
        "def split_wav_for_flat_map(wav, label, fold):\n",
        "  wavs = split_wav(wav, width=16000, stride=4000)\n",
        "  labels = tf.repeat(label, tf.shape(wavs)[0])\n",
        "  folds = tf.repeat(fold, tf.shape(wavs)[0])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices((wavs, labels, folds))\n",
        "\n",
        "split_wav_ds = wav_ds.flat_map(split_wav_for_flat_map)\n",
        "split_wav_ds = split_wav_ds.filter(lambda x, y, z: wav_not_empty(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyiCY4I-9AFS"
      },
      "source": [
        "Let's plot the first 5 soundbites over time using `matplotlib`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeRO3Z4khchs"
      },
      "source": [
        "for wav, _, _ in split_wav_ds.take(5):\n",
        "  _ = plt.plot(wav)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdV6yAYYhsBw"
      },
      "source": [
        "#### Create Spectrograms\n",
        "\n",
        "Rather than passing in the time series data directly into our TensorFlow model, we will transform the audio data into an audio spectrogram representation. This will create a 2D representation of the audio signal’s frequency content over time.\n",
        "\n",
        "The input audio signal we will use will have a sampling rate of 16kHz, this means one second of audio will contain 16,000 samples. Using TensorFlow’s [`tf.signal.stft(...)`](https://www.tensorflow.org/api_docs/python/tf/signal/stft) function we can transform a 1 second audio signal into a 2D tensor representation. We will choose a frame length of 256 and a frame step of 128, so the output of this feature extraction stage will be a Tensor that has a shape of `(124, 129)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzOXIaNkh9jW"
      },
      "source": [
        "@tf.function\n",
        "def create_spectrogram(samples):\n",
        "  return tf.abs(\n",
        "      tf.signal.stft(samples, frame_length=256, frame_step=128)\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts4rVQvG9kgL"
      },
      "source": [
        "Let's take the same 2 - 3 second interval of the first dog barking wave file and create it's spectrogram representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuGcCinfilFs"
      },
      "source": [
        "spectrogram = create_spectrogram(test_wav_data[32000:48000])\n",
        "\n",
        "spectrogram.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFq1Mpq_-Ecc"
      },
      "source": [
        "We can then create `plot_spectrogram` function to plot the spectrogram representation using `matplotlib`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrj38Jdiig3H"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_spectrogram(spectrogram, vmax=None):\n",
        "  transposed_spectrogram = tf.transpose(spectrogram)\n",
        "\n",
        "  fig = plt.figure(figsize=(8,6))\n",
        "  height = transposed_spectrogram.shape[0]\n",
        "  X = np.arange(transposed_spectrogram.shape[1])\n",
        "  Y = np.arange(height * int(sample_rate / 256), step=int(sample_rate / 256))\n",
        "\n",
        "  im = plt.pcolormesh(X, Y, tf.transpose(spectrogram), vmax=vmax)\n",
        "\n",
        "  fig.colorbar(im)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_spectrogram(spectrogram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THC4DDLg-SyA"
      },
      "source": [
        "Then we can map each split wave item to a spectrogram:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw-KTABdvSpG"
      },
      "source": [
        "def create_spectrogram_for_map(samples, label, fold):\n",
        "  return create_spectrogram(samples), label, fold\n",
        "\n",
        "spectrograms_ds = split_wav_ds.map(create_spectrogram_for_map)\n",
        "spectrograms_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMFP7b-l-kOb"
      },
      "source": [
        "Let's plot the first 5 spectrograms in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc3fqcs6lMal"
      },
      "source": [
        "for s, _, _ in spectrograms_ds.take(5):\n",
        "  plot_spectrogram(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrCmiQVyQ_NB"
      },
      "source": [
        "### Split Dataset\n",
        "\n",
        "Before we start training the ML classifier model, we must split the dataset up in three parts: training, validation, and test.\n",
        "\n",
        "We will use the same technique in TensorFlow's [Transfer learning with YAMNet for environmental sound classification](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio#split_the_data) guide, and use the `fold` column of the ESC-50 dataset to determine the split.\n",
        "\n",
        "Before splitting the dataset, let's set a random seed for reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3bR-BHBRCYi"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "random_seed = 42\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq3kDVG9_nhW"
      },
      "source": [
        "Entries with a `fold` value of less than 4 will used for training, the ones with a `value` will be used for validation, and finally the remaining items with be used for testing.\n",
        "\n",
        "The `fold` column will be removed as it is no longer needed, and the dimensions of the spectrogram shape will be expanded from `(124, 129)` to `(124, 129, 1)`. The training items will also be shuffled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyclwrPnMEIh"
      },
      "source": [
        "cached_ds = spectrograms_ds.cache()\n",
        "\n",
        "train_ds = cached_ds.filter(lambda spectrogram, label, fold: fold < 4)\n",
        "val_ds = cached_ds.filter(lambda spectrogram, label, fold: fold == 4)\n",
        "test_ds = cached_ds.filter(lambda spectrogram, label, fold: fold > 4)\n",
        "\n",
        "# remove the folds column as it's no longer needed\n",
        "remove_fold_column = lambda spectrogram, label, fold: (tf.expand_dims(spectrogram, axis=-1), label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000, seed=random_seed).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIdHUP8mRF-9"
      },
      "source": [
        "### Train Model\n",
        "\n",
        "Now that we have the features extracted from the audio signal, we can create a model using TensorFlow’s Keras  API. The model will consist of 8 layers:\n",
        "\n",
        " 1. An input layer.\n",
        " 1. A preprocessing layer, that will resize the input tensor from 124x129x1 to 32x32x1.\n",
        " 1. A normalization layer, that will scale the input values between -1 and 1\n",
        " 1. A 2D convolution layer with: 8 filters, a kernel size of 8x8, and stride of 2x2, and ReLU activation function.\n",
        " 1. A 2D max pooling layer with size of 2x2\n",
        " 1. A flatten layer to flatten the 2D data to 1D\n",
        " 1. A dropout layer, that will help reduce overfitting during training\n",
        " 1. A dense layer with 50 outputs and a softmax activation function, which outputs the likelihood of the sound category (between 0 and 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbTAU5yZA43R"
      },
      "source": [
        "Before we build the model using [Tensflow's Keras API's](https://www.tensorflow.org/api_docs/python/tf/keras), we will create normalization layer and feed in all the spectrogram dataset items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFLHe9y-iGmj"
      },
      "source": [
        "for spectrogram, _, _ in cached_ds.take(1):\n",
        "    input_shape = tf.expand_dims(spectrogram, axis=-1).shape\n",
        "    print('Input shape:', input_shape)\n",
        "\n",
        "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "norm_layer.adapt(cached_ds.map(lambda x, y, z: tf.reshape(x, input_shape)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3N5rys9EF8B"
      },
      "source": [
        "Define a sequential 8 layer model as described above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyO5ilzPPePi"
      },
      "source": [
        "baseline_model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=input_shape),\n",
        "  tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"),\n",
        "  norm_layer,\n",
        "  tf.keras.layers.Conv2D(8, kernel_size=(8,8), strides=(2, 2), activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Dense(50, activation='softmax')\n",
        "])\n",
        "\n",
        "baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4PgoMF8ENpu"
      },
      "source": [
        "Compile the model with `accuracy` metrics, an Adam optimizer and a sparse categorical crossentropy loss function. As well as define early stopping and dynamic learning rate scheduler callbacks for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyTlP0G1QHD6"
      },
      "source": [
        "METRICS = [\n",
        "      \"accuracy\",\n",
        "]\n",
        "\n",
        "baseline_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 100:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=25),\n",
        "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBUof7CcErKE"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqs-O9o8QV58"
      },
      "source": [
        "EPOCHS = 250\n",
        "history = baseline_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjnfP2hCFB9z"
      },
      "source": [
        "Evaluate the loss and accuracy of the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpXhy1eQmgH"
      },
      "source": [
        "baseline_model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnDIdyN38UVh"
      },
      "source": [
        "The baseline model has a relatively low accuracy ~24%, however in the next steps we will use it as a starting point to fine tune a more accurate model for our use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkW5RFLVFIQO"
      },
      "source": [
        "Save the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmZNjn-OR-rF"
      },
      "source": [
        "baseline_model.save(\"baseline_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CalvtW18FOra"
      },
      "source": [
        "Create a zip file of the saved model, for download purposes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00l_RwHWSY-Y"
      },
      "source": [
        "!zip -r baseline_model.zip baseline_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EopW6v06SsRu"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "Now we will use Transfer Learning and change the classification head of the model to train a binary classification model for fire alarm sounds.\n",
        "\n",
        "Transfer Learning is the process of retraining a model that has been developed for a task to complete a new similar task. The idea is that the model has learned transferable \"skills\" and the weights and biases can be used in other models as a starting point.\n",
        "\n",
        "Transfer learning is very common in computer vision. Big data companies spend weeks training models on ImageNet, this is not possible for most people and so people reuse the models built in these research companies to complete their own tasks. A model designed to recognise 1000 different objects in a image can be adapted to recognise other or similar objects.\n",
        "\n",
        "As humans we use transfer learning too. The skills you developed to learn to walk could also be used to learn to run later on.\n",
        "\n",
        "In a neural network, the first few layers of a model start to perform a \"feature extraction\" such as finding shapes, edges and colours. The layers later on are used as classifiers; they take the extracted features and classify them.\n",
        "\n",
        "You can find more information and visualizations about this here https://yosinski.com/deepvis.\n",
        "\n",
        "Because of this, we can assume the first few layers have learned quite general feature extraction techniques that can be applied to all similar tasks and so we can freeze all these layers. The classifier layer will need to be trained based on the new classes.\n",
        "\n",
        "To do this, we break the process into two steps:\n",
        "Freeze the \"backbone\" of the model and train the head with a fairly high learning rate. We slowly reduce the learning rate.\n",
        "Unfreeze the \"backbone\" and fine-tune the model with a low learning rate.\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We have collected 10 fire alarm clips from [freesound.org](https://freesound.org/) and [BigSoundBank.com](https://bigsoundbank.com/).  Background noise clips from the [SpeechCommands](https://www.tensorflow.org/datasets/catalog/speech_commands) dataset, will be used for non-fire alarm sounds. This dataset is small and represents the sort of data you might expect to see in the real world. Data augmentation techniques will be used to supplement the training data we’ve collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APrXeNMkZmyT"
      },
      "source": [
        "### Download datasets\n",
        "\n",
        "We've created an archive with the following wave files for you:\n",
        "\n",
        " * https://freesound.org/people/rayprice/sounds/155006/ ([CC BY 3.0 license](https://creativecommons.org/licenses/by/3.0/))\n",
        "\n",
        " * https://freesound.org/people/deleted_user_2104797/sounds/164686/ ([CC0 1.0 license](https://creativecommons.org/publicdomain/zero/1.0/))\n",
        "\n",
        " * https://freesound.org/people/AdamWeeden/sounds/255180/ ([CC BY 3.0 license](https://creativecommons.org/licenses/by/3.0/))\n",
        "\n",
        "* https://freesound.org/people/MoonlightShadow/sounds/325367/([CC0 1.0 license](https://creativecommons.org/publicdomain/zero/1.0/))\n",
        "\n",
        "* https://freesound.org/people/SpliceSound/sounds/369847/ ([CC0 1.0 license](https://creativecommons.org/publicdomain/zero/1.0/))\n",
        "\n",
        "* https://freesound.org/people/SpliceSound/sounds/369848/ ([CC0 1.0 license](https://creativecommons.org/publicdomain/zero/1.0/))\n",
        "\n",
        "* https://bigsoundbank.com/detail-0800-smoke-detector-alarm.html ([free of charge and royalty free.](https://bigsoundbank.com/droit.html))\n",
        "\n",
        "* https://bigsoundbank.com/detail-1151-smoke-detector-alarm-2.html ([free of charge and royalty free.](https://bigsoundbank.com/droit.html))\n",
        "\n",
        "* https://bigsoundbank.com/detail-1153-smoke-detector-alarm-3.html ([free of charge and royalty free.](https://bigsoundbank.com/droit.html))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-FUUQXMZaJQ"
      },
      "source": [
        "tf.keras.utils.get_file('fire_alarms.tar.gz',\n",
        "                        'https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico/archive/refs/heads/fire_alarms.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpS5FBOJaK0z"
      },
      "source": [
        "# Since we only need the files in the _background_noise_ folder of the dataset\n",
        "# use the curl command to download the archive file and then manually extract\n",
        "# using the tar command, instead of using tf.keras.utils.get_file(...)\n",
        "# in Python\n",
        "\n",
        "!mkdir -p datasets/speech_commands\n",
        "!curl -L -o datasets/speech_commands_v0.02.tar.gz http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "!tar --wildcards --directory datasets/speech_commands -xzvf datasets/speech_commands_v0.02.tar.gz './_background_noise_/*'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NoayTieb_WR"
      },
      "source": [
        "### Load dataset\n",
        "\n",
        "Instead of using a pandas DataFrame to load the dataset, we will load the fire alarm files and background noise files separately. The `label` and `fold` values will be mapped manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owe9kEqBcWR0"
      },
      "source": [
        "fire_alarm_files_ds = tf.data.Dataset.list_files(\"datasets/ml-audio-classifier-example-for-pico-fire_alarms/*.wav\", shuffle=False)\n",
        "fire_alarm_files_ds = fire_alarm_files_ds.map(lambda x: (x, 1, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StJH-68kbYvF"
      },
      "source": [
        "background_noise_files_ds = tf.data.Dataset.list_files(\"datasets/speech_commands/_background_noise_/*.wav\", shuffle=False)\n",
        "background_noise_files_ds = background_noise_files_ds.map(lambda x: (x, 0, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMOJWhoFcm-5"
      },
      "source": [
        "fire_alarm_wav_ds = fire_alarm_files_ds.map(load_wav_for_map)\n",
        "fire_alarm_wav_ds = fire_alarm_wav_ds.cache()\n",
        "\n",
        "background_noise_wav_ds = background_noise_files_ds.map(load_wav_for_map)\n",
        "background_noise_wav_ds = background_noise_wav_ds.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l29lUf44JB1r"
      },
      "source": [
        "Let's plot and listen to the first fire alarm file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TpRuruRTwpi"
      },
      "source": [
        "for wav_data, _, _ in fire_alarm_wav_ds.take(1):\n",
        "  plt.plot(wav_data)\n",
        "  plt.ylim([-1, 1])\n",
        "  plt.show()\n",
        "\n",
        "  display.display(display.Audio(wav_data, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i6TmVUUJeuh"
      },
      "source": [
        "Then do the same for the first background noise file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIxUgvNaZWWN"
      },
      "source": [
        "for wav_data, _, _ in background_noise_wav_ds.take(1):\n",
        "  plt.plot(wav_data)\n",
        "  plt.ylim([-1, 1])\n",
        "  plt.show()\n",
        "\n",
        "  display.display(display.Audio(wav_data, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNoTz4FpJv8i"
      },
      "source": [
        "Then split the audio samples into 1 second soundbites:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYJT95GgpX4p"
      },
      "source": [
        "split_fire_alarm_wav_ds = fire_alarm_wav_ds.flat_map(split_wav_for_flat_map)\n",
        "split_fire_alarm_wav_ds = split_fire_alarm_wav_ds.filter(lambda x, y, z: wav_not_empty(x))\n",
        "\n",
        "split_background_noise_wav_ds = background_noise_wav_ds.flat_map(split_wav_for_flat_map)\n",
        "split_background_noise_wav_ds = split_background_noise_wav_ds.filter(lambda x, y, z: wav_not_empty(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LICTjFR3J8uO"
      },
      "source": [
        "TensorFlow Lite for Microcontroller (TFLu) provides a subset of TensorFlow operations, so we are unable to use the `tf.signal.sft(...)` API we’ve used for feature extraction of the baseline model on our MCU. However, we can leverage Arm’s CMSIS-DSP library to generate spectrograms on the MCU. CMSIS-DSP contains support for both floating-point and fixed-point DSP operations which are optimized for Arm Cortex-M processors, including the Arm Cortex-M0+ that we will be deploying the ML model to. The Arm Cortex-M0+ does not contain a floating-point unit (FPU) so it would be better to leverage a 16-bit fixed-point DSP based feature extraction pipeline on the board.\n",
        "\n",
        "We can leverage CMSIS-DSP’s Python Wrapper in the notebook to perform the same operations on our training pipeline using 16-bit fixed-point math. At a high level we can replicate the TensorFlow SFT API with the following CMSIS-DSP based operations:\n",
        "\n",
        " 1. Manually creating a Hanning Window of length 256 using the Hanning Window formula along with CMSIS-DSP’s `arm_cos_f32` API.\n",
        " 1. Creating a CMSIS-DSP `arm_rfft_instance_q15` instance and initializing it using CMSIS-DSP’s `arm_rfft_init_q15` API.\n",
        " 1. Looping through the audio data 256 samples at a time, with a stride of 128 (this matches the parameters we’ve passed into the TF sft API)\n",
        "  1. Multiplying the 256 samples by the Hanning Window, using CMSIS-DSP’s `arm_mult_q15` API\n",
        "  1. Calculating the FFT of the output of the previous step, using CMSIS-DSP’s `arm_rfft_q15` API\n",
        "  1. Calculating the magnitude of the previous step, using CMSIS-DSP’s `arm_cmplx_mag_q15` API\n",
        " 1. Each audio soundbites’s FFT magnitude represents the one column of the spectrogram.\n",
        " 1. Since our baseline model expects a floating-point input, instead of the 16-bit quantized value we were using, the CMSIS-DSP `arm_q15_to_float` API can be used to convert the spectrogram data from a 16-bit fixed-point value to a floating-point value for training.\n",
        "\n",
        "For an in-depth description of how to create audio spectrograms using fixed-point operations with CMSIS-DSP, please see the [Towards Data Science “Fixed-point DSP for Data Scientists” guide](https://towardsdatascience.com/fixed-point-dsp-for-data-scientists-d773a4271f7f).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhrliYiAfKE"
      },
      "source": [
        "import cmsisdsp\n",
        "from numpy import pi as PI\n",
        "\n",
        "window_size = 256\n",
        "step_size = 128\n",
        "\n",
        "hanning_window_f32 = np.zeros(window_size)\n",
        "for i in range(window_size):\n",
        "  hanning_window_f32[i] = 0.5 * (1 - cmsisdsp.arm_cos_f32(2 * PI * i / window_size ))\n",
        "\n",
        "hanning_window_q15 = cmsisdsp.arm_float_to_q15(hanning_window_f32)\n",
        "\n",
        "rfftq15 = cmsisdsp.arm_rfft_instance_q15()\n",
        "status = cmsisdsp.arm_rfft_init_q15(rfftq15, window_size, 0, 1)\n",
        "\n",
        "def get_arm_spectrogram(waveform):\n",
        "\n",
        "  num_frames = int(1 + (len(waveform) - window_size) // step_size)\n",
        "  fft_size = int(window_size // 2 + 1)\n",
        "\n",
        "  # Convert the audio to q15\n",
        "  waveform_q15 = cmsisdsp.arm_float_to_q15(waveform)\n",
        "\n",
        "  # Create empty spectrogram array\n",
        "  spectrogram_q15 = np.empty((num_frames, fft_size), dtype = np.int16)\n",
        "\n",
        "  start_index = 0\n",
        "\n",
        "  for index in range(num_frames):\n",
        "    # Take the window from the waveform.\n",
        "    window = waveform_q15[start_index:start_index + window_size]\n",
        "\n",
        "    # Apply the Hanning Window.\n",
        "    window = cmsisdsp.arm_mult_q15(window, hanning_window_q15)\n",
        "\n",
        "    # Calculate the FFT, shift by 7 according to docs\n",
        "    window = cmsisdsp.arm_rfft_q15(rfftq15, window)\n",
        "\n",
        "    # Take the absolute value of the FFT and add to the Spectrogram.\n",
        "    spectrogram_q15[index] = cmsisdsp.arm_cmplx_mag_q15(window)[:fft_size]\n",
        "\n",
        "    # Increase the start index of the window by the overlap amount.\n",
        "    start_index += step_size\n",
        "\n",
        "  # Convert to numpy output ready for keras\n",
        "  return cmsisdsp.arm_q15_to_float(spectrogram_q15).reshape(num_frames,fft_size) * 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hHnibH7LCz1"
      },
      "source": [
        "Let's create a spectrogram representation for all of the fire alarm soundbites, and plot the first spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRDqTa1CZfEA"
      },
      "source": [
        "@tf.function\n",
        "def create_arm_spectrogram_for_map(wav, label, fold):\n",
        "  spectrogram = tf.py_function(get_arm_spectrogram, [wav], tf.float32)\n",
        "\n",
        "  return spectrogram, label, fold\n",
        "\n",
        "fire_alarm_spectrograms_ds = split_fire_alarm_wav_ds.map(create_arm_spectrogram_for_map)\n",
        "fire_alarm_spectrograms_ds = fire_alarm_spectrograms_ds.cache()\n",
        "\n",
        "for spectrogram, _, _ in fire_alarm_spectrograms_ds.take(1):\n",
        "  plot_spectrogram(spectrogram)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ-fpDdkLyyy"
      },
      "source": [
        "The do the same for the background noise soundbites:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haGn0RT9Z1FI"
      },
      "source": [
        "background_noise_spectrograms_ds = split_background_noise_wav_ds.map(create_arm_spectrogram_for_map)\n",
        "background_noise_spectrograms_ds = background_noise_spectrograms_ds.cache()\n",
        "\n",
        "for spectrogram, _, _ in background_noise_spectrograms_ds.take(1):\n",
        "  plot_spectrogram(spectrogram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi-trTZtL6FG"
      },
      "source": [
        "Now let's calculate the lengths of each dataset to see how balanced they are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouhaiuLzMFiN"
      },
      "source": [
        "def calculate_ds_len(ds):\n",
        "  count = 0\n",
        "  for _, _, _ in ds:\n",
        "    count += 1\n",
        "\n",
        "  return count\n",
        "\n",
        "num_fire_alarm_spectrograms = calculate_ds_len(fire_alarm_spectrograms_ds)\n",
        "num_background_noise_spectrograms = calculate_ds_len(background_noise_spectrograms_ds)\n",
        "\n",
        "print(f\"num_fire_alarm_spectrograms = {num_fire_alarm_spectrograms}\")\n",
        "print(f\"num_background_noise_spectrograms = {num_background_noise_spectrograms}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EigYdlAXMpkP"
      },
      "source": [
        "We can see there a more background noise samples than fire alarm samples. In the next section we will use data augmentation to balance this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxYKzLPCmIrO"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGZI32bKMI9Z"
      },
      "source": [
        "Data augmentation is a set of techniques used to increase the size of a dataset. This is done by slightly modifying samples from the dataset or by creating synthetic data. In this situation we are using audio and we will create a few functions to augment the different samples. We will use three techniques:\n",
        "\n",
        " * adding white noise to the audio samples\n",
        " * adding random silence to the audio\n",
        " * mixing two audio samples together\n",
        "\n",
        "As well as increasing the size of the dataset, data augmentation also helps to reduce overfitting by training the model on different (not perfect) data samples. For example, on a microcontroller you are unlikely to have perfect high quality audio, and so a technique like adding white noise can help the model work in situations where your microphone might every so often have noise in there.\n",
        "\n",
        "First let's plot the time representation of the first fire alarm soundbite over time along with it's spectrogram representation, so we can compare against the augmented versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjWJd9Zdo4NH"
      },
      "source": [
        "for wav, _, _ in split_fire_alarm_wav_ds.take(1):\n",
        "  test_fire_alarm_wav = wav\n",
        "\n",
        "plt.plot(test_fire_alarm_wav)\n",
        "plt.ylim([-1, 1])\n",
        "plt.show()\n",
        "\n",
        "plot_spectrogram(get_arm_spectrogram(test_fire_alarm_wav), vmax=25)\n",
        "\n",
        "display.display(display.Audio(test_fire_alarm_wav, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDSEeTsLo5aN"
      },
      "source": [
        "#### White Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61lkrtSuNIS1"
      },
      "source": [
        "TensorFlow's [`tf.random.uniform(...)`](https://www.tensorflow.org/api_docs/python/tf/random/uniform) API can be used generate a Tensor of equal shape to the original audio. This Tensor can then be multiplied by a random scalar, and then added to the original audio samples. The [`tf.clip_by_value(...)`](https://www.tensorflow.org/api_docs/python/tf/clip_by_value) API will also be used to ensure the audio remains in the range of -1.0 to 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXV0un4cmRTY"
      },
      "source": [
        "def add_white_noise(audio):\n",
        "  #generate noise and the scalar multiplier\n",
        "  noise = tf.random.uniform(shape=tf.shape(audio), minval=-1, maxval=1)\n",
        "  noise_scalar = tf.random.uniform(shape=[1], minval=0, maxval=0.2)\n",
        "\n",
        "  # add them to the original audio\n",
        "  audio_with_noise = audio + (noise * noise_scalar)\n",
        "\n",
        "  # final clip the values to ensure they are still between -1 and 1\n",
        "  audio_with_noise = tf.clip_by_value(audio_with_noise, clip_value_min=-1, clip_value_max=1)\n",
        "\n",
        "  return audio_with_noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z-wGwCmNx4x"
      },
      "source": [
        "Let's apply the white noise to the fire alarm sound and then plot it to compare. We can also listen to the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsffnIb7paHj"
      },
      "source": [
        "test_fire_alarm_with_white_noise_wav = add_white_noise(test_fire_alarm_wav)\n",
        "\n",
        "plt.plot(test_fire_alarm_with_white_noise_wav)\n",
        "plt.ylim([-1, 1])\n",
        "plt.show()\n",
        "\n",
        "plot_spectrogram(get_arm_spectrogram(test_fire_alarm_with_white_noise_wav), vmax=25)\n",
        "\n",
        "display.display(display.Audio(test_fire_alarm_with_white_noise_wav, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCE-90pJpoio"
      },
      "source": [
        "#### Random Silence\n",
        "\n",
        "TensorFlow's [`tf.random.categorical(...)`](https://www.tensorflow.org/api_docs/python/tf/random/categorical) API can be used generate a Tensor of equal shape to the original audio containing mask of `True` or `False`. This mask can then be casted to a float type of 1.0 or 0.0, so that it can be multiplied by the original audio single to create random periods of silence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSNkZPqPnW_k"
      },
      "source": [
        "def add_random_silence(audio):\n",
        "  audio_mask = tf.random.categorical(tf.math.log([[0.2, 0.8]]), num_samples=tf.shape(audio)[0])\n",
        "  audio_mask = tf.cast(audio_mask, dtype=tf.float32)\n",
        "  audio_mask = tf.squeeze(audio_mask, axis=0)\n",
        "\n",
        "  # multiply the audio input by the mask\n",
        "  augmented_audio = audio * audio_mask\n",
        "\n",
        "  return augmented_audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ2p-MiwOzm9"
      },
      "source": [
        "Let's apply the random silence to the fire alarm sound and then plot it to compare. We can also listen to the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl94M9Ryp9zJ"
      },
      "source": [
        "test_fire_alarm_with_random_silence_wav = add_random_silence(test_fire_alarm_wav)\n",
        "\n",
        "plt.plot(test_fire_alarm_with_random_silence_wav)\n",
        "plt.ylim([-1, 1])\n",
        "plt.show()\n",
        "\n",
        "plot_spectrogram(get_arm_spectrogram(test_fire_alarm_with_random_silence_wav), vmax=25)\n",
        "\n",
        "display.display(display.Audio(test_fire_alarm_with_random_silence_wav, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSfVXJhcs1sn"
      },
      "source": [
        "#### Audio Mixups\n",
        "\n",
        "We can combine a fire alarm soundbite with a background noise soundbite to create a mixed up version of the two.\n",
        "\n",
        "Let's select the first background noise soundbite do see how this can be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mfaofOktV19"
      },
      "source": [
        "for wav, _, _ in split_background_noise_wav_ds.take(1):\n",
        "  test_background_noise_wav = wav\n",
        "\n",
        "plt.plot(test_background_noise_wav)\n",
        "plt.ylim([-1, 1])\n",
        "plt.show()\n",
        "\n",
        "plot_spectrogram(get_arm_spectrogram(test_background_noise_wav))\n",
        "\n",
        "display.display(display.Audio(test_background_noise_wav, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmiYtvxwPXd0"
      },
      "source": [
        "We will multiply the background noise soundbite with a random scalar before adding it to the original fire alarm soundbite. Then ensure the mixed up value is between the range of -1.0 and 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdzr8Hy9szgl"
      },
      "source": [
        "def add_audio_mixup(audio, mixup_audio):\n",
        "  # randomly generate a scalar\n",
        "  noise_scalar = tf.random.uniform(shape=[1], minval=0, maxval=1)\n",
        "\n",
        "  # add the background noise to the audio\n",
        "  augmented_audio = audio + (mixup_audio * noise_scalar)\n",
        "\n",
        "  #final clip the values so they are stil between -1 and 1\n",
        "  augmented_audio = tf.clip_by_value(augmented_audio, clip_value_min=-1, clip_value_max=1)\n",
        "\n",
        "  return augmented_audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FvodQe0Pu8w"
      },
      "source": [
        "Let's apply the audio mixup to the fire alarm sound and then plot it to compare. We can also listen to the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82af5IyPuqQU"
      },
      "source": [
        "test_fire_alarm_with_mixup_wav = add_audio_mixup(test_fire_alarm_wav, test_background_noise_wav)\n",
        "\n",
        "plt.plot(test_fire_alarm_with_mixup_wav)\n",
        "plt.ylim([-1, 1])\n",
        "plt.show()\n",
        "\n",
        "plot_spectrogram(get_arm_spectrogram(test_fire_alarm_with_mixup_wav), vmax=25)\n",
        "\n",
        "display.display(display.Audio(test_fire_alarm_with_mixup_wav, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntHK8oSHvl3N"
      },
      "source": [
        "### Create Augmented Dataset\n",
        "\n",
        "We can now combine all three augmententation techniques to balance our dataset.\n",
        "\n",
        "First let's calculate how many augmented files we need to generate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFrYCTPYL0Vs"
      },
      "source": [
        "num_augmented_fire_alarm_spectrograms = num_background_noise_spectrograms - num_fire_alarm_spectrograms\n",
        "\n",
        "print(f'num_augmented_fire_alarm_spectrograms = {num_augmented_fire_alarm_spectrograms}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RkWzf_qQSlG"
      },
      "source": [
        "Then we can divide by 3, to calculate how many augmented soundbites per technique to generate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJwqgWbNLCk"
      },
      "source": [
        "num_white_noise_fire_alarm_spectrograms = num_augmented_fire_alarm_spectrograms // 3\n",
        "num_random_silence_fire_alarm_spectrograms = num_augmented_fire_alarm_spectrograms // 3\n",
        "num_audio_mixup_fire_alarm_spectrograms = num_augmented_fire_alarm_spectrograms // 3\n",
        "\n",
        "print(f'num_white_noise_fire_alarm_spectrograms = {num_white_noise_fire_alarm_spectrograms}')\n",
        "print(f'num_random_silence_fire_alarm_spectrograms = {num_random_silence_fire_alarm_spectrograms}')\n",
        "print(f'num_audio_mixup_fire_alarm_spectrograms = {num_audio_mixup_fire_alarm_spectrograms}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y12lDIeNQe7e"
      },
      "source": [
        "Select and shuffle the number of soundbites required:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0_hV-Y8vrFx"
      },
      "source": [
        "split_fire_alarm_wav_ds = split_fire_alarm_wav_ds.cache()\n",
        "preaugmented_split_fire_alarm_wav = split_fire_alarm_wav_ds.shuffle(num_augmented_fire_alarm_spectrograms, seed=random_seed).take(num_augmented_fire_alarm_spectrograms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQKWXZSuQk2T"
      },
      "source": [
        "Create the white noise augmented soundbites:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBu718tfNxLY"
      },
      "source": [
        "def add_white_noise_for_map(wav, label, fold):\n",
        "  return add_white_noise(wav), label, fold\n",
        "\n",
        "white_noise_fire_alarm_wav_ds = preaugmented_split_fire_alarm_wav.take(num_white_noise_fire_alarm_spectrograms)\n",
        "white_noise_fire_alarm_wav_ds = white_noise_fire_alarm_wav_ds.map(add_white_noise_for_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK2pAnK-QqIq"
      },
      "source": [
        "Create the random noise augmented soundbites:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oKTJN6uPVIT"
      },
      "source": [
        "def add_random_silence_for_map(wav, label, fold):\n",
        "  return add_random_silence(wav), label, fold\n",
        "\n",
        "random_silence_fire_alarm_wav_ds = preaugmented_split_fire_alarm_wav.skip(num_white_noise_fire_alarm_spectrograms)\n",
        "random_silence_fire_alarm_wav_ds = random_silence_fire_alarm_wav_ds.take(num_random_silence_fire_alarm_spectrograms)\n",
        "random_silence_fire_alarm_wav_ds = random_silence_fire_alarm_wav_ds.map(add_random_silence_for_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmYVy-VwQx_J"
      },
      "source": [
        "Create the audio mixup augmented soundbites:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ0jfAJVQre7"
      },
      "source": [
        "audio_mixup_background_noise_ds = split_background_noise_wav_ds.shuffle(num_audio_mixup_fire_alarm_spectrograms).take(num_audio_mixup_fire_alarm_spectrograms)\n",
        "audio_mixup_background_noise_iter = iter(audio_mixup_background_noise_ds.map(lambda x, y, z: x))\n",
        "\n",
        "def add_audio_mixup_for_map(wav, label, fold):\n",
        "  return add_audio_mixup(wav, next(audio_mixup_background_noise_iter)), label, fold\n",
        "\n",
        "audio_mixup_split_fire_alarm_wav_ds = preaugmented_split_fire_alarm_wav.skip(num_white_noise_fire_alarm_spectrograms + num_random_silence_fire_alarm_spectrograms)\n",
        "audio_mixup_split_fire_alarm_wav_ds = audio_mixup_split_fire_alarm_wav_ds.take(num_audio_mixup_fire_alarm_spectrograms)\n",
        "audio_mixup_split_fire_alarm_wav_ds = audio_mixup_split_fire_alarm_wav_ds.map(add_audio_mixup_for_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx-yL-pdQ5KF"
      },
      "source": [
        "Combine all the augmented soundbites together and map them to their spectrogram representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N85bl1Xx1C4l"
      },
      "source": [
        "augment_split_fire_alarm_wav_ds = tf.data.Dataset.concatenate(white_noise_fire_alarm_wav_ds, random_silence_fire_alarm_wav_ds)\n",
        "augment_split_fire_alarm_wav_ds = tf.data.Dataset.concatenate(augment_split_fire_alarm_wav_ds, audio_mixup_split_fire_alarm_wav_ds)\n",
        "\n",
        "augment_fire_alarm_spectrograms_ds = augment_split_fire_alarm_wav_ds.map(create_arm_spectrogram_for_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD6zn_eWu-YS"
      },
      "source": [
        "### Split Dataset\n",
        "\n",
        "Now combine the spectrogram datasets, and split them into training, validation, and test sets. Instead of using the `fold` value to split them, we will shuffle all the items, and then split by percentage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmQxp4EtFiVn"
      },
      "source": [
        "full_ds = tf.data.Dataset.concatenate(fire_alarm_spectrograms_ds, background_noise_spectrograms_ds)\n",
        "full_ds = tf.data.Dataset.concatenate(full_ds, augment_fire_alarm_spectrograms_ds)\n",
        "full_ds = full_ds.cache()\n",
        "\n",
        "full_ds_size = calculate_ds_len(full_ds)\n",
        "\n",
        "print(f'full_ds_size = {full_ds_size}')\n",
        "\n",
        "full_ds = full_ds.shuffle(full_ds_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CltJjc1Dgjl"
      },
      "source": [
        "train_ds_size = int(0.60 * full_ds_size)\n",
        "val_ds_size = int(0.20 * full_ds_size)\n",
        "test_ds_size = int(0.20 * full_ds_size)\n",
        "\n",
        "train_ds = full_ds.take(train_ds_size)\n",
        "\n",
        "remaining_ds = full_ds.skip(train_ds_size)\n",
        "val_ds = remaining_ds.take(val_ds_size)\n",
        "test_ds = remaining_ds.skip(val_ds_size)\n",
        "\n",
        "# remove the folds column as it's no longer needed\n",
        "remove_fold_column = lambda spectrogram, label, fold: (tf.expand_dims(spectrogram, axis=-1), label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ffwTQt_vFAx"
      },
      "source": [
        "### Replace Baseline Model Classification Head and Train Model\n",
        "\n",
        "The model we previously trained on the ESC-50 dataset, predicted the presence of 50 sound types, and which resulted in the final dense layer of the model having 50 outputs. The new model we would like to create is a binary classifier, and needs to have a single output value.\n",
        "\n",
        "We will load the baseline model, and swap out the final dense layer to match our needs:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqY5I0QeGKAV"
      },
      "source": [
        "# we need a new head with one neuron.\n",
        "model_body = tf.keras.Model(inputs=baseline_model.input, outputs=baseline_model.layers[-2].output)\n",
        "\n",
        "classifier_head = tf.keras.layers.Dense(1, activation=\"sigmoid\")(model_body.output)\n",
        "\n",
        "fine_tune_model = tf.keras.Model(model_body.input, classifier_head)\n",
        "\n",
        "fine_tune_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Sla0atSL1G"
      },
      "source": [
        "To freeze a layer in TensorFlow we can set `layer.trainable = False`. Let's loop through all the layers and do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbcW4ANCGXUa"
      },
      "source": [
        "for layer in fine_tune_model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY9pZMYjSSHV"
      },
      "source": [
        "and now unfreeze the last layer (the head):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBdchlqSToY"
      },
      "source": [
        "fine_tune_model.layers[-1].trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuE1ONbpSWil"
      },
      "source": [
        "Then we can `compile` the model, this time with using a binary crossentropy loss function as this model contains a single output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkVzymnGOOV"
      },
      "source": [
        "METRICS = [\n",
        "      \"accuracy\",\n",
        "]\n",
        "\n",
        "fine_tune_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 10:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
        "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YblAlGy4SpRQ"
      },
      "source": [
        "Kick off training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoEcHNBcGqnT"
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "history_1 = fine_tune_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCWZ_SUkS95R"
      },
      "source": [
        "Now unfreeze all the layers, and train for a few more epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiJYDSOxe6uQ"
      },
      "source": [
        "for layer in fine_tune_model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "fine_tune_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  return lr * tf.math.exp(-0.1)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
        "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRq1N0qXIBSs"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history_2 = fine_tune_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOewhILQcemp"
      },
      "source": [
        "fine_tune_model.save(\"fine_tuned_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXENjCJJzyrO"
      },
      "source": [
        "## Training with your own audio (optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIIuAagAjoVq"
      },
      "source": [
        "We now have an ML model which can classify the presence of fire alarm sound. However this model was trained on publicly available sound recordings which might not match the sound characteristics of the hardware microphone we will use for inferencing.\n",
        "\n",
        "The Raspberry Pi RP2040 MCU has a native USB feature that allows it to act like a USB microphone. We can flash an application to the board to enable it to act like a USB microphone to our PC. Then we can extend Google Colab’s capabilities with the [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) on a modern Web browser like Google Chrome to collect live data samples all from within Google Colab!\n",
        "\n",
        "**If you don't have a fire alarm handy to record from, you can skip to the [next section](#scrollTo=Model_Optimization).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ajrgSMaBvI"
      },
      "source": [
        "### Record your own audio\n",
        "\n",
        "#### Software Setup\n",
        "\n",
        "Now we can use the USB microphone example from the [Microphone Library for Pico](https://github.com/ArmDeveloperEcosystem/microphone-library-for-pico). The example application can be compiled using `cmake` and `make`. Then we can flash the example application to the board over USB by putting the board into “boot ROM mode” which will allow us to upload an application to the board.\n",
        "\n",
        "Let's use `git` to clone the library source code and accompanying examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBsW1c5xaFwy"
      },
      "source": [
        "%%shell\n",
        "git clone https://github.com/ArmDeveloperEcosystem/microphone-library-for-pico.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ9vcp6ApjXN"
      },
      "source": [
        "Now let's change to the libraries directory folder, and create a `build` folder to run `cmake` on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R5fJC5HaMom"
      },
      "source": [
        "%%shell\n",
        "cd microphone-library-for-pico\n",
        "mkdir -p build\n",
        "cd build\n",
        "cmake .. -DPICO_BOARD=${PICO_BOARD}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr0Txtlip6iw"
      },
      "source": [
        "Then we can run `make` to compile the example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7ohkHZnp_Ns"
      },
      "source": [
        "%%shell\n",
        "cd microphone-library-for-pico/build\n",
        "\n",
        "make -j usb_microphone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9asx3qzuIvR"
      },
      "source": [
        "#### Flashing the board\n",
        "\n",
        "If you are using a [WebUSB API](https://wicg.github.io/webusb/) enabled browser like Google Chrome, you can directly flash the image onto the board from within Google Collab! (Otherwise, you can manually download the .uf2 file to your computer and then drag it onto the USB disk for the RP2040 board.)\n",
        "\n",
        "**Note for Windows**: If you are using Windows you must install WinUSB drivers in order to use WebUSB, you can do so by following the instructions found [here](https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico/blob/main/windows.md).\n",
        "\n",
        "**Note for Linux**: If you are using Linux you must configure udev in order to use WebUSB, you can do so by following the instructions found [here](https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico/blob/main/linux.md).\n",
        "\n",
        "First you must place the board in USB Boot ROM mode, as follows:\n",
        "\n",
        " * SparkFun MicroMod\n",
        "  * Plug the USB-C cable into the board and your PC to power the board\n",
        "  * While holding down the BOOT button on the board, tap the RESET button\n",
        " * Raspberry Pi Pico\n",
        "  * Plug the USB Micro cable into your PC, but do NOT plug in the Pico side.\n",
        "  * While holding down the white BOOTSEL button, plug in the micro USB cable to the Pico\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmtVMRLrvfvw"
      },
      "source": [
        "Run the code cell below and then click the \"Flash\" button to upload the USB microphone example application to the board over USB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhy9ddOXAZnY"
      },
      "source": [
        "from colab_utils.pico import flash_pico\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1GJDEYIwI0r"
      },
      "source": [
        "Now you can record audio by running the cells below, select the \"MicNode\" item from the drop down, and then click the \"Starting Recording\" button to start capturing audio. You must click the \"Stop Recording\" button to stop recording."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlpeIDxIxSIw"
      },
      "source": [
        "Record the your own fire alarm sounds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pgq9iZ7ybmt"
      },
      "source": [
        "from colab_utils.audio import record_wav_file\n",
        "\n",
        "os.makedirs('datasets/custom/fire_alarm', exist_ok=True)\n",
        "\n",
        "record_wav_file('datasets/custom/fire_alarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8nXgrdTyIfB"
      },
      "source": [
        "Record the your own background noise sounds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AH4-FQ3zSHV"
      },
      "source": [
        "os.makedirs('datasets/custom/background_noise', exist_ok=True)\n",
        "\n",
        "record_wav_file('datasets/custom/background_noise')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoQuE4jOzKoX"
      },
      "source": [
        "We can zip up the recorded wave files to download and use again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxsdP2uY0IpD"
      },
      "source": [
        "!zip -r custom.zip datasets/custom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqOLRYDT33Y2"
      },
      "source": [
        "### Load dataset\n",
        "\n",
        "We can load and transform the custom recorded dataset using the same pipeline we used before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8D_30220vlN"
      },
      "source": [
        "custom_fire_alarm_ds = tf.data.Dataset.list_files(\"datasets/custom/fire_alarm/*.wav\", shuffle=False)\n",
        "custom_fire_alarm_ds = custom_fire_alarm_ds.map(lambda x: (x, 1, -1))\n",
        "custom_fire_alarm_ds = custom_fire_alarm_ds.map(load_wav_for_map)\n",
        "custom_fire_alarm_ds = custom_fire_alarm_ds.flat_map(split_wav_for_flat_map)\n",
        "custom_fire_alarm_ds = custom_fire_alarm_ds.map(create_arm_spectrogram_for_map)\n",
        "\n",
        "custom_background_noise_ds = tf.data.Dataset.list_files(\"datasets/custom/background_noise/*.wav\", shuffle=False)\n",
        "custom_background_noise_ds = custom_background_noise_ds.map(lambda x: (x, 0, -1))\n",
        "custom_background_noise_ds = custom_background_noise_ds.map(load_wav_for_map)\n",
        "custom_background_noise_ds = custom_background_noise_ds.flat_map(split_wav_for_flat_map)\n",
        "custom_background_noise_ds = custom_background_noise_ds.map(create_arm_spectrogram_for_map)\n",
        "\n",
        "custom_ds = tf.data.Dataset.concatenate(custom_fire_alarm_ds, custom_background_noise_ds)\n",
        "custom_ds = custom_ds.map(lambda x, y, z: (tf.expand_dims(x, axis=-1), y, z))\n",
        "custom_ds_len = calculate_ds_len(custom_ds)\n",
        "\n",
        "print(f'{custom_ds_len}')\n",
        "\n",
        "custom_ds = custom_ds.map(lambda x, y,z: (x, y))\n",
        "\n",
        "custom_ds = custom_ds.shuffle(custom_ds_len).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H8L-1jp0MX2"
      },
      "source": [
        "Evaluate dataset performance before training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJDj7dD1PGfN"
      },
      "source": [
        "fine_tune_model.evaluate(custom_ds.batch(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DphMcfEf3_hR"
      },
      "source": [
        "### Fine tune model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp8R30Uc5FRc"
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "for layer in fine_tune_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "fine_tune_model.layers[-1].trainable = True\n",
        "\n",
        "fine_tune_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "history3 = fine_tune_model.fit(\n",
        "    custom_ds.take(int(custom_ds_len * 0.8)).batch(1),\n",
        "    validation_data=custom_ds.skip(int(custom_ds_len * 0.8)).batch(1),\n",
        "    epochs=EPOCHS,\n",
        "    # callbacks=callbacks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx0iguJePXHN"
      },
      "source": [
        "fine_tune_model.evaluate(custom_ds.batch(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alhP_RWTPkNl"
      },
      "source": [
        "fine_tune_model.save('fine_tuned_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HW-PdaT5Fx0"
      },
      "source": [
        "## Model Optimization\n",
        "\n",
        "To optimize the model to run on the Arm Cortex-M0+ processor, we will use a process called model quantization. Model quantization converts the model’s weights and biases from 32-bit floating-point values to 8-bit values. The [pico-tflmicro](https://github.com/raspberrypi/pico-tflmicro) library, which is a port of TFLu for the RP2040’s Pico SDK contains Arm’s CMSIS-NN library, which supports optimized kernel operations for quantized 8-bit weights on Arm Cortex-M processors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HRVOG9-5JsE"
      },
      "source": [
        "### Quantization Aware Training\n",
        "\n",
        "We can use [TensorFlow’s Quantization Aware Training (QAT)](https://www.tensorflow.org/model_optimization/guide/quantization/training) feature to easily convert the floating-point model to quantized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Keras version (Keras is part of TensorFlow)\n",
        "print(\"Keras version:\", tf.keras.__version__)\n"
      ],
      "metadata": {
        "id": "5-pqPEHfHGij",
        "outputId": "cf12ecfb-e737-4a35-9118-19aafb18b6cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.1\n",
            "Keras version: 3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "with h5py.File(\"model_gearbox.hdf5\", \"r\") as f:\n",
        "    # Print all keys in the file to inspect stored data\n",
        "    print(list(f.attrs.keys()))\n",
        "\n",
        "    # Try accessing specific attributes\n",
        "    if \"keras_version\" in f.attrs:\n",
        "        print(\"Keras version:\", f.attrs[\"keras_version\"])\n",
        "    if \"backend\" in f.attrs:\n",
        "        print(\"Keras backend:\", f.attrs[\"backend\"])\n"
      ],
      "metadata": {
        "id": "zKnhSjPdFDQJ",
        "outputId": "567d34f4-79ca-4a11-e754-1b8041db7539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['backend', 'keras_version', 'model_config', 'training_config']\n",
            "Keras version: 3.5.0\n",
            "Keras backend: tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyWDswSD5PKF",
        "outputId": "b6d0b275-8c1e-49fb-f10f-e7e253bd688f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_model = tf.keras.models.load_model(\"model_gearbox.hdf5\")\n",
        "\n",
        "final_model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m82,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │              \u001b[38;5;34m32\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m1,152\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)                 │          \u001b[38;5;34m82,560\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,560</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,994\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,994</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m267,928\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,928</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,064\u001b[0m (8.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> (8.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to annotate Dense layers for quantization\n",
        "def apply_qat_to_dense(layer):\n",
        "    # Check if the layer is a Keras Layer instance\n",
        "    if isinstance(layer, tf.keras.layers.Layer):\n",
        "        # Annotate Dense layers only\n",
        "        if isinstance(layer, tf.keras.layers.Dense):\n",
        "            return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "    return layer\n",
        "\n",
        "# Clone and annotate model\n",
        "annotated_model = tf.keras.models.clone_model(\n",
        "    final_model,  # Replace with your autoencoder model variable\n",
        "    clone_function=apply_qat_to_dense\n",
        ")\n",
        "\n",
        "# Apply quantization-aware training\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "quant_aware_model.summary()\n",
        "\n",
        "# Compile and train as before\n",
        "quant_aware_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']  # Replace 'accuracy' with your custom METRICS if applicable\n",
        ")\n",
        "\n",
        "# Train the quantized model\n",
        "EPOCHS = 1\n",
        "quant_aware_history = quant_aware_model.fit(\n",
        "    train_ds,  # Replace with your training dataset\n",
        "    validation_data=val_ds,  # Replace with your validation dataset\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "\n",
        "# Convert the model to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Define a representative dataset for quantization\n",
        "def representative_data_gen():\n",
        "    for input_value, _ in train_ds.unbatch().batch(1).take(100):\n",
        "        yield [input_value]\n",
        "\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Set TFLite target specification for int8 quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Convert and save the quantized TFLite model\n",
        "tflite_model_quant = converter.convert()\n",
        "with open(\"autoencoder_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_quant)\n"
      ],
      "metadata": {
        "id": "CCJq4Lg-P8Wk",
        "outputId": "b50f92e3-e25e-40ce-e791-2cc545192d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`to_annotate` can only be a `keras.layers.Layer` instance. You passed an instance of type: Dense.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5167e9caaafd>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Clone and annotate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m annotated_model = tf.keras.models.clone_model(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Replace with your autoencoder model variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mclone_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_qat_to_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function, call_function, recursive, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mclone_function\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         ):\n\u001b[0;32m--> 182\u001b[0;31m             return _clone_functional_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mclone_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, clone_function, input_tensors, call_function)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     output_tensors = model._run_through_graph(\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0moperation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/function.py\u001b[0m in \u001b[0;36m_run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcall_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36moperation_fn\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mnew_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36mwrapped_clone_function\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-5167e9caaafd>\u001b[0m in \u001b[0;36mapply_qat_to_dense\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Annotate Dense layers only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_annotate_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\u001b[0m in \u001b[0;36mquantize_annotate_layer\u001b[0;34m(to_annotate, quantize_config)\u001b[0m\n\u001b[1;32m    268\u001b[0m   if not isinstance(to_annotate, keras.layers.Layer) or isinstance(\n\u001b[1;32m    269\u001b[0m       to_annotate, keras.Model):\n\u001b[0;32m--> 270\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;34m'`to_annotate` can only be a `keras.layers.Layer` instance. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         'You passed an instance of type: {input}.'.format(\n",
            "\u001b[0;31mValueError\u001b[0m: `to_annotate` can only be a `keras.layers.Layer` instance. You passed an instance of type: Dense."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the HDF5 model\n",
        "autoencoder = tf.keras.models.load_model(\"model_gearbox.hdf5\")\n",
        "\n",
        "# Check model summary to confirm it's loaded correctly\n",
        "autoencoder.summary()\n"
      ],
      "metadata": {
        "id": "TaoiBwAFS09V",
        "outputId": "fc2fed95-7d85-4f0c-eb32-ecb52f219401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m82,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │              \u001b[38;5;34m32\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m1,152\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)                 │          \u001b[38;5;34m82,560\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,560</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,994\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,994</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m267,928\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,928</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,064\u001b[0m (8.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> (8.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to TensorFlow Lite with dynamic range quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert the model\n",
        "quantized_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"quantized_autoencoder_dynamic.tflite\", \"wb\") as f:\n",
        "    f.write(quantized_model)\n"
      ],
      "metadata": {
        "id": "qD_4LLK4TS1Z",
        "outputId": "12186d91-62e0-44a7-c1b7-7e0703212aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpupdrcknh'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 640), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 640), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136028054797504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050329792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050338592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050339824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050334016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050336304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136030871559104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050338064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050376128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050375424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050340880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050375248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050332080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050378768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050379120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050375600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050378416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050378064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050379824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050380352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050385104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050384928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050389680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050387392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050387040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050387216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050388448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026374096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026373392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026373040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026380080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026376736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026377440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026377616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026377264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026379904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026384656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026385888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026382016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026382192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026384480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026386944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026439280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026437696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026387824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026438576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026379376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026441392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Define a representative dataset function\n",
        "def representative_dataset():\n",
        "    for _ in range(100):  # Replace with your dataset\n",
        "        yield [np.random.rand(1, 28, 28, 1).astype(np.float32)]  # Use actual input shape of your model\n",
        "\n",
        "# Convert the model to TensorFlow Lite with full integer quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Ensure input/output tensors are also quantized\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Convert the model\n",
        "quantized_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"quantized_autoencoder_full_integer.tflite\", \"wb\") as f:\n",
        "    f.write(quantized_model)\n"
      ],
      "metadata": {
        "id": "rAa-l7k6TZEs",
        "outputId": "bac00c96-2d5a-4aec-863a-d8ba094e949d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpomx76_h1'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 640), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 640), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136028054797504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050329792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050338592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050339824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050334016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050336304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136030871559104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050338064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050376128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050375424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050340880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050375248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050332080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050378768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050379120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050375600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050378416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050378064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050379824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050380352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050385104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050382288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050384928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050389680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050387392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050387040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050387216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028050388448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026374096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026373392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026373040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026375328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026380080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026376736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026377440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026377616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026377264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026379904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026384656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026385888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026382016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026382192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026384480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026386944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026439280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026437696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026387824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026438576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026379376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028026441392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"quantized_autoencoder_full_integer.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Print details to verify\n",
        "print(\"Input details:\", input_details)\n",
        "print(\"Output details:\", output_details)\n"
      ],
      "metadata": {
        "id": "uWsRlD5XT4fI",
        "outputId": "9b61237a-67dd-4231-a3d1-60f2b56aba66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input details: [{'name': 'serving_default_input_layer:0', 'index': 0, 'shape': array([  1, 640], dtype=int32), 'shape_signature': array([ -1, 640], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921552561223507, 0), 'quantization_parameters': {'scales': array([0.00392155], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 32, 'shape': array([  1, 640], dtype=int32), 'shape_signature': array([ -1, 640], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.08831164985895157, 255), 'quantization_parameters': {'scales': array([0.08831165], dtype=float32), 'zero_points': array([255], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create test input with the correct shape and data type\n",
        "test_input = np.random.randint(0, 256, size=(1, 640), dtype=np.uint8)  # Random uint8 input\n",
        "\n",
        "\n",
        "# Set input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], test_input)\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get output tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Output data:\", output_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "7hoIGRIZT7HV",
        "outputId": "651c98fa-c535-4860-c5b2-5ca37241b8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output data: [[207 237 215 198 205 201 171 184 166 168 141 149 138 122 116 147 157 123\n",
            "  136 184 174 172 201 193 156 164 170 162 173 162 119 149 174 197 154 137\n",
            "  123 110 103  87  76 102  75  65  70  86 122 128 115  98  82  63  60  50\n",
            "   79 119 130 108 102  98  88  97 105 119 126  77  55  57  69  95 121 109\n",
            "  136 117 103 128 128 125 158 169 157 156 159 164 165 164 134 167 159 162\n",
            "  153 146 133 147 175 175 173 189 176 149 147 139 127 137 125 119 128 130\n",
            "  107  98  96 102 101  81  65  64  48  38  54  59  80  81  82  80  48  41\n",
            "   41  27 208 243 216 193 207 201 169 183 162 164 139 149 138 124 119 147\n",
            "  156 121 139 183 178 172 202 193 151 152 150 152 179 167 124 151 173 196\n",
            "  151 130 118 106  99  85  73  97  69  55  57  71 108 116 103  89  74  56\n",
            "   38  30  69 115 125  96  88  88  91  95  96 111 121  80  57  55  58  87\n",
            "  114  97 124 116 105 130 124 114 152 165 151 150 160 163 163 168 138 163\n",
            "  155 162 144 142 128 143 171 171 169 186 172 141 134 121 112 123 116 109\n",
            "  120 132 111  96  97 102 102  84  74  73  53  33  50  54  71  76  78  79\n",
            "   50  43  40  25 208 238 214 184 204 200 171 187 167 169 140 148 141 124\n",
            "  116 146 155 121 136 185 178 173 204 196 155 156 162 167 188 171 127 155\n",
            "  178 198 152 134 122 111 105  85  71  97  69  60  64  79 117 120  97  83\n",
            "   67  57  59  55  80 117 125 102  88  87  95 108 103 126 133  89  60  60\n",
            "   67 101 127 103 125 113 102 131 125 116 158 168 153 155 164 168 171 173\n",
            "  136 164 167 175 159 153 140 156 181 182 178 190 174 145 147 140 132 140\n",
            "  128 119 125 132 113 105 103 109 109  95  85  77  49  21  43  54  74  80\n",
            "   73  75  50  43  39  23 213 246 212 186 203 199 171 185 165 167 140 148\n",
            "  139 124 117 145 154 119 132 182 175 170 202 192 153 169 175 166 178 163\n",
            "  120 146 171 197 150 131 120 115 109  85  74  99  74  66  64  75 116 124\n",
            "  106  96  95  83  69  64  81 111 125 119 118 104 101 112  99 129 141 101\n",
            "   80  81  83 120 138 112 138 129 108 139 138 131 163 169 161 159 166 171\n",
            "  180 186 156 179 171 178 165 165 156 161 182 186 184 182 174 156 153 152\n",
            "  144 142 128 118 126 132 119 108 100 102 101  80  66  49  25  14  38  46\n",
            "   69  70  58  59  35  30  22   9 207 236 209 185 202 198 170 184 163 166\n",
            "  137 146 137 123 112 143 155 117 131 179 169 167 196 185 148 161 164 150\n",
            "  166 155 114 137 162 191 149 131 118 110 102  79  67  93  66  57  58  73\n",
            "  109 118 105  98  94  73  63  56  83 121 123 111 113 113 117 123 125 145\n",
            "  147 104  82  72  73 106 134 116 143 136 121 146 145 141 164 167 159 160\n",
            "  166 168 175 184 157 175 168 172 156 161 146 155 176 181 182 183 176 156\n",
            "  152 143 131 132 120 112 124 135 121 103  92  94  92  74  55  41  27  13\n",
            "   31  32  50  57  46  47  22  17  15   2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_dataset():\n",
        "    for i in range(len(data)):\n",
        "        yield [data[i:i+1]]  # Yield one batch at a time\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Convert the model\n",
        "quantized_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"fully_quantized_autoencoder.tflite\", \"wb\") as f:\n",
        "    f.write(quantized_model)\n"
      ],
      "metadata": {
        "id": "z3tNp7fySFBq",
        "outputId": "a7ceb052-ab88-4d4b-cc0c-a381487182d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpuhow662b'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 640), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 640), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136028056664224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056702112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056710912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056712144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056706336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056708624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056710736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056713552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054652352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054651824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056714608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054651648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028056703872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054654464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054654816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054652000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054654112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054653760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054655520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054656048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054660800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054658688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054658160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054658336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054657984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054660624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054665376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054666608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054662736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054662912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054665200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054659920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054717888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054719472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054717184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054716832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054719296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054719120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054723872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054720528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054721232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054721408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054721056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054723696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054728448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054729680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054725808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054725984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054732496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054730208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054730912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054722816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054731968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054782368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054723168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136028054785184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-e537859b35a9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Save the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \"\"\"\n\u001b[0;32m-> 1744\u001b[0;31m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1723\u001b[0m       )\n\u001b[1;32m   1724\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m         return super(TFLiteKerasModelConverterV2, self).convert(\n\u001b[0m\u001b[1;32m   1726\u001b[0m             \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m     return self._optimize_tflite_model(\n\u001b[0m\u001b[1;32m   1474\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quant_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_optimize_tflite_model\u001b[0;34m(self, model, quant_mode, debug_options, quant_io)\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0mq_allow_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allow_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0mq_variable_quantization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_mlir_variable_quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         model = self._quantize(\n\u001b[0m\u001b[1;32m   1128\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mq_in_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization, debug_options)\u001b[0m\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_calibrate_only\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       calibrated = calibrate_quantize.calibrate(\n\u001b[0m\u001b[1;32m    749\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mdataset_gen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mgenerates\u001b[0m \u001b[0mcalibration\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \"\"\"\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36m_feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0minitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-e537859b35a9>\u001b[0m in \u001b[0;36mrepresentative_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Yield one batch at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uWWv-Wd5Uvx",
        "outputId": "ca228d51-2d28-4153-9a9e-c52b8f0d79b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "def apply_qat_to_dense_and_cnn(layer):\n",
        "  if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  return layer\n",
        "\n",
        "#apply_qat_to_dense_and_cnn(tf.keras.layers.Dense(128))\n",
        "\n",
        "annotated_model = tf.keras.models.clone_model(\n",
        "    final_model,\n",
        "    clone_function=apply_qat_to_dense_and_cnn,\n",
        ")\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "quant_aware_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`to_annotate` can only be a `keras.layers.Layer` instance. You passed an instance of type: Dense.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-19a46c026991>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#apply_qat_to_dense_and_cnn(tf.keras.layers.Dense(128))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m annotated_model = tf.keras.models.clone_model(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mclone_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_qat_to_dense_and_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function, call_function, recursive, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mclone_function\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         ):\n\u001b[0;32m--> 182\u001b[0;31m             return _clone_functional_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mclone_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, clone_function, input_tensors, call_function)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     output_tensors = model._run_through_graph(\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0moperation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/function.py\u001b[0m in \u001b[0;36m_run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcall_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36moperation_fn\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mnew_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36mwrapped_clone_function\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-19a46c026991>\u001b[0m in \u001b[0;36mapply_qat_to_dense_and_cnn\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_qat_to_dense_and_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_annotate_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\u001b[0m in \u001b[0;36mquantize_annotate_layer\u001b[0;34m(to_annotate, quantize_config)\u001b[0m\n\u001b[1;32m    268\u001b[0m   if not isinstance(to_annotate, keras.layers.Layer) or isinstance(\n\u001b[1;32m    269\u001b[0m       to_annotate, keras.Model):\n\u001b[0;32m--> 270\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;34m'`to_annotate` can only be a `keras.layers.Layer` instance. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         'You passed an instance of type: {input}.'.format(\n",
            "\u001b[0;31mValueError\u001b[0m: `to_annotate` can only be a `keras.layers.Layer` instance. You passed an instance of type: Dense."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "# Define the function to annotate layers for QAT\n",
        "def apply_qat_to_dense_and_cnn(layer):\n",
        "    if isinstance(layer, tf.keras.layers.Layer) and not isinstance(layer, tf.keras.Model):\n",
        "        if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
        "            return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "    return layer\n",
        "\n",
        "# Clone and annotate the model\n",
        "annotated_model = tf.keras.models.clone_model(\n",
        "    final_model,\n",
        "    clone_function=apply_qat_to_dense_and_cnn,\n",
        ")\n",
        "\n",
        "# Apply QAT\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "# Compile the quantized model\n",
        "quant_aware_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Summarize the quantized model\n",
        "quant_aware_model.summary()\n"
      ],
      "metadata": {
        "id": "Ma5WmVV3PCoH",
        "outputId": "5d735ad7-8b7d-4eac-e86d-a6942ad048da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`to_annotate` can only be a `keras.layers.Layer` instance. You passed an instance of type: Dense.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-54a0f863e5fb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Clone and annotate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m annotated_model = tf.keras.models.clone_model(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclone_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_qat_to_dense_and_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function, call_function, recursive, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mclone_function\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         ):\n\u001b[0;32m--> 182\u001b[0;31m             return _clone_functional_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mclone_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, clone_function, input_tensors, call_function)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     output_tensors = model._run_through_graph(\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0moperation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/function.py\u001b[0m in \u001b[0;36m_run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcall_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36moperation_fn\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mnew_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/cloning.py\u001b[0m in \u001b[0;36mwrapped_clone_function\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-54a0f863e5fb>\u001b[0m in \u001b[0;36mapply_qat_to_dense_and_cnn\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_annotate_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\u001b[0m in \u001b[0;36mquantize_annotate_layer\u001b[0;34m(to_annotate, quantize_config)\u001b[0m\n\u001b[1;32m    268\u001b[0m   if not isinstance(to_annotate, keras.layers.Layer) or isinstance(\n\u001b[1;32m    269\u001b[0m       to_annotate, keras.Model):\n\u001b[0;32m--> 270\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;34m'`to_annotate` can only be a `keras.layers.Layer` instance. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         'You passed an instance of type: {input}.'.format(\n",
            "\u001b[0;31mValueError\u001b[0m: `to_annotate` can only be a `keras.layers.Layer` instance. You passed an instance of type: Dense."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmHPDwpI5g29"
      },
      "source": [
        "quant_aware_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "EPOCHS=1\n",
        "quant_aware_history = quant_aware_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rINV6dEU5sqp"
      },
      "source": [
        "### Saving model in TFLite format\n",
        "\n",
        "We will now use the [tf.lite.TFLiteConverter.from_keras_model(...)](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#from_keras_model) API to convert the quantized Keras model to TF Lite format, and then save it to disk as a `.tflite` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRuuJi145r8r"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "def representative_data_gen():\n",
        "  for input_value, output_value in train_ds.unbatch().batch(1).take(100):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield [input_value]\n",
        "\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_model_quant = converter.convert()\n",
        "\n",
        "with open(\"tflite_model.tflite\", \"wb\") as f:\n",
        "  f.write(tflite_model_quant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StbdFxpk6D_8"
      },
      "source": [
        "### Test TF Lite model\n",
        "\n",
        "Since TensorFlow also supports loading TF Lite models using [`tensorflow.lite`](https://www.tensorflow.org/api_docs/python/tf/lite), we can also verify the functionality of the quantized model and compare its accuracy with the regular unquantized model inside the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZkEUTBr6Hd-"
      },
      "source": [
        "import tensorflow.lite as tflite\n",
        "\n",
        "# Load the interpreter and allocate tensors\n",
        "interpreter = tflite.Interpreter(\"quantized_autoencoder_full_integer.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Load input and output details\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "# Set quantization values\n",
        "input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "output_scale, output_zero_point = output_details[\"quantization\"]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLw9Yy7w6bwy",
        "outputId": "aed53155-78c4-45de-b4d1-15a629e926e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Calculate the number of correct predictions\n",
        "correct = 0\n",
        "test_ds_len = 0\n",
        "\n",
        "# Loop through entire test set\n",
        "for x, y in test_ds.unbatch():\n",
        "  # original shape is [124, 129, 1] expand to [1, 124, 129, 1]\n",
        "  x = tf.expand_dims(x, 0).numpy()\n",
        "\n",
        "  # quantize the input value\n",
        "  if (input_scale, input_zero_point) != (0, 0):\n",
        "    x = x / input_scale + input_zero_point\n",
        "  x = x.astype(input_details['dtype'])\n",
        "\n",
        "  # add the input tensor to interpreter\n",
        "  interpreter.set_tensor(input_details[\"index\"], x)\n",
        "\n",
        "  #run the model\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Get output data from model and convert to fp32\n",
        "  output_data = interpreter.get_tensor(output_details[\"index\"])\n",
        "  output_data = output_data.astype(np.float32)\n",
        "\n",
        "  # Dequantize the output\n",
        "  if (output_scale, output_zero_point) != (0.0, 0):\n",
        "    output_data = (output_data - output_zero_point) * output_scale\n",
        "\n",
        "  # convert output to category\n",
        "  if output_data[0][0] >= 0.5:\n",
        "    category = 1\n",
        "  else:\n",
        "    category = 0\n",
        "\n",
        "  # add 1 if category = y\n",
        "  correct += 1 if category == y.numpy() else 0\n",
        "\n",
        "  test_ds_len += 1"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-455ad16adb7d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Loop through entire test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m# original shape is [124, 129, 1] expand to [1, 124, 129, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R38f75_p61se"
      },
      "source": [
        "accuracy = correct / test_ds_len\n",
        "print(f\"Accuracy for quantized model is {accuracy*100:.2f}% (to 2 D.P) on test set.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exRvsfKz7JPR"
      },
      "source": [
        "## Deploy on Device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcWf2CCK_dst"
      },
      "source": [
        "#### Convert `.tflite` to `.h` file\n",
        "\n",
        "The RP2040 MCU on the boards we are deploying to, does not have a built-in file system, which means we cannot use the .tflite file directly on the board. However, we can use the Linux `xxd` command to convert the .tflite file to a .h file which can then be compiled in the inference application in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWBsWuIN7MHx",
        "outputId": "f1c136ab-ed12-41cc-a36d-89a663bdfdbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "echo \"alignas(8) const unsigned char tflite_model[] = {\" > tflite_model.h\n",
        "cat quantized_autoencoder_full_integer.tflite | xxd -i >> tflite_model.h\n",
        "echo \"};\"                                               >> tflite_model.h"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x14EFzyN_mk_"
      },
      "source": [
        "#### Inference Application\n",
        "\n",
        "We now have a model that is ready to be deployed to the device! We’ve created an application template for inference which can be compiled with the .h file that we’ve generated for the model.\n",
        "\n",
        "The C++ application uses the `pico-sdk` as the base, along with the `CMSIS-DSP`, `pico-tflmicro`, and `Microphone Libary for Pico` libraries. It’s general structure is as follows:\n",
        " 1. Initialization\n",
        "  1. Configure the board's built-in LED for output. The application will map the brightness of the LED to the output of the model. (0.0 LED off, 1.0 LED on with full brightness)\n",
        "  1. Setup the TF Lite library and TF Lite model for inference\n",
        "  1. Setup the CMSIS-DSP based DSP pipeline\n",
        "  1. Setup and start the microphone for real-time audio\n",
        " 1. Inference loop\n",
        "  1. Wait for 128 * 4 = 512 new audio samples from the microphone\n",
        "  1. Shift the spectrogram array over by 4 columns\n",
        "  1. Shift the audio input buffer over by 128 * 4 = 512 samples and copy in the new samples\n",
        "  1. Calculate 4 new spectrogram columns for the updated input buffer\n",
        "  1. Perform inference on the spectrogram data\n",
        "  1. Map the inference output value to the on-board LED’s brightness and output the status to the USB port\n",
        "\n",
        "In-order to run in real-time each cycle of the inference loop must take under (512 / 16000) = 0.032 seconds or 32 milliseconds. The model we’ve trained and converted takes 24 ms for inference, which gives us ~8 ms for the other operations in the loop.\n",
        "\n",
        "128 was used above to match the stride of 128 used in the training pipeline for the spectrogram. We used a shift of 4 in the spectrogram to fit within the real-time constraints we had.\n",
        "\n",
        "The source code for the inference application can be found on GitHub: https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico/tree/main/inference-app\n",
        "\n",
        "**Note:** We have already cloned this project in the setup steps from earlier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbJTbrhI2Lm7"
      },
      "source": [
        "Now we can copy the updated `tflite_model.h` file over:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieVMgugZ-k5L"
      },
      "source": [
        "!cp tflite_model.h inference-app/src/tflite_model.h"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NehBUZd_ydi"
      },
      "source": [
        "#### Compile Inference Application\n",
        "\n",
        "Once again we can use `cmake` to setup project before compiling it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPswlG9u_AfN",
        "outputId": "9a6c1706-c7af-42c4-a496-764c3724ff5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "cd inference-app\n",
        "mkdir -p build\n",
        "cd build\n",
        "cmake .. -DPICO_BOARD=${PICO_BOARD}"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mPICO_SDK_PATH is /content/pico-sdk\u001b[0m\n",
            "\u001b[0mPICO platform is rp2040.\u001b[0m\n",
            "\u001b[0mUsing PICO_BOARD from environment ('pico')\u001b[0m\n",
            "\u001b[0mUsing board configuration from /content/pico-sdk/src/boards/include/boards/pico.h\u001b[0m\n",
            "\u001b[0mTinyUSB available at /content/pico-sdk/lib/tinyusb/src/portable/raspberrypi/rp2040; adding USB support.\u001b[0m\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/inference-app/build\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m89TzMy2r01"
      },
      "source": [
        "Then use `make` to compile it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm9D9Bsv2mre",
        "outputId": "6b6f6a9d-4334-44fc-d1d9-b1259bd99d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "cd inference-app/build\n",
        "\n",
        "make -j"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m\u001b[1mScanning dependencies of target bs2_default\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target CMSISDSPComplexMath\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target CMSISDSPBasicMath\u001b[0m\n",
            "[  0%] \u001b[34m\u001b[1mPerforming build step for 'ELF2UF2Build'\u001b[0m\n",
            "[  0%] Built target bs2_default\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target CMSISDSPFastMath\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target CMSISDSPTransform\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target CMSISDSPCommon\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target CMSISDSPSupport\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target elf2uf2\u001b[0m\n",
            "[  0%] Built target CMSISDSPComplexMath\n",
            "[100%] Built target elf2uf2\n",
            "[  0%] Built target CMSISDSPBasicMath\n",
            "[  0%] Built target CMSISDSPTransform\n",
            "[  0%] Built target CMSISDSPFastMath\n",
            "[  0%] Built target CMSISDSPCommon\n",
            "[  0%] \u001b[34m\u001b[1mNo install step for 'ELF2UF2Build'\u001b[0m\n",
            "[  0%] \u001b[34m\u001b[1mPerforming build step for 'PioasmBuild'\u001b[0m\n",
            "[  0%] Built target CMSISDSPSupport\n",
            "[  0%] Built target bs2_default_padded_checksummed_asm\n",
            "[  0%] \u001b[34m\u001b[1mCompleted 'ELF2UF2Build'\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target pico-tflmicro\u001b[0m\n",
            "[  0%] Built target ELF2UF2Build\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target pioasm\u001b[0m\n",
            "[100%] Built target pioasm\n",
            "[  0%] \u001b[34m\u001b[1mNo install step for 'PioasmBuild'\u001b[0m\n",
            "[  0%] \u001b[34m\u001b[1mCompleted 'PioasmBuild'\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target pico-tflmicro\u001b[0m\n",
            "[  0%] Built target PioasmBuild\n",
            "[  0%] Built target pico_pdm_microphone_pdm_microphone_pio_h\n",
            "[ 66%] Built target pico-tflmicro\n",
            "\u001b[35m\u001b[1mScanning dependencies of target pico_inference_app\u001b[0m\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target pico_inference_app\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/pico_inference_app.dir/src/main.cpp.obj\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable pico_inference_app.elf\u001b[0m\n",
            "[100%] Built target pico_inference_app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PIvWvzJABRb"
      },
      "source": [
        "#### Flash inferencing application to board\n",
        "\n",
        "You’ll need to put the board into USB boot ROM mode again to load the new application to it. If you are using a WebUSB API enabled browser like Google Chrome, you can directly flash the image onto the board from within Google Collab! Otherwise, you can manually download the .uf2 file to your computer and then drag it onto the USB disk for the RP2040 board.\n",
        "\n",
        "**Note for Windows**: If you are using Windows you must install WinUSB drivers in order to use WebUSB, you can do so by following the instructions found [here](https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico/blob/main/windows.md).\n",
        "\n",
        "**Note for Linux**: If you are using Linux you must configure udev in order to use WebUSB, you can do so by following the instructions found [here](https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico/blob/main/linux.md).\n",
        "\n",
        " * SparkFun MicroMod\n",
        "  * Plug the USB-C cable into the board and your PC to power the board\n",
        "  * While holding down the BOOT button on the board, tap the RESET button\n",
        "\n",
        " * Raspberry Pi Pico\n",
        "  * Plug the USB Micro cable into your PC, but do NOT plug in the Pico side.\n",
        "  * While holding down the white BOOTSEL button, plug in the micro USB cable to the Pico\n",
        "\n",
        "\n",
        "Then run the code cell below and click the \"flash\" button."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsWtAuAiAFYc",
        "outputId": "a9a1162f-4f1e-4ac7-dc5f-e45d5ff93193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from colab_utils.pico import flash_pico\n",
        "\n",
        "flash_pico('inference-app/build/pico_inference_app.bin')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    const picotoolJsScript = document.createElement(\"script\");\n",
              "\n",
              "    picotoolJsScript.src = \"https://armdeveloperecosystem.github.io/picotool.js/src/picotool.js\";\n",
              "    picotoolJsScript.type = \"text/javascript\";\n",
              "\n",
              "    document.body.append(picotoolJsScript);\n",
              "\n",
              "    async function readBinary() {\n",
              "      const result = await google.colab.kernel.invokeFunction('notebook.read_binary_base64', ['inference-app/build/pico_inference_app.bin']);\n",
              "      let str = result.data['text/plain'];\n",
              "\n",
              "      str = str.substring(1, str.length - 1);\n",
              "\n",
              "      return Uint8Array.from(atob(str), c => c.charCodeAt(0));\n",
              "    }\n",
              "\n",
              "    async function flashPico() {\n",
              "      statusDiv.innerHTML = '';\n",
              "\n",
              "      try {\n",
              "        statusDiv.innerHTML += \"requesting device ... <br/>\";\n",
              "        const usbDevice = await PicoTool.requestDevice();\n",
              "\n",
              "        const fileData = (await readBinary()).buffer;\n",
              "        const writeData = new ArrayBuffer(PicoTool.FLASH_SECTOR_ERASE_SIZE);\n",
              "    \n",
              "        const srcDataView = new DataView(fileData);\n",
              "        const dstDataView = new DataView(writeData);\n",
              "\n",
              "        const picoTool = new PicoTool(usbDevice);\n",
              "    \n",
              "        statusDiv.innerHTML += \"opening device ... <br/>\";\n",
              "        await picoTool.open();\n",
              "      \n",
              "        // statusDiv.innerHTML += \"reset ... <br/>\";\n",
              "        await picoTool.reset();\n",
              "      \n",
              "        // statusDiv.innerHTML += \"exlusive access device ... <br/>\";\n",
              "        await picoTool.exlusiveAccess(1);\n",
              "      \n",
              "        // statusDiv.innerHTML += \"exit xip ... <br/>\";\n",
              "        await picoTool.exitXip();\n",
              "\n",
              "        for (let i = 0; i < fileData.byteLength; i += PicoTool.FLASH_SECTOR_ERASE_SIZE) {\n",
              "          let j = 0;\n",
              "          for (j = 0; j < PicoTool.FLASH_SECTOR_ERASE_SIZE && (i + j) < fileData.byteLength; j++) {\n",
              "            dstDataView.setUint8(j, srcDataView.getUint8(i + j));\n",
              "          }\n",
              "\n",
              "          statusDiv.innerHTML += \"erasing ... \";\n",
              "          await picoTool.flashErase(PicoTool.FLASH_START + i, PicoTool.FLASH_SECTOR_ERASE_SIZE);\n",
              "\n",
              "          statusDiv.innerHTML += \"writing ... \";\n",
              "          await picoTool.write(PicoTool.FLASH_START + i, writeData);\n",
              "\n",
              "          statusDiv.innerHTML += \" \" + ((i + j) * 100 / fileData.byteLength).toFixed(2) + \"% <br/>\";\n",
              "        }\n",
              "\n",
              "        statusDiv.innerHTML += \"rebooting device ... <br/>\";\n",
              "        await picoTool.reboot(0, PicoTool.SRAM_END, 512);\n",
              "    \n",
              "        statusDiv.innerHTML += \"closing device ... <br/>\";\n",
              "        await picoTool.close();\n",
              "      } catch (e) {\n",
              "        statusDiv.innerHTML = 'Error: ' + e.message;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    let statusDiv = undefined; \n",
              "\n",
              "    if ('usb' in navigator) {\n",
              "      const flashButton = document.createElement(\"button\");\n",
              "\n",
              "      flashButton.innerHTML = \"Flash\";\n",
              "      flashButton.onclick = flashPico;\n",
              "\n",
              "      document.querySelector(\"#output-area\").appendChild(flashButton);\n",
              "\n",
              "      statusDiv = document.createElement(\"div\");\n",
              "      statusDiv.style = \"margin: 5px\";\n",
              "\n",
              "      document.querySelector(\"#output-area\").appendChild(statusDiv);\n",
              "    } else {\n",
              "      document.querySelector(\"#output-area\").appendChild(document.createTextNode(\n",
              "        \"Oh no! Your browser does not support WebUSB!\"\n",
              "      ));\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qFp9s1I0P7G"
      },
      "source": [
        "### Monitoring the Inference on the board\n",
        "\n",
        "\n",
        "Now that the inference application is running on the board you can observe it in action in two ways:\n",
        "\n",
        " 1. Visually by observing the brightness of the LED on the board. It should remain off or dim when no fire alarm sound is present - and be on when a fire alarm sound is present.\n",
        "\n",
        " 1. Connecting to the board’s USB serial port to view output from the inference application. If you are using a [Web Serial API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Serial_API) enabled browser like Google Chrome, this can be done directly from Google Colab!\n",
        "\n",
        "#### Test Audio\n",
        "\n",
        "Use the code cell below to playback the fire alarms sounds used during training from your computer. You may need to adjust the speaker volume from your computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gqmp3nPtp6P"
      },
      "source": [
        "for wav, _, _ in fire_alarm_wav_ds:\n",
        "  display.display(display.Audio(wav, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ufVilt4BKU"
      },
      "source": [
        "#### Serial Monitor\n",
        "\n",
        "Run the code cell below and then click the \"Connect Port\" button to view the serial output from the board:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAvIj39m3fUa",
        "outputId": "8e2ff258-f399-4e11-d8ba-fd0fe10e6a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "from colab_utils.serial_monitor import run_serial_monitor\n",
        "\n",
        "run_serial_monitor()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    if ('serial' in navigator) {\n",
              "      const scriptElement = document.createElement(\"script\");\n",
              "      scriptElement.src = \"https://cdnjs.cloudflare.com/ajax/libs/xterm/3.14.5/xterm.min.js\";\n",
              "      document.body.appendChild(scriptElement);\n",
              "\n",
              "      const linkElement = document.createElement(\"link\");\n",
              "      linkElement.rel = \"stylesheet\"\n",
              "      linkElement.href = \"https://cdnjs.cloudflare.com/ajax/libs/xterm/3.14.5/xterm.min.css\";\n",
              "      document.body.appendChild(linkElement);\n",
              "\n",
              "      const connectDisconnectButton = document.createElement(\"button\");\n",
              "\n",
              "      connectDisconnectButton.innerHTML = \"Connect Port\";\n",
              "\n",
              "      document.querySelector(\"#output-area\").appendChild(connectDisconnectButton);\n",
              "\n",
              "      terminalDiv = document.createElement(\"div\");\n",
              "      terminalDiv.style = \"margin: 5px\";\n",
              "\n",
              "      document.querySelector(\"#output-area\").appendChild(terminalDiv);\n",
              "\n",
              "      let port = undefined;\n",
              "      let reader = undefined;\n",
              "      let keepReading = true;\n",
              "      let term = undefined;\n",
              "\n",
              "      connectDisconnectButton.onclick = async () => {\n",
              "        if (port !== undefined) {\n",
              "          if (reader !== undefined) {\n",
              "            keepReading = false;\n",
              "            try {\n",
              "              await reader.cancel();\n",
              "            } catch (e) {}\n",
              "          }\n",
              "          port = undefined;\n",
              "          reader = undefined;\n",
              "\n",
              "          connectDisconnectButton.innerHTML = \"Connect Port\";\n",
              "\n",
              "          return;\n",
              "        }\n",
              "\n",
              "        port = await navigator.serial.requestPort();\n",
              "        keepReading = true;\n",
              "\n",
              "        connectDisconnectButton.innerHTML = \"Disconnect Port\";\n",
              "        \n",
              "        await port.open({ baudRate: 115200 });\n",
              "\n",
              "        if (term === undefined) {\n",
              "          term = new Terminal();\n",
              "          term.open(terminalDiv);\n",
              "        }\n",
              "        term.clear();\n",
              "    \n",
              "        const decoder = new TextDecoder();\n",
              "\n",
              "        while (port && keepReading) {\n",
              "          try {\n",
              "            reader = port.readable.getReader();\n",
              "          \n",
              "            while (true) {\n",
              "              const { value, done } = await reader.read();\n",
              "              if (done) {\n",
              "                keepReading = false;\n",
              "                break;\n",
              "              }\n",
              "\n",
              "              term.write(decoder.decode(value, { stream: true }));\n",
              "            }\n",
              "          } catch (error) {\n",
              "            keepReading = false;\n",
              "          } finally {\n",
              "            await reader.releaseLock();\n",
              "          }\n",
              "        }\n",
              "        \n",
              "        await port.close();\n",
              "\n",
              "        port = undefined;\n",
              "        reader = undefined;\n",
              "\n",
              "        connectDisconnectButton.innerHTML = \"Connect Port\";\n",
              "      };\n",
              "    } else {\n",
              "      document.querySelector(\"#output-area\").appendChild(document.createTextNode(\n",
              "        \"Oh no! Your browser does not support Web Serial!\"\n",
              "      ));\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoQzVwGA41Eq"
      },
      "source": [
        "## Improving the model\n",
        "\n",
        "You now have the first version of the model deployed to the board, and it is performing inference on live 16,000 kHz audio data!\n",
        "\n",
        "Test out various sounds to see if the model has the expected output. Maybe the fire alarm sound is being falsely detected (false positive) or not detected when it should be (false negative).\n",
        "\n",
        "If this occurs, you can record more new audio data for the scenario(s) by flashing the USB microphone application firmware to the board, recording the data for training, re-training the model and converting to TF lite format, and re-compiling + flashing the inference application to the board.\n",
        "\n",
        "Supervised machine learning models can generally only be as good as the training data they are trained with, so additional training data for these scenarios might help. You can also try to experiment with changing the model architecture or feature extraction process - but keep in mind that your model must be small enough and fast enough to run on the RP2040 MCU!\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This guide covered an end-to-end flow of how to train a custom audio classifier model to run locally on a development board that uses an Arm Cortex-M0+ processor. Google Colab was used to train the model using Transfer Learning techniques along with a smaller dataset and data augmentation techniques. We also collected our own data from the microphone that is used at inference time by loading an USB microphone application to the board, and extending Colab’s features with the Web Audio API and custom JavaScript\n",
        "\n",
        "The training side of the project combined Google’s Colab service and Chrome browser, with the open source TensorFlow library. The inference application captured audio data from a digital microphone, used Arm’s CMSIS-DSP library for the feature extraction stage, then used TensorFlow Lite for Microcontrollers with Arm CMSIS-NN accelerated kernels to perform inference with a 8-bit quantized model that classified a real-time 16 kHz audio input on an Arm Cortex-M0+ processor.\n",
        "\n",
        "The Web Audio API, Web USB API, and Web Serial API features of Google Chrome were used to extend Google Colab’s functionality to interact with the development board. This allowed us to experiment with and develop our application entirely with a web browser and deploy it to a constrained development board for on-device inference.\n",
        "\n",
        "Since the ML processing was performed on the development boards RP2040 MCU, privacy was preserved as no raw audio data left the device at inference time.\n",
        "\n"
      ]
    }
  ]
}