{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb83258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment output will be saved to: soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold # GroupKFold for robust CV if needed later, but main split is fixed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import train_models\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Notebook Specific Configurations ---\n",
    "INPUT_FILE = 'New_Features_Added_ALL.csv' # Make sure this file is generated by process_data.py\n",
    "TARGET_COL = 'SOH_cycle_capacity_%'\n",
    "\n",
    "# Define fixed numbers for battery splits\n",
    "N_TEST_BATTERIES = 5\n",
    "N_VAL_BATTERIES = 2\n",
    "SPLIT_RANDOM_SEED = 2024\n",
    "\n",
    "EXTRA_EXCLUDE_COLS_FROM_FEATURES = [\n",
    "    'capacity_Ah', 'energy_Wh',\n",
    "    'capacity_Ah_roll_mean_3', 'capacity_Ah_roll_std_3', 'capacity_Ah_diff_3',\n",
    "    'capacity_Ah_roll_mean_5', 'capacity_Ah_roll_std_5', 'capacity_Ah_diff_5',\n",
    "    'capacity_Ah_roll_mean_10', 'capacity_Ah_roll_std_10', 'capacity_Ah_diff_10',\n",
    "    'energy_Wh_roll_mean_3', 'energy_Wh_roll_std_3', 'energy_Wh_diff_3',\n",
    "    'energy_Wh_roll_mean_5', 'energy_Wh_roll_std_5', 'energy_Wh_diff_5',\n",
    "    'energy_Wh_roll_mean_10', 'energy_Wh_roll_std_10', 'energy_Wh_diff_10',\n",
    "    'SOH_cycle_capacity_%_roll_mean_3', 'SOH_cycle_capacity_%_roll_std_3', 'SOH_cycle_capacity_%_diff_3',\n",
    "    'SOH_cycle_capacity_%_roll_mean_5', 'SOH_cycle_capacity_%_roll_std_5', 'SOH_cycle_capacity_%_diff_5',\n",
    "    'SOH_cycle_capacity_%_roll_mean_10', 'SOH_cycle_capacity_%_roll_std_10', 'SOH_cycle_capacity_%_diff_10'\n",
    "]\n",
    "\n",
    "\n",
    "MODELS_TO_TRAIN = ['rf', 'gb', 'xgb', 'lr'] # Models to tune and train\n",
    "SAVE_ARTIFACTS = True\n",
    "EXPERIMENT_NAME = \"SOH_Prediction_FixedSplit_Run3\" # Give a descriptive name\n",
    "BASE_OUTPUT_DIR = \"soh_prediction_outputs\"\n",
    "EXPERIMENT_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "os.makedirs(EXPERIMENT_OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Experiment output will be saved to: {EXPERIMENT_OUTPUT_DIR}\")\n",
    "\n",
    "# Define battery regime mapping\n",
    "BATTERY_REGIME_MAP = {\n",
    "    'battery00': 'regular_constant', 'battery01': 'regular_constant',\n",
    "    'battery10': 'regular_constant', 'battery11': 'regular_constant',\n",
    "    'battery20': 'regular_constant', 'battery21': 'regular_constant',\n",
    "    'battery30': 'regular_constant', 'battery31': 'regular_constant',\n",
    "    'battery40': 'regular_constant', 'battery50': 'regular_constant',\n",
    "    'battery22': 'regular_variable', 'battery23': 'regular_variable',\n",
    "    'battery41': 'regular_variable', 'battery51': 'regular_variable',\n",
    "    'battery52': 'regular_variable',\n",
    "    'battery02': 'recommissioned_two_stage', 'battery12': 'recommissioned_two_stage',\n",
    "    'battery24': 'recommissioned_two_stage', 'battery32': 'recommissioned_two_stage',\n",
    "    'battery53': 'recommissioned_two_stage',\n",
    "    'battery03': 'recommissioned_three_stage', 'battery25': 'recommissioned_three_stage',\n",
    "    'battery33': 'recommissioned_three_stage',\n",
    "}\n",
    "ALL_REGIMES = sorted(list(set(BATTERY_REGIME_MAP.values())))\n",
    "\n",
    "# --- GridSearchCV Configurations ---\n",
    "CV_FOLDS_GRIDSEARCH = 3 # Folds for inner CV of GridSearchCV\n",
    "GRIDSEARCH_SCORING = 'neg_mean_squared_error' # Or 'r2', 'neg_mean_absolute_error'\n",
    "\n",
    "PARAM_GRIDS = {\n",
    "    'rf': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [10, 20, 25],\n",
    "        'min_samples_split': [5, 10]\n",
    "    },\n",
    "    'gb': {\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_iter': [150, 200, 250],\n",
    "        'max_depth': [4, 5, 7]\n",
    "    },\n",
    "    'xgb': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [5, 7, 9]\n",
    "    },\n",
    "    'lr': {\n",
    "        'fit_intercept': [True]\n",
    "    }\n",
    "}\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab618ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not using train_models.py module, define functions here:\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from the specified CSV file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully from {filepath}. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df, target_col_name, current_extra_exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Prepares data for training:\n",
    "    1. Drops rows with NaN in the target column.\n",
    "    2. Separates features (X) and target (y).\n",
    "    3. Selects features, converts types if necessary.\n",
    "    `current_extra_exclude_cols` is a list of additional columns to remove from features.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"Error: Input DataFrame to preprocess_data is None or empty.\")\n",
    "        return None, None, None\n",
    "        \n",
    "    df_cleaned = df.dropna(subset=[target_col_name]).copy()\n",
    "    if df_cleaned.empty:\n",
    "        print(f\"Error: No data remaining after dropping NaNs in target column '{target_col_name}'.\")\n",
    "        return None, None, None\n",
    "\n",
    "    y = df_cleaned[target_col_name]\n",
    "\n",
    "    base_excluded_cols = ['start_time', 'cycle_number', 'battery_id', 'regime'] # Add 'regime' and 'battery_id' here\n",
    "    \n",
    "    # Combine standard exclusions, the target itself, SOH (if not target), and any extras passed\n",
    "    excluded_cols_from_features = list(set(\n",
    "        base_excluded_cols +\n",
    "        [target_col_name, 'SOH_cycle_capacity_%'] + # SOH_cycle_capacity_% is often the target or closely related\n",
    "        (current_extra_exclude_cols if current_extra_exclude_cols else [])\n",
    "    ))\n",
    "    \n",
    "    potential_feature_cols_candidates = [col for col in df_cleaned.columns if col not in excluded_cols_from_features]\n",
    "    \n",
    "    temp_X_df = df_cleaned[potential_feature_cols_candidates].copy()\n",
    "\n",
    "    if 'is_reference_cycle' in temp_X_df.columns:\n",
    "        temp_X_df['is_reference_cycle'] = temp_X_df['is_reference_cycle'].astype(int)\n",
    "\n",
    "    feature_cols_for_X = []\n",
    "    for col in temp_X_df.columns:\n",
    "        if temp_X_df[col].dtype == 'object':\n",
    "            try: # Attempt to convert object columns to numeric if possible\n",
    "                temp_X_df[col] = pd.to_numeric(temp_X_df[col])\n",
    "                feature_cols_for_X.append(col)\n",
    "                print(f\"Converted object column '{col}' to numeric.\")\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Skipping object type column '{col}' as it could not be converted to numeric.\")\n",
    "                continue\n",
    "        else:\n",
    "            feature_cols_for_X.append(col)\n",
    "    \n",
    "    feature_cols_for_X = sorted(list(set(feature_cols_for_X)))\n",
    "    X = temp_X_df[feature_cols_for_X].copy()\n",
    "\n",
    "    print(f\"Target variable: {target_col_name}\")\n",
    "    print(f\"Number of features selected: {len(feature_cols_for_X)}\")\n",
    "    if X.empty: print(\"Error: X is empty after feature selection.\")\n",
    "    if y.empty: print(\"Error: y is empty.\")\n",
    "    print(f\"Shape of X (features for model): {X.shape}, Shape of y: {y.shape}\")\n",
    "        \n",
    "    return X, y, feature_cols_for_X # Return actual feature names\n",
    "\n",
    "def scale_features(X_train, X_val, X_test, feature_names):\n",
    "    \"\"\"\n",
    "    Scales features using StandardScaler.\n",
    "    Assumes X_train, X_val, X_test are DataFrames and already imputed.\n",
    "    Returns scaled DataFrames and the scaler.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_names, index=X_train.index)\n",
    "    \n",
    "    X_val_scaled = None\n",
    "    if X_val is not None and not X_val.empty:\n",
    "        X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=feature_names, index=X_val.index)\n",
    "    \n",
    "    X_test_scaled = None\n",
    "    if X_test is not None and not X_test.empty:\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_names, index=X_test.index)\n",
    "        \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "# --- END OF CELL 2 IF DEFINING FUNCTIONS HERE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343ca07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from New_Features_Added_ALL.csv. Shape: (8220, 200)\n",
      "--- Data Loaded and Regimes Annotated ---\n",
      "Master DataFrame shape: (8220, 201)\n",
      "Regime counts (batteries per regime):\n",
      "regime\n",
      "recommissioned_three_stage     3\n",
      "recommissioned_two_stage       5\n",
      "regular_constant              10\n",
      "regular_variable               5\n",
      "Name: battery_id, dtype: int64\n",
      "Total unique batteries in loaded data: 23\n"
     ]
    }
   ],
   "source": [
    "df_master_full = load_data(INPUT_FILE) # or train_models.load_data(INPUT_FILE)\n",
    "\n",
    "if df_master_full is None:\n",
    "    raise ValueError(f\"Failed to load data from {INPUT_FILE}.\")\n",
    "\n",
    "# Ensure 'cycle_number' is numeric for sorting\n",
    "df_master_full['cycle_number'] = pd.to_numeric(df_master_full['cycle_number'], errors='coerce')\n",
    "df_master_full.dropna(subset=['cycle_number'], inplace=True)\n",
    "df_master_full['cycle_number'] = df_master_full['cycle_number'].astype(int)\n",
    "\n",
    "# Annotate with 'regime'\n",
    "df_master_full['regime'] = df_master_full['battery_id'].map(BATTERY_REGIME_MAP)\n",
    "if df_master_full['regime'].isnull().any():\n",
    "    unmapped_batteries = df_master_full[df_master_full['regime'].isnull()]['battery_id'].unique()\n",
    "    print(f\"Warning: Unmapped batteries found: {unmapped_batteries}. These will be excluded from splits if they remain unmapped.\")\n",
    "    # Option: df_master_full.dropna(subset=['regime'], inplace=True) # Or handle them based on policy\n",
    "\n",
    "print(\"--- Data Loaded and Regimes Annotated ---\")\n",
    "print(f\"Master DataFrame shape: {df_master_full.shape}\")\n",
    "print(\"Regime counts (batteries per regime):\")\n",
    "print(df_master_full.groupby('regime')['battery_id'].nunique())\n",
    "print(f\"Total unique batteries in loaded data: {df_master_full['battery_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf4e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Battery Split (Seed: 123) ---\n",
      "Training Batteries (16): ['battery00', 'battery01', 'battery02', 'battery03', 'battery10', 'battery11', 'battery12', 'battery20', 'battery21', 'battery24', 'battery30', 'battery31', 'battery32', 'battery40', 'battery50', 'battery53']\n",
      "Validation Batteries (2): ['battery25', 'battery33']\n",
      "Test Batteries (5): ['battery22', 'battery23', 'battery41', 'battery51', 'battery52']\n",
      "  Train Regimes: {'regular_constant': 10, 'recommissioned_two_stage': 5, 'recommissioned_three_stage': 1}\n",
      "  Validation Regimes: {'recommissioned_three_stage': 2}\n",
      "  Test Regimes: {'regular_variable': 5}\n"
     ]
    }
   ],
   "source": [
    "def get_fixed_stratified_battery_splits(\n",
    "    df_with_regime,\n",
    "    n_test_batteries,\n",
    "    n_val_batteries,\n",
    "    random_seed,\n",
    "    regime_col='regime',\n",
    "    battery_id_col='battery_id'\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits batteries into fixed-size, stratified train, validation, and test sets.\n",
    "    Ensures batteries with no regime mapping are excluded from the split process.\n",
    "    \"\"\"\n",
    "    # Filter out batteries with no regime mapping\n",
    "    df_mappable = df_with_regime.dropna(subset=[regime_col])\n",
    "    if len(df_mappable) < len(df_with_regime):\n",
    "        print(f\"Dropped {len(df_with_regime) - len(df_mappable)} rows with missing regimes before splitting.\")\n",
    "\n",
    "    all_batteries_info = df_mappable[[battery_id_col, regime_col]].drop_duplicates().reset_index(drop=True)\n",
    "    unique_batteries = all_batteries_info[battery_id_col].unique()\n",
    "\n",
    "    if len(unique_batteries) < (n_test_batteries + n_val_batteries):\n",
    "        raise ValueError(\n",
    "            f\"Not enough unique mappable batteries ({len(unique_batteries)}) to create test ({n_test_batteries}) \"\n",
    "            f\"and validation ({n_val_batteries}) sets.\"\n",
    "        )\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    test_battery_ids = []\n",
    "    val_battery_ids = []\n",
    "    \n",
    "    # Stratified selection for test set\n",
    "    # Group by regime and sample, then supplement if needed\n",
    "    batteries_by_regime = all_batteries_info.groupby(regime_col)[battery_id_col].apply(list)\n",
    "    \n",
    "    # Shuffle regimes to avoid bias in picking order\n",
    "    available_regimes = list(batteries_by_regime.index)\n",
    "    np.random.shuffle(available_regimes)\n",
    "\n",
    "    # Pick for TEST set\n",
    "    for regime in available_regimes:\n",
    "        if len(test_battery_ids) >= n_test_batteries: break\n",
    "        candidates = [b for b in batteries_by_regime[regime] if b not in test_battery_ids]\n",
    "        np.random.shuffle(candidates)\n",
    "        can_take = n_test_batteries - len(test_battery_ids)\n",
    "        actual_take = min(len(candidates), can_take)\n",
    "        test_battery_ids.extend(candidates[:actual_take])\n",
    "        \n",
    "    # If not enough from stratified, fill randomly from remaining\n",
    "    remaining_for_test_fill = [b for b in unique_batteries if b not in test_battery_ids]\n",
    "    np.random.shuffle(remaining_for_test_fill)\n",
    "    if len(test_battery_ids) < n_test_batteries:\n",
    "        needed = n_test_batteries - len(test_battery_ids)\n",
    "        test_battery_ids.extend(remaining_for_test_fill[:needed])\n",
    "\n",
    "    # Pick for VALIDATION set (from batteries not in test set)\n",
    "    available_for_val = [b for b in unique_batteries if b not in test_battery_ids]\n",
    "    for regime in available_regimes: # Iterate through regimes again for val\n",
    "        if len(val_battery_ids) >= n_val_batteries: break\n",
    "        candidates = [b for b in batteries_by_regime.get(regime, []) if b in available_for_val and b not in val_battery_ids]\n",
    "        np.random.shuffle(candidates)\n",
    "        can_take = n_val_batteries - len(val_battery_ids)\n",
    "        actual_take = min(len(candidates), can_take)\n",
    "        val_battery_ids.extend(candidates[:actual_take])\n",
    "\n",
    "    # If not enough from stratified for val, fill randomly\n",
    "    remaining_for_val_fill = [b for b in available_for_val if b not in val_battery_ids]\n",
    "    np.random.shuffle(remaining_for_val_fill)\n",
    "    if len(val_battery_ids) < n_val_batteries:\n",
    "        needed = n_val_batteries - len(val_battery_ids)\n",
    "        val_battery_ids.extend(remaining_for_val_fill[:needed])\n",
    "\n",
    "    train_battery_ids = [\n",
    "        b for b in unique_batteries if b not in test_battery_ids and b not in val_battery_ids\n",
    "    ]\n",
    "    np.random.shuffle(train_battery_ids) # Shuffle train IDs too\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(set(train_battery_ids).intersection(set(test_battery_ids))) == 0, \"Overlap: Train-Test\"\n",
    "    assert len(set(train_battery_ids).intersection(set(val_battery_ids))) == 0, \"Overlap: Train-Val\"\n",
    "    assert len(set(val_battery_ids).intersection(set(test_battery_ids))) == 0, \"Overlap: Val-Test\"\n",
    "    assert len(train_battery_ids) + len(val_battery_ids) + len(test_battery_ids) == len(unique_batteries), \"Mismatch in total batteries\"\n",
    "    assert len(test_battery_ids) == n_test_batteries, f\"Expected {n_test_batteries} test batteries, got {len(test_battery_ids)}\"\n",
    "    assert len(val_battery_ids) == n_val_batteries, f\"Expected {n_val_batteries} validation batteries, got {len(val_battery_ids)}\"\n",
    "    if not train_battery_ids : print(\"CRITICAL WARNING: Training set is empty!\")\n",
    "\n",
    "\n",
    "    print(f\"--- Battery Split (Seed: {random_seed}) ---\")\n",
    "    print(f\"Training Batteries ({len(train_battery_ids)}): {sorted(train_battery_ids)}\")\n",
    "    print(f\"Validation Batteries ({len(val_battery_ids)}): {sorted(val_battery_ids)}\")\n",
    "    print(f\"Test Batteries ({len(test_battery_ids)}): {sorted(test_battery_ids)}\")\n",
    "    \n",
    "    # Optional: print regime distribution for each set\n",
    "    for name, ids in zip([\"Train\", \"Validation\", \"Test\"], [train_battery_ids, val_battery_ids, test_battery_ids]):\n",
    "        if ids: # Check if the list is not empty\n",
    "            regimes = all_batteries_info[all_batteries_info[battery_id_col].isin(ids)][regime_col].value_counts().to_dict()\n",
    "            print(f\"  {name} Regimes: {regimes}\")\n",
    "        else:\n",
    "            print(f\"  {name} Regimes: No batteries in this set.\")\n",
    "\n",
    "\n",
    "    return train_battery_ids, val_battery_ids, test_battery_ids\n",
    "\n",
    "# Perform the split\n",
    "train_ids, val_ids, test_ids = get_fixed_stratified_battery_splits(\n",
    "    df_master_full,\n",
    "    n_test_batteries=N_TEST_BATTERIES,\n",
    "    n_val_batteries=N_VAL_BATTERIES,\n",
    "    random_seed=SPLIT_RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52eb1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted_soh(\n",
    "    df_test_eval, # DataFrame with 'battery_id', 'cycle_number', actual_soh_col, predicted_soh_col\n",
    "    battery_id_to_plot,\n",
    "    model_name_str,\n",
    "    output_dir,\n",
    "    actual_soh_col='y_actual', # Ensure these column names match what's in df_test_eval\n",
    "    predicted_soh_col='y_pred'\n",
    "):\n",
    "    \"\"\"Plots actual vs. predicted SOH for a specific battery and saves it.\"\"\"\n",
    "    battery_data = df_test_eval[df_test_eval['battery_id'] == battery_id_to_plot].sort_values('cycle_number')\n",
    "    \n",
    "    if battery_data.empty:\n",
    "        print(f\"Warning: No data found for battery {battery_id_to_plot} in the evaluation results. Skipping plot.\")\n",
    "        return\n",
    "    if len(battery_data) < 2: # Handle edge case from previous discussion\n",
    "         print(f\"Warning: Battery {battery_id_to_plot} has fewer than 2 data points. Plot might be uninformative.\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(battery_data['cycle_number'], battery_data[actual_soh_col], label='Actual SOH', marker='o', linestyle='-')\n",
    "    plt.plot(battery_data['cycle_number'], battery_data[predicted_soh_col], label=f'Predicted SOH ({model_name_str})', marker='x', linestyle='--')\n",
    "    \n",
    "    plt.title(f'Actual vs. Predicted SOH for Battery: {battery_id_to_plot} (Model: {model_name_str.upper()})')\n",
    "    plt.xlabel('Cycle Number')\n",
    "    plt.ylabel('SOH (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plot_filename = os.path.join(output_dir, f\"soh_comparison_{battery_id_to_plot}_{model_name_str}.png\")\n",
    "    try:\n",
    "        plt.savefig(plot_filename)\n",
    "        print(f\"  Saved SOH comparison plot to {plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving SOH comparison plot for {battery_id_to_plot}: {e}\")\n",
    "    plt.close() # Close the plot to free memory\n",
    "\n",
    "def plot_feature_importances(\n",
    "    importances_df, # DataFrame with 'feature' and 'importance' columns\n",
    "    model_name_str,\n",
    "    output_dir,\n",
    "    top_n=15\n",
    "):\n",
    "    \"\"\"Plots feature importances as a horizontal bar chart and saves it.\"\"\"\n",
    "    if importances_df is None or importances_df.empty:\n",
    "        print(f\"No feature importances to plot for {model_name_str}.\")\n",
    "        return\n",
    "\n",
    "    importances_df = importances_df.sort_values(by='importance', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, max(6, len(importances_df) * 0.4))) # Adjust height based on N features\n",
    "    sns.barplot(x='importance', y='feature', data=importances_df, hue='feature', palette='viridis', legend=False, dodge=False)\n",
    "    plt.title(f'Top {top_n} Feature Importances: {model_name_str.upper()}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    \n",
    "    plot_filename = os.path.join(output_dir, f\"feature_importances_{model_name_str}.png\")\n",
    "    try:\n",
    "        plt.savefig(plot_filename)\n",
    "        print(f\"  Saved feature importance plot to {plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving feature importance plot for {model_name_str}: {e}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6635929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_set(\n",
    "    model,\n",
    "    model_name_str, # e.g., \"rf_tuned\"\n",
    "    X_data,\n",
    "    y_data,\n",
    "    feature_names_list, # For feature importance calculation\n",
    "    set_name, # \"Validation\" or \"Test\"\n",
    "    output_dir_for_plots, # For saving plots\n",
    "    battery_ids_for_this_set=None, # Series/list of battery_ids corresponding to X_data rows\n",
    "    plot_soh_curves=False # Flag to control SOH curve plotting (usually True for Test set)\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a fitted model on a given dataset, optionally plots SOH curves and feature importances.\n",
    "    \"\"\"\n",
    "    results_metrics = {}\n",
    "    feature_importance_df_model = None # Initialize\n",
    "    print(f\"\\nEvaluating {model_name_str.upper()} on {set_name} Set...\")\n",
    "\n",
    "    rmse, mae, r2 = np.nan, np.nan, np.nan\n",
    "    y_pred = None # Initialize y_pred\n",
    "\n",
    "    if X_data is not None and not X_data.empty and y_data is not None and not y_data.empty:\n",
    "        if len(X_data) != len(y_data):\n",
    "            print(f\"  Error: X_data length ({len(X_data)}) and y_data length ({len(y_data)}) mismatch for {set_name}. Skipping.\")\n",
    "        else:\n",
    "            try:\n",
    "                y_pred = model.predict(X_data)\n",
    "                rmse = np.sqrt(mean_squared_error(y_data, y_pred))\n",
    "                mae = mean_absolute_error(y_data, y_pred)\n",
    "                r2 = r2_score(y_data, y_pred)\n",
    "                print(f\"  {set_name} Metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error during {set_name} evaluation for {model_name_str}: {e}\")\n",
    "    else:\n",
    "        print(f\"  {set_name} data is empty or y_data is empty. Skipping {set_name} metrics.\")\n",
    "\n",
    "    results_metrics = {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "    # Feature Importances (calculated once, typically using training data or general model property)\n",
    "    # This function now focuses on PLOTTING them if provided, or extracting them if simple.\n",
    "    # The main feature importance calculation can be done after GridSearchCV.\n",
    "    # For now, let's assume we primarily get it from model attributes.\n",
    "    if feature_names_list:\n",
    "        importances_values = None\n",
    "        model_type_for_fi = model_name_str.split('_')[0] # e.g., 'rf' from 'rf_tuned'\n",
    "\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances_values = model.feature_importances_\n",
    "        elif model_type_for_fi == 'lr' and hasattr(model, 'coef_'):\n",
    "            importances_values = np.abs(model.coef_)\n",
    "            if len(importances_values.shape) > 1: # If multi-target (not expected here)\n",
    "                importances_values = np.mean(importances_values, axis=0)\n",
    "        \n",
    "        if importances_values is not None:\n",
    "            if len(feature_names_list) == len(importances_values):\n",
    "                feature_importance_df_model = pd.DataFrame({'feature': feature_names_list, 'importance': importances_values})\n",
    "                feature_importance_df_model = feature_importance_df_model.sort_values(by='importance', ascending=False) # Keep full, plot top N later\n",
    "                # Plotting of FI will be called separately if desired, after getting this df\n",
    "            else:\n",
    "                print(f\"  Warning: Mismatch in feature names ({len(feature_names_list)}) and importances ({len(importances_values)}) for {model_name_str}.\")\n",
    "        elif set_name.lower() == \"test\": # Only warn if we expect FI and don't have it for test eval display\n",
    "             print(f\"  Warning: Model {model_name_str} does not have standard 'feature_importances_' or 'coef_'.\")\n",
    "\n",
    "\n",
    "    # Plot SOH curves for each battery in this set (if applicable, e.g., for test set)\n",
    "    if plot_soh_curves and y_pred is not None and battery_ids_for_this_set is not None and not X_data.empty :\n",
    "        print(f\"  Generating SOH comparison plots for {set_name} set...\")\n",
    "        # We need 'cycle_number' for plotting. It's not in X_data.\n",
    "        # We need to retrieve it from the original df slice for this set.\n",
    "        # This implies `evaluate_model_on_set` needs access to the original df_subset or its relevant columns.\n",
    "        # For simplicity, let's assume the calling function prepares a df_eval_results.\n",
    "\n",
    "        # Create a temporary DataFrame for plotting\n",
    "        # Ensure y_data (actual) and y_pred (predicted) are Series with the same index as X_data.\n",
    "        # The battery_ids_for_this_set should also align with this index.\n",
    "        # Cycle numbers need to be fetched by joining back to the original data using index or battery_id and original cycle numbers.\n",
    "        \n",
    "        # THIS PART REQUIRES CAREFUL INDEX ALIGNMENT.\n",
    "        # Assuming y_data is a Series and X_data is a DataFrame, both from the same split.\n",
    "        # Let's make df_eval_results in the main experiment runner.\n",
    "        df_eval_temp = pd.DataFrame({\n",
    "            'battery_id': battery_ids_for_this_set.values, # Ensure this is aligned with X_data.index\n",
    "            'cycle_number': df_master_full.loc[X_data.index, 'cycle_number'].values, # Fetch cycle_number using original index\n",
    "            'y_actual': y_data.values,\n",
    "            'y_pred': y_pred\n",
    "        })\n",
    "\n",
    "        for batt_id in df_eval_temp['battery_id'].unique():\n",
    "            plot_actual_vs_predicted_soh(\n",
    "                df_test_eval=df_eval_temp,\n",
    "                battery_id_to_plot=batt_id,\n",
    "                model_name_str=model_name_str,\n",
    "                output_dir=output_dir_for_plots,\n",
    "                actual_soh_col='y_actual',\n",
    "                predicted_soh_col='y_pred'\n",
    "            )\n",
    "            \n",
    "    return results_metrics, feature_importance_df_model # Return metrics and the full FI dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750e5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main_experiment(\n",
    "    experiment_tag, # e.g., EXPERIMENT_NAME\n",
    "    df_full,\n",
    "    train_battery_ids,\n",
    "    val_battery_ids,\n",
    "    test_battery_ids,\n",
    "    target_variable_name,\n",
    "    extra_feature_exclusions,\n",
    "    models_to_run,\n",
    "    model_param_grids,\n",
    "    gridsearch_cv_folds,\n",
    "    gridsearch_metric_name,\n",
    "    base_artifact_dir, # Where to save everything for this experiment\n",
    "    save_all_artifacts=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the main SOH prediction experiment:\n",
    "    1. Filters data for train, validation, test sets based on battery IDs.\n",
    "    2. Preprocesses data (impute & scale).\n",
    "    3. For each model, performs GridSearchCV on training data.\n",
    "    4. Evaluates the best model from GridSearchCV on validation and test sets.\n",
    "    5. Saves models, scaler, imputer, plots, and results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Starting Experiment: {experiment_tag} {'='*20}\")\n",
    "    current_exp_output_dir = os.path.join(base_artifact_dir) # Already includes experiment_name\n",
    "    # os.makedirs(current_exp_output_dir, exist_ok=True) # Already created in Cell 1\n",
    "\n",
    "    # --- 1. Filter Data for Train/Validation/Test using provided IDs ---\n",
    "    df_train = df_full[df_full['battery_id'].isin(train_battery_ids)].copy()\n",
    "    df_val = df_full[df_full['battery_id'].isin(val_battery_ids)].copy()\n",
    "    df_test = df_full[df_full['battery_id'].isin(test_battery_ids)].copy()\n",
    "\n",
    "    print(f\"Data split: Train batteries={len(train_battery_ids)}, Val batteries={len(val_battery_ids)}, Test batteries={len(test_battery_ids)}\")\n",
    "    print(f\"Train data shape: {df_train.shape}, Val data shape: {df_val.shape}, Test data shape: {df_test.shape}\")\n",
    "\n",
    "    if df_train.empty:\n",
    "        print(\"ERROR: Training data is empty. Aborting experiment.\")\n",
    "        return None\n",
    "    if df_test.empty:\n",
    "        print(\"ERROR: Test data is empty. Aborting experiment.\")\n",
    "        return None\n",
    "    # Validation set can be empty if N_VAL_BATTERIES was 0 (though we set it > 0)\n",
    "\n",
    "    # --- 2. Preprocess Data (Extract X, y for each set) ---\n",
    "    # Note: preprocess_data drops 'battery_id' and 'regime' from X\n",
    "    X_train_raw, y_train, actual_feature_names = preprocess_data(df_train, target_variable_name, extra_feature_exclusions)\n",
    "    X_val_raw, y_val, _ = preprocess_data(df_val, target_variable_name, extra_feature_exclusions)\n",
    "    X_test_raw, y_test, _ = preprocess_data(df_test, target_variable_name, extra_feature_exclusions)\n",
    "\n",
    "    if X_train_raw is None or X_train_raw.empty:\n",
    "        print(\"Error: X_train is empty after preprocessing. Aborting.\")\n",
    "        return None\n",
    "    # Store original indices for later use if needed (e.g. detailed error analysis, fetching other columns)\n",
    "    train_indices = X_train_raw.index\n",
    "    val_indices = X_val_raw.index if X_val_raw is not None else None\n",
    "    test_indices = X_test_raw.index if X_test_raw is not None else None\n",
    "\n",
    "\n",
    "    # --- 3. Impute Data (Fit on Train, Transform Val/Test) ---\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=actual_feature_names, index=train_indices)\n",
    "    \n",
    "    X_val_imputed = None\n",
    "    if X_val_raw is not None and not X_val_raw.empty:\n",
    "        X_val_imputed = pd.DataFrame(imputer.transform(X_val_raw), columns=actual_feature_names, index=val_indices)\n",
    "    \n",
    "    X_test_imputed = None\n",
    "    if X_test_raw is not None and not X_test_raw.empty:\n",
    "        X_test_imputed = pd.DataFrame(imputer.transform(X_test_raw), columns=actual_feature_names, index=test_indices)\n",
    "\n",
    "    # --- 4. Scale Data (Fit on Train, Transform Val/Test) ---\n",
    "    # scale_features now returns X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_features(\n",
    "        X_train_imputed, X_val_imputed, X_test_imputed, actual_feature_names\n",
    "    )\n",
    "\n",
    "    experiment_summary_results = {} # To store metrics for all models\n",
    "    all_trained_models = {} # To store the best_estimator from GridSearchCV\n",
    "    all_feature_importances = {} # To store FI dataframes\n",
    "\n",
    "    # --- 5. GridSearchCV for each model ---\n",
    "    for model_name_code in models_to_run: # e.g., 'rf', 'gb'\n",
    "        print(f\"\\n--- Tuning {model_name_code.upper()} ---\")\n",
    "        \n",
    "        # Determine data to use (scaled or unscaled)\n",
    "        current_X_train_for_tuning = X_train_imputed\n",
    "        current_X_val_for_eval = X_val_imputed # For evaluation after tuning\n",
    "        current_X_test_for_eval = X_test_imputed\n",
    "        \n",
    "        if model_name_code in ['lr', 'gb']: # 'gb' is HistGradientBoostingRegressor\n",
    "            current_X_train_for_tuning = X_train_scaled\n",
    "            current_X_val_for_eval = X_val_scaled\n",
    "            current_X_test_for_eval = X_test_scaled\n",
    "            print(f\"  Using SCALED data for {model_name_code.upper()}\")\n",
    "        else:\n",
    "            print(f\"  Using IMPUTED (unscaled) data for {model_name_code.upper()}\")\n",
    "\n",
    "        # Initialize base model\n",
    "        base_model = None\n",
    "        if model_name_code == 'rf': base_model = RandomForestRegressor(random_state=SPLIT_RANDOM_SEED, n_jobs=-1)\n",
    "        elif model_name_code == 'gb': base_model = HistGradientBoostingRegressor(random_state=SPLIT_RANDOM_SEED)\n",
    "        elif model_name_code == 'xgb': base_model = xgb.XGBRegressor(random_state=SPLIT_RANDOM_SEED, n_jobs=-1)\n",
    "        elif model_name_code == 'lr': base_model = LinearRegression()\n",
    "        else:\n",
    "            print(f\"Warning: Unknown model '{model_name_code}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=model_param_grids[model_name_code],\n",
    "            scoring=gridsearch_metric_name,\n",
    "            cv=gridsearch_cv_folds,\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        print(f\"  Starting GridSearchCV for {model_name_code.upper()}...\")\n",
    "        grid_search.fit(current_X_train_for_tuning, y_train) # y_train corresponds to X_train_raw/imputed/scaled\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"  Best parameters for {model_name_code.upper()}: {best_params}\")\n",
    "        \n",
    "        all_trained_models[model_name_code] = best_model\n",
    "        \n",
    "        # --- 6. Evaluate the BEST tuned model ---\n",
    "        model_eval_results = {'best_params': best_params}\n",
    "        \n",
    "        # Evaluate on Validation Set (if val set exists)\n",
    "        val_metrics, fi_df_val = (None, None)\n",
    "        if current_X_val_for_eval is not None and not current_X_val_for_eval.empty and y_val is not None and not y_val.empty:\n",
    "            val_metrics, fi_df_val = evaluate_model_on_set(\n",
    "                best_model, f\"{model_name_code}_tuned\", current_X_val_for_eval, y_val, actual_feature_names,\n",
    "                \"Validation\", current_exp_output_dir,\n",
    "                battery_ids_for_this_set=df_val['battery_id'], # Pass battery IDs for val set\n",
    "                plot_soh_curves=False # Typically no SOH curves for validation set\n",
    "            )\n",
    "            model_eval_results['val_metrics'] = val_metrics\n",
    "        else:\n",
    "            model_eval_results['val_metrics'] = {'rmse': np.nan, 'mae': np.nan, 'r2': np.nan}\n",
    "            print(\"  Validation set is empty or not available, skipping validation evaluation.\")\n",
    "\n",
    "        # Evaluate on Test Set\n",
    "        test_metrics, fi_df_test = (None, None)\n",
    "        if current_X_test_for_eval is not None and not current_X_test_for_eval.empty and y_test is not None and not y_test.empty:\n",
    "            # For test set evaluation, we need battery IDs and cycle numbers associated with X_test_for_eval\n",
    "            # These are on df_test which has the same original indices as X_test_raw (and thus X_test_for_eval)\n",
    "            test_battery_ids_series = df_test.loc[X_test_raw.index, 'battery_id']\n",
    "\n",
    "            test_metrics, fi_df_test = evaluate_model_on_set(\n",
    "                best_model, f\"{model_name_code}_tuned\", current_X_test_for_eval, y_test, actual_feature_names,\n",
    "                \"Test\", current_exp_output_dir,\n",
    "                battery_ids_for_this_set=test_battery_ids_series, # Pass battery IDs for test set\n",
    "                plot_soh_curves=True # Plot SOH curves for the test set\n",
    "            )\n",
    "            model_eval_results['test_metrics'] = test_metrics\n",
    "        else:\n",
    "            model_eval_results['test_metrics'] = {'rmse': np.nan, 'mae': np.nan, 'r2': np.nan}\n",
    "            print(\"  Test set is empty or not available, skipping test evaluation.\")\n",
    "\n",
    "        # Store feature importances (use the one from test evaluation or val if test is empty)\n",
    "        # The fi_df is calculated from the model attributes, so it's the same regardless of val/test data used for eval.\n",
    "        final_fi_df = fi_df_test if fi_df_test is not None else fi_df_val\n",
    "        if final_fi_df is not None and not final_fi_df.empty:\n",
    "            all_feature_importances[model_name_code] = final_fi_df\n",
    "            # Plot feature importances\n",
    "            plot_feature_importances(final_fi_df, f\"{model_name_code}_tuned\", current_exp_output_dir)\n",
    "        \n",
    "        experiment_summary_results[model_name_code] = model_eval_results\n",
    "\n",
    "    # --- 7. Save Artifacts ---\n",
    "    if save_all_artifacts:\n",
    "        joblib.dump(scaler, os.path.join(current_exp_output_dir, f\"scaler_{experiment_tag}.joblib\"))\n",
    "        joblib.dump(imputer, os.path.join(current_exp_output_dir, f\"imputer_{experiment_tag}.joblib\"))\n",
    "        joblib.dump(actual_feature_names, os.path.join(current_exp_output_dir, f\"feature_names_{experiment_tag}.joblib\"))\n",
    "        print(f\"\\nScaler, Imputer, and Feature Names saved for {experiment_tag} in {current_exp_output_dir}\")\n",
    "\n",
    "        for model_name_code, model_instance in all_trained_models.items():\n",
    "            model_filename = os.path.join(current_exp_output_dir, f\"model_{model_name_code}_tuned_{experiment_tag}.joblib\")\n",
    "            joblib.dump(model_instance, model_filename)\n",
    "        print(f\"Tuned models saved for {experiment_tag} in {current_exp_output_dir}\")\n",
    "        \n",
    "        # Save feature importances dataframes\n",
    "        if all_feature_importances:\n",
    "            fi_path = os.path.join(current_exp_output_dir, f\"all_feature_importances_{experiment_tag}.joblib\")\n",
    "            joblib.dump(all_feature_importances, fi_path)\n",
    "            print(f\"Feature importances data saved to {fi_path}\")\n",
    "            \n",
    "    return experiment_summary_results, all_trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53054931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Experiment: SOH_Prediction_FixedSplit_Run2 ====================\n",
      "Data split: Train batteries=16, Val batteries=2, Test batteries=5\n",
      "Train data shape: (5669, 201), Val data shape: (895, 201), Test data shape: (1656, 201)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 168\n",
      "Shape of X (features for model): (5669, 168), Shape of y: (5669,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 168\n",
      "Shape of X (features for model): (895, 168), Shape of y: (895,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 168\n",
      "Shape of X (features for model): (1656, 168), Shape of y: (1656,)\n",
      "\n",
      "--- Tuning RF ---\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "  Best parameters for RF: {'max_depth': 25, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Evaluating RF_TUNED on Validation Set...\n",
      "  Validation Metrics - RMSE: 1.0443, MAE: 0.8028, R2: 0.9741\n",
      "\n",
      "Evaluating RF_TUNED on Test Set...\n",
      "  Test Metrics - RMSE: 2.7080, MAE: 1.7331, R2: 0.8404\n",
      "  Generating SOH comparison plots for Test set...\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery22_rf_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery23_rf_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery41_rf_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery51_rf_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery52_rf_tuned.png\n",
      "  Saved feature importance plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/feature_importances_rf_tuned.png\n",
      "\n",
      "--- Tuning GB ---\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "  Best parameters for GB: {'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 200}\n",
      "\n",
      "Evaluating GB_TUNED on Validation Set...\n",
      "  Validation Metrics - RMSE: 0.9063, MAE: 0.6927, R2: 0.9805\n",
      "\n",
      "Evaluating GB_TUNED on Test Set...\n",
      "  Test Metrics - RMSE: 2.1643, MAE: 1.5050, R2: 0.8980\n",
      "  Warning: Model gb_tuned does not have standard 'feature_importances_' or 'coef_'.\n",
      "  Generating SOH comparison plots for Test set...\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery22_gb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery23_gb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery41_gb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery51_gb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery52_gb_tuned.png\n",
      "\n",
      "--- Tuning XGB ---\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 150}\n",
      "\n",
      "Evaluating XGB_TUNED on Validation Set...\n",
      "  Validation Metrics - RMSE: 0.8179, MAE: 0.6162, R2: 0.9841\n",
      "\n",
      "Evaluating XGB_TUNED on Test Set...\n",
      "  Test Metrics - RMSE: 2.4095, MAE: 1.6271, R2: 0.8736\n",
      "  Generating SOH comparison plots for Test set...\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery22_xgb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery23_xgb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery41_xgb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery51_xgb_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery52_xgb_tuned.png\n",
      "  Saved feature importance plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/feature_importances_xgb_tuned.png\n",
      "\n",
      "--- Tuning LR ---\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "\n",
      "Evaluating LR_TUNED on Validation Set...\n",
      "  Validation Metrics - RMSE: 0.7766, MAE: 0.5918, R2: 0.9857\n",
      "\n",
      "Evaluating LR_TUNED on Test Set...\n",
      "  Test Metrics - RMSE: 3.3772, MAE: 2.0598, R2: 0.7517\n",
      "  Generating SOH comparison plots for Test set...\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery22_lr_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery23_lr_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery41_lr_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery51_lr_tuned.png\n",
      "  Saved SOH comparison plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/soh_comparison_battery52_lr_tuned.png\n",
      "  Saved feature importance plot to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/feature_importances_lr_tuned.png\n",
      "\n",
      "Scaler, Imputer, and Feature Names saved for SOH_Prediction_FixedSplit_Run2 in soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2\n",
      "Tuned models saved for SOH_Prediction_FixedSplit_Run2 in soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2\n",
      "Feature importances data saved to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/all_feature_importances_SOH_Prediction_FixedSplit_Run2.joblib\n",
      "\n",
      "--- Main Experiment Run Completed ---\n"
     ]
    }
   ],
   "source": [
    "if not train_ids or not test_ids: # Basic check\n",
    "    print(\"CRITICAL: Training or Test battery IDs are empty. Cannot run experiment.\")\n",
    "    # Add more robust checks if N_VAL_BATTERIES > 0 and val_ids is empty\n",
    "elif N_VAL_BATTERIES > 0 and not val_ids:\n",
    "     print(\"CRITICAL: Validation batteries are configured (N_VAL_BATTERIES > 0) but validation ID list is empty. Cannot run experiment.\")\n",
    "else:\n",
    "    final_experiment_results, final_trained_models = run_main_experiment(\n",
    "        experiment_tag=EXPERIMENT_NAME,\n",
    "        df_full=df_master_full,\n",
    "        train_battery_ids=train_ids,\n",
    "        val_battery_ids=val_ids,\n",
    "        test_battery_ids=test_ids,\n",
    "        target_variable_name=TARGET_COL,\n",
    "        extra_feature_exclusions=EXTRA_EXCLUDE_COLS_FROM_FEATURES,\n",
    "        models_to_run=MODELS_TO_TRAIN,\n",
    "        model_param_grids=PARAM_GRIDS,\n",
    "        gridsearch_cv_folds=CV_FOLDS_GRIDSEARCH,\n",
    "        gridsearch_metric_name=GRIDSEARCH_SCORING,\n",
    "        base_artifact_dir=EXPERIMENT_OUTPUT_DIR, # Pass the specific output dir for this experiment\n",
    "        save_all_artifacts=SAVE_ARTIFACTS\n",
    "    )\n",
    "    print(\"\\n--- Main Experiment Run Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2660968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Consolidated Experiment Results Summary ---\n",
      "\n",
      "Model: RF\n",
      "  Best Params: {'max_depth': 25, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "  Validation Set: RMSE=1.0443, MAE=0.8028, R2=0.9741\n",
      "  Test Set:       RMSE=2.7080, MAE=1.7331, R2=0.8404\n",
      "\n",
      "Model: GB\n",
      "  Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 200}\n",
      "  Validation Set: RMSE=0.9063, MAE=0.6927, R2=0.9805\n",
      "  Test Set:       RMSE=2.1643, MAE=1.5050, R2=0.8980\n",
      "\n",
      "Model: XGB\n",
      "  Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 150}\n",
      "  Validation Set: RMSE=0.8179, MAE=0.6162, R2=0.9841\n",
      "  Test Set:       RMSE=2.4095, MAE=1.6271, R2=0.8736\n",
      "\n",
      "Model: LR\n",
      "  Best Params: {'fit_intercept': True}\n",
      "  Validation Set: RMSE=0.7766, MAE=0.5918, R2=0.9857\n",
      "  Test Set:       RMSE=3.3772, MAE=2.0598, R2=0.7517\n",
      "\n",
      "\n",
      "--- Experiment Results DataFrame ---\n",
      "                       Experiment Model     Status                                        Best Params  Val RMSE   Val MAE    Val R2  Test RMSE  Test MAE   Test R2\n",
      "0  SOH_Prediction_FixedSplit_Run2    RF  Completed  {'max_depth': 25, 'min_samples_split': 5, 'n_e...  1.044299  0.802788  0.974143   2.707952  1.733126  0.840361\n",
      "1  SOH_Prediction_FixedSplit_Run2    GB  Completed  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...  0.906343  0.692743  0.980524   2.164303  1.504994  0.898025\n",
      "2  SOH_Prediction_FixedSplit_Run2   XGB  Completed  {'learning_rate': 0.05, 'max_depth': 5, 'n_est...  0.817876  0.616182  0.984140   2.409535  1.627107  0.873607\n",
      "3  SOH_Prediction_FixedSplit_Run2    LR  Completed                            {'fit_intercept': True}  0.776565  0.591785  0.985702   3.377165  2.059834  0.751708\n",
      "\n",
      "Results summary saved to soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2/experiment_summary_SOH_Prediction_FixedSplit_Run2.csv\n",
      "\n",
      "All plots and artifacts saved in: soh_prediction_outputs/SOH_Prediction_FixedSplit_Run2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Consolidated Experiment Results Summary ---\")\n",
    "\n",
    "results_list_for_df = []\n",
    "if 'final_experiment_results' in locals() and final_experiment_results:\n",
    "    for model_name, model_summary in final_experiment_results.items():\n",
    "        print(f\"\\nModel: {model_name.upper()}\")\n",
    "        print(f\"  Best Params: {model_summary.get('best_params', 'N/A')}\")\n",
    "\n",
    "        val_metrics = model_summary.get('val_metrics', {})\n",
    "        test_metrics = model_summary.get('test_metrics', {})\n",
    "\n",
    "        val_rmse = val_metrics.get('rmse', float('nan'))\n",
    "        val_mae = val_metrics.get('mae', float('nan'))\n",
    "        val_r2 = val_metrics.get('r2', float('nan'))\n",
    "        \n",
    "        test_rmse = test_metrics.get('rmse', float('nan'))\n",
    "        test_mae = test_metrics.get('mae', float('nan'))\n",
    "        test_r2 = test_metrics.get('r2', float('nan'))\n",
    "\n",
    "        print(f\"  Validation Set: RMSE={val_rmse:.4f}, MAE={val_mae:.4f}, R2={val_r2:.4f}\")\n",
    "        print(f\"  Test Set:       RMSE={test_rmse:.4f}, MAE={test_mae:.4f}, R2={test_r2:.4f}\")\n",
    "        \n",
    "        entry = {\n",
    "            'Experiment': EXPERIMENT_NAME, # Using the global experiment name\n",
    "            'Model': model_name.upper(),\n",
    "            'Best Params': str(model_summary.get('best_params', 'N/A')),\n",
    "            'Val RMSE': val_rmse,\n",
    "            'Val MAE': val_mae,\n",
    "            'Val R2': val_r2,\n",
    "            'Test RMSE': test_rmse,\n",
    "            'Test MAE': test_mae,\n",
    "            'Test R2': test_r2,\n",
    "            'Status': 'Completed'\n",
    "        }\n",
    "        results_list_for_df.append(entry)\n",
    "\n",
    "    if results_list_for_df:\n",
    "        df_results_summary = pd.DataFrame(results_list_for_df)\n",
    "        \n",
    "        ordered_cols = [\n",
    "            'Experiment', 'Model', 'Status', 'Best Params',\n",
    "            'Val RMSE', 'Val MAE', 'Val R2',\n",
    "            'Test RMSE', 'Test MAE', 'Test R2'\n",
    "        ]\n",
    "        final_cols_to_use = [col for col in ordered_cols if col in df_results_summary.columns]\n",
    "        df_results_summary = df_results_summary[final_cols_to_use]\n",
    "\n",
    "        print(\"\\n\\n--- Experiment Results DataFrame ---\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 1000) # Adjust for your display\n",
    "        print(df_results_summary)\n",
    "\n",
    "        # Save the results dataframe\n",
    "        csv_path = os.path.join(EXPERIMENT_OUTPUT_DIR, f\"experiment_summary_{EXPERIMENT_NAME}.csv\")\n",
    "        try:\n",
    "            df_results_summary.to_csv(csv_path, index=False)\n",
    "            print(f\"\\nResults summary saved to {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving results summary CSV: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo results to create a summary DataFrame.\")\n",
    "else:\n",
    "    print(\"\\nNo experiment results found to display. Please check the execution of Cell 8.\")\n",
    "\n",
    "print(f\"\\nAll plots and artifacts saved in: {EXPERIMENT_OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
