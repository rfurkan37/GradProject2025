{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb83258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV # <<< ADDED\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance # For HistGBM importance\n",
    "\n",
    "# Assuming train_models.py is in the same directory or in python path\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) # if notebook is in a subfolder\n",
    "# if project_root not in sys.path:\n",
    "#    sys.path.append(project_root)\n",
    "import train_models # Your refactored train_models.py\n",
    "\n",
    "# --- Notebook Specific Configurations ---\n",
    "INPUT_FILE = 'notebook_processed_battery_data_simplified_ALL.csv'\n",
    "TARGET_COL = 'SOH_cycle_capacity_%'\n",
    "EXTRA_EXCLUDE_COLS_FROM_FEATURES = ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
    "MODELS_TO_TUNE_AND_TRAIN = ['rf', 'gb', 'xgb', 'lr'] # Models to apply GridSearchCV to\n",
    "SAVE_MODELS_FLAG = True\n",
    "BASE_OUTPUT_DIR = \"regime_aware_tuned_experiments\" # New directory for tuned results\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define battery regime mapping (as before)\n",
    "BATTERY_REGIME_MAP = {\n",
    "    'battery00': 'regular_constant', 'battery01': 'regular_constant',\n",
    "    'battery10': 'regular_constant', 'battery11': 'regular_constant',\n",
    "    'battery20': 'regular_constant', 'battery21': 'regular_constant',\n",
    "    'battery30': 'regular_constant', 'battery31': 'regular_constant',\n",
    "    'battery40': 'regular_constant', 'battery50': 'regular_constant',\n",
    "    'battery22': 'regular_variable', 'battery23': 'regular_variable',\n",
    "    'battery41': 'regular_variable', 'battery51': 'regular_variable',\n",
    "    'battery52': 'regular_variable',\n",
    "    'battery02': 'recommissioned_two_stage', 'battery12': 'recommissioned_two_stage',\n",
    "    'battery24': 'recommissioned_two_stage', 'battery32': 'recommissioned_two_stage',\n",
    "    'battery53': 'recommissioned_two_stage',\n",
    "    'battery03': 'recommissioned_three_stage', 'battery25': 'recommissioned_three_stage',\n",
    "    'battery33': 'recommissioned_three_stage',\n",
    "}\n",
    "ALL_REGIMES = sorted(list(set(BATTERY_REGIME_MAP.values())))\n",
    "\n",
    "# --- GridSearchCV Configurations ---\n",
    "CV_FOLDS = 3 # Number of cross-validation folds for GridSearchCV (start small, e.g., 2 or 3)\n",
    "GRIDSEARCH_SCORING = 'neg_mean_squared_error' # Metric to optimize for (lower MSE is better)\n",
    "\n",
    "# Define Parameter Grids (start small to test)\n",
    "PARAM_GRIDS = {\n",
    "    'rf': {\n",
    "        'n_estimators': [50, 100], # Default 100\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'gb': { # HistGradientBoostingRegressor\n",
    "        'learning_rate': [0.01, 0.1], # Default 0.1\n",
    "        'max_iter': [100, 200],       # Default 100 (number of trees)\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'l2_regularization': [0, 0.1] # Default 0\n",
    "    },\n",
    "    'xgb': {\n",
    "        'n_estimators': [50, 100, 200], # Default 100\n",
    "        'learning_rate': [0.05, 0.1, 0.3], # Default 0.3\n",
    "        'max_depth': [3, 5, 7],          # Default 6\n",
    "        # 'subsample': [0.8, 1.0],\n",
    "        # 'colsample_bytree': [0.8, 1.0]\n",
    "    },\n",
    "    'lr': { # Linear Regression often doesn't need much tuning,\n",
    "            # but you could tune fit_intercept or other params if using Ridge/Lasso etc.\n",
    "        'fit_intercept': [True, False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab618ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from notebook_processed_battery_data_simplified_ALL.csv. Shape: (8220, 47)\n",
      "--- Data Loaded and Regimes Annotated ---\n",
      "Master DataFrame shape: (8220, 48)\n",
      "Regime counts (batteries per regime):\n",
      "regime\n",
      "recommissioned_three_stage     3\n",
      "recommissioned_two_stage       5\n",
      "regular_constant              10\n",
      "regular_variable               5\n",
      "Name: battery_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data and Annotate Regimes\n",
    "df_master_full = train_models.load_data(INPUT_FILE)\n",
    "if df_master_full is None:\n",
    "    raise ValueError(f\"Failed to load data from {INPUT_FILE}.\")\n",
    "\n",
    "df_master_full['regime'] = df_master_full['battery_id'].map(BATTERY_REGIME_MAP)\n",
    "if df_master_full['regime'].isnull().any():\n",
    "    unmapped_batteries = df_master_full[df_master_full['regime'].isnull()]['battery_id'].unique()\n",
    "    print(f\"Warning: Unmapped batteries: {unmapped_batteries}. Dropping them.\")\n",
    "    df_master_full.dropna(subset=['regime'], inplace=True)\n",
    "\n",
    "print(\"--- Data Loaded and Regimes Annotated ---\")\n",
    "print(f\"Master DataFrame shape: {df_master_full.shape}\")\n",
    "print(\"Regime counts (batteries per regime):\")\n",
    "print(df_master_full.groupby('regime')['battery_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343ca07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Helper Function to Evaluate a Fitted Model\n",
    "def evaluate_fitted_model(model, model_name_str, X_val_data, y_val_data, X_test_data, y_test_data, feature_names_list):\n",
    "    \"\"\"Evaluates a pre-fitted model on validation and test sets and gathers feature importances.\"\"\"\n",
    "    results_metrics = {}\n",
    "    print(f\"Evaluating {model_name_str.upper()}...\")\n",
    "\n",
    "    # Evaluation on Validation Set\n",
    "    val_rmse, val_mae, val_r2 = np.nan, np.nan, np.nan\n",
    "    if X_val_data is not None and not X_val_data.empty and y_val_data is not None and not y_val_data.empty:\n",
    "        try:\n",
    "            y_pred_val = model.predict(X_val_data)\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_val_data, y_pred_val))\n",
    "            val_mae = mean_absolute_error(y_val_data, y_pred_val)\n",
    "            val_r2 = r2_score(y_val_data, y_pred_val)\n",
    "            print(f\"  Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, R2: {val_r2:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error during validation evaluation for {model_name_str}: {e}\")\n",
    "    else:\n",
    "        print(\"  Validation set empty or y_val empty. Skipping validation metrics.\")\n",
    "\n",
    "    # Evaluation on Test Set\n",
    "    test_rmse, test_mae, test_r2 = np.nan, np.nan, np.nan\n",
    "    if X_test_data is not None and not X_test_data.empty and y_test_data is not None and not y_test_data.empty:\n",
    "        try:\n",
    "            y_pred_test = model.predict(X_test_data)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test_data, y_pred_test))\n",
    "            test_mae = mean_absolute_error(y_test_data, y_pred_test)\n",
    "            test_r2 = r2_score(y_test_data, y_pred_test)\n",
    "            print(f\"  Test RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R2: {test_r2:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error during test evaluation for {model_name_str}: {e}\")\n",
    "    else:\n",
    "        print(\"  Test set empty or y_test empty. Skipping test metrics.\")\n",
    "\n",
    "    results_metrics = {\n",
    "        'val_rmse': val_rmse, 'val_mae': val_mae, 'val_r2': val_r2,\n",
    "        'test_rmse': test_rmse, 'test_mae': test_mae, 'test_r2': test_r2\n",
    "    }\n",
    "\n",
    "    # Feature Importances\n",
    "    if feature_names_list:\n",
    "        importances_values = None\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances_values = model.feature_importances_\n",
    "        elif model_name_str == 'lr' and hasattr(model, 'coef_'):\n",
    "            importances_values = np.abs(model.coef_)\n",
    "            if len(importances_values.shape) > 1: importances_values = np.mean(importances_values, axis=0)\n",
    "        elif model_name_str == 'gb': # HistGradientBoostingRegressor - using permutation importance\n",
    "            print(f\"  Calculating permutation importance for {model_name_str.upper()} on validation data...\")\n",
    "            if X_val_data is not None and not X_val_data.empty and y_val_data is not None and not y_val_data.empty and len(X_val_data) > 1:\n",
    "                try:\n",
    "                    perm_result = permutation_importance(model, X_val_data, y_val_data, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "                    importances_values = perm_result.importances_mean\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error calculating permutation importance for {model_name_str}: {e}\")\n",
    "            else:\n",
    "                print(f\"    Skipping permutation importance for {model_name_str}: validation data unsuitable.\")\n",
    "\n",
    "        if importances_values is not None:\n",
    "            if len(feature_names_list) == len(importances_values):\n",
    "                feature_importance_df = pd.DataFrame({'feature': feature_names_list, 'importance': importances_values})\n",
    "                feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False).head(15)\n",
    "                print(f\"\\n  Top 15 Feature Importances for {model_name_str.upper()}:\")\n",
    "                print(feature_importance_df)\n",
    "            else:\n",
    "                print(f\"Warning: Mismatch feature names ({len(feature_names_list)}) & importances ({len(importances_values)}) for {model_name_str}.\")\n",
    "        elif model_name_str not in ['lr', 'gb'] and not hasattr(model, 'feature_importances_'):\n",
    "             print(f\"Warning: Model {model_name_str} has no 'feature_importances_' or 'coef_'.\")\n",
    "\n",
    "    return results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf4e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define Experiment Runner Function (Modified for GridSearchCV)\n",
    "\n",
    "def run_experiment_with_gridsearch(\n",
    "    exp_name, df_full_data, train_battery_ids, val_battery_ids, test_battery_ids,\n",
    "    target_col_name, extra_exclude_cols, models_to_tune, param_grids_dict,\n",
    "    cv_folds_gs, scoring_gs, save_artifacts=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a single experiment with GridSearchCV for hyperparameter tuning:\n",
    "    1. Filters data for train, validation, and test sets.\n",
    "    2. Preprocesses data.\n",
    "    3. Imputes and scales features.\n",
    "    4. For each model, performs GridSearchCV on training data.\n",
    "    5. Evaluates the best model from GridSearchCV on validation and test data.\n",
    "    6. Optionally saves models, scaler, and imputer.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Experiment with GridSearchCV: {exp_name} ---\")\n",
    "    exp_output_dir = os.path.join(BASE_OUTPUT_DIR, exp_name)\n",
    "    os.makedirs(exp_output_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1. Filter Data ---\n",
    "    df_train_exp = df_full_data[df_full_data['battery_id'].isin(train_battery_ids)].copy()\n",
    "    df_val_exp = df_full_data[df_full_data['battery_id'].isin(val_battery_ids)].copy()\n",
    "    df_test_exp = df_full_data[df_full_data['battery_id'].isin(test_battery_ids)].copy()\n",
    "\n",
    "    # Explicitly drop 'regime' if present to avoid warnings in preprocess_data\n",
    "    for df_to_clean in [df_train_exp, df_val_exp, df_test_exp]:\n",
    "        if 'regime' in df_to_clean.columns:\n",
    "            df_to_clean.drop(columns=['regime'], inplace=True)\n",
    "\n",
    "    if df_train_exp.empty: # Val/Test can be empty if not enough batteries, but train must exist\n",
    "        print(f\"ERROR: Training data is empty for experiment {exp_name}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # Drop extra feature columns BEFORE preprocessing\n",
    "    cols_to_drop = [col for col in extra_exclude_cols if col in df_train_exp.columns]\n",
    "    if cols_to_drop:\n",
    "        df_train_exp.drop(columns=cols_to_drop, inplace=True)\n",
    "        if not df_val_exp.empty: df_val_exp.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "        if not df_test_exp.empty: df_test_exp.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "        print(f\"Dropped extra feature columns: {cols_to_drop}\")\n",
    "\n",
    "    # --- 2. Preprocess Data ---\n",
    "    # preprocess_data returns X (features), y (target), battery_ids (not used here), feature_names\n",
    "    X_train, y_train, _, actual_feature_names = train_models.preprocess_data(df_train_exp, target_col_name)\n",
    "    \n",
    "    X_val, y_val = (None, None)\n",
    "    if not df_val_exp.empty:\n",
    "        X_val, y_val, _, _ = train_models.preprocess_data(df_val_exp, target_col_name)\n",
    "\n",
    "    X_test, y_test = (None, None)\n",
    "    if not df_test_exp.empty:\n",
    "        X_test, y_test, _, _ = train_models.preprocess_data(df_test_exp, target_col_name)\n",
    "\n",
    "    if X_train is None or X_train.empty:\n",
    "        print(f\"Error: X_train is None or empty after preprocessing for exp: {exp_name}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 3. Impute & Scale ---\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    \n",
    "    X_val_imputed = None\n",
    "    if X_val is not None and not X_val.empty:\n",
    "        X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "    \n",
    "    X_test_imputed = None\n",
    "    if X_test is not None and not X_test.empty:\n",
    "        X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, scaler = train_models.scale_features(\n",
    "        X_train_imputed, X_val_imputed, X_test_imputed # Pass potentially None X_val/X_test\n",
    "    )\n",
    "    \n",
    "    experiment_results = {}\n",
    "    trained_best_models = {}\n",
    "\n",
    "    # --- 4. GridSearchCV for each model ---\n",
    "    for model_name_key in models_to_tune:\n",
    "        print(f\"\\n-- Tuning {model_name_key.upper()} for experiment {exp_name} --\")\n",
    "        \n",
    "        current_X_train_data = X_train_imputed\n",
    "        current_X_val_data = X_val_imputed\n",
    "        current_X_test_data = X_test_imputed\n",
    "        \n",
    "        # Select appropriate dataset (scaled or unscaled) for tuning and final evaluation\n",
    "        if model_name_key in ['lr', 'gb']:\n",
    "            current_X_train_data = X_train_scaled\n",
    "            current_X_val_data = X_val_scaled\n",
    "            current_X_test_data = X_test_scaled\n",
    "            print(f\"  Using SCALED data for {model_name_key.upper()}\")\n",
    "        else:\n",
    "            print(f\"  Using IMPUTED (unscaled) data for {model_name_key.upper()}\")\n",
    "\n",
    "        base_model = None\n",
    "        if model_name_key == 'rf': base_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        elif model_name_key == 'gb': base_model = HistGradientBoostingRegressor(random_state=42) # Early stopping auto\n",
    "        elif model_name_key == 'xgb': base_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        elif model_name_key == 'lr': base_model = LinearRegression()\n",
    "        else:\n",
    "            print(f\"Warning: Unknown model '{model_name_key}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        if not param_grids_dict.get(model_name_key):\n",
    "            print(f\"  No parameter grid for {model_name_key}. Fitting with defaults.\")\n",
    "            base_model.fit(current_X_train_data, y_train)\n",
    "            best_model_found = base_model\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=base_model,\n",
    "                param_grid=param_grids_dict[model_name_key],\n",
    "                scoring=scoring_gs,\n",
    "                cv=cv_folds_gs,\n",
    "                verbose=1,\n",
    "                n_jobs=-1 \n",
    "            )\n",
    "            print(f\"  Starting GridSearchCV for {model_name_key.upper()}...\")\n",
    "            grid_search.fit(current_X_train_data, y_train)\n",
    "            print(f\"  Best parameters for {model_name_key.upper()}: {grid_search.best_params_}\")\n",
    "            best_model_found = grid_search.best_estimator_ # Already refitted on full train data\n",
    "\n",
    "        trained_best_models[model_name_key] = best_model_found\n",
    "        \n",
    "        # --- 5. Evaluate the BEST model ---\n",
    "        # Ensure y_val and y_test are Series, even if empty, for the evaluation function\n",
    "        y_val_safe = y_val if (y_val is not None and not y_val.empty) else pd.Series(dtype=y_train.dtype)\n",
    "        y_test_safe = y_test if (y_test is not None and not y_test.empty) else pd.Series(dtype=y_train.dtype)\n",
    "\n",
    "        metrics = evaluate_fitted_model(\n",
    "            best_model_found, model_name_key,\n",
    "            current_X_val_data, y_val_safe,\n",
    "            current_X_test_data, y_test_safe,\n",
    "            actual_feature_names # Pass the correct feature names\n",
    "        )\n",
    "        experiment_results[model_name_key] = metrics\n",
    "\n",
    "    # --- 6. Save Artifacts ---\n",
    "    if save_artifacts:\n",
    "        joblib.dump(scaler, os.path.join(exp_output_dir, f\"scaler_{exp_name}.joblib\"))\n",
    "        joblib.dump(imputer, os.path.join(exp_output_dir, f\"imputer_{exp_name}.joblib\"))\n",
    "        print(f\"\\nScaler and Imputer saved for {exp_name} in {exp_output_dir}\")\n",
    "        if SAVE_MODELS_FLAG:\n",
    "            for model_name_key, model_instance in trained_best_models.items():\n",
    "                model_filename = os.path.join(exp_output_dir, f\"{model_name_key}_tuned_{exp_name}.joblib\")\n",
    "                joblib.dump(model_instance, model_filename)\n",
    "            print(f\"Tuned models saved for {exp_name} in {exp_output_dir}\")\n",
    "            \n",
    "    return experiment_results, trained_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52eb1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defined 7 experiment scenarios for tuning.\n",
      "  1. IntraRegime_RC_Tuned\n",
      "    Train Batts=8, Val Batts=1, Test Batts=1\n",
      "  2. IntraRegime_RV_Tuned\n",
      "    Train Batts=3, Val Batts=1, Test Batts=1\n",
      "  3. IntraRegime_R2S_Tuned\n",
      "    Train Batts=3, Val Batts=1, Test Batts=1\n",
      "  4. IntraRegime_R3S_Tuned\n",
      "    Train Batts=1, Val Batts=1, Test Batts=1\n",
      "  5. CrossRegime_TrainReg_TestRecomm_Tuned\n",
      "    Train Batts=12, Val Batts=3, Test Batts=8\n",
      "  6. CrossRegime_TrainRecomm_TestReg_Tuned\n",
      "    Train Batts=7, Val Batts=1, Test Batts=15\n",
      "  7. TrainCombined_MultiTest_Tuned\n",
      "    Train Batts=16, Val Batts=3\n",
      "    Test Batts (Regular)=2\n",
      "    Test Batts (Recomm)=2\n",
      "    Test Batts (Overall Combined for main run)=4\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Define Experiment Scenarios\n",
    "all_battery_ids_by_regime = df_master_full.groupby('regime')['battery_id'].unique().to_dict()\n",
    "for regime, ids in all_battery_ids_by_regime.items():\n",
    "    all_battery_ids_by_regime[regime] = list(ids) # Ensure they are lists\n",
    "\n",
    "experiment_configs = []\n",
    "np.random.seed(42) # For reproducible shuffles\n",
    "\n",
    "# --- Scenario 1: Intra-Regime - Regular Constant (RC) ---\n",
    "rc_ids_orig = list(all_battery_ids_by_regime.get('regular_constant', []))\n",
    "np.random.shuffle(rc_ids_orig) # Shuffle for random assignment\n",
    "if len(rc_ids_orig) >= 3:\n",
    "    # Adjust split logic to ensure train, val, and test are not empty if possible\n",
    "    test_rc_count = 1\n",
    "    val_rc_count = 1 if len(rc_ids_orig) - test_rc_count >= 2 else 0 # Need at least 1 for train\n",
    "    train_rc_count = len(rc_ids_orig) - test_rc_count - val_rc_count\n",
    "    if train_rc_count > 0 :\n",
    "      experiment_configs.append({\n",
    "          'name': 'IntraRegime_RC_Tuned',\n",
    "          'train_battery_ids': rc_ids_orig[:train_rc_count],\n",
    "          'val_battery_ids': rc_ids_orig[train_rc_count : train_rc_count + val_rc_count] if val_rc_count > 0 else rc_ids_orig[:train_rc_count], # Use train as val if no dedicated val\n",
    "          'test_battery_ids': rc_ids_orig[train_rc_count + val_rc_count:],\n",
    "      })\n",
    "\n",
    "# --- Scenario 2: Intra-Regime - Regular Variable (RV) ---\n",
    "rv_ids_orig = list(all_battery_ids_by_regime.get('regular_variable', []))\n",
    "np.random.shuffle(rv_ids_orig)\n",
    "if len(rv_ids_orig) >= 3:\n",
    "    test_rv_count = 1\n",
    "    val_rv_count = 1 if len(rv_ids_orig) - test_rv_count >= 2 else 0\n",
    "    train_rv_count = len(rv_ids_orig) - test_rv_count - val_rv_count\n",
    "    if train_rv_count > 0:\n",
    "      experiment_configs.append({\n",
    "          'name': 'IntraRegime_RV_Tuned',\n",
    "          'train_battery_ids': rv_ids_orig[:train_rv_count],\n",
    "          'val_battery_ids': rv_ids_orig[train_rv_count : train_rv_count + val_rv_count] if val_rv_count > 0 else rv_ids_orig[:train_rv_count],\n",
    "          'test_battery_ids': rv_ids_orig[train_rv_count + val_rv_count:],\n",
    "      })\n",
    "\n",
    "# --- Scenario 3: Intra-Regime - Recommissioned Two-Stage (R2S) ---\n",
    "r2s_ids_orig = list(all_battery_ids_by_regime.get('recommissioned_two_stage', []))\n",
    "np.random.shuffle(r2s_ids_orig)\n",
    "if len(r2s_ids_orig) >= 3:\n",
    "    test_r2s_count = 1\n",
    "    val_r2s_count = 1 if len(r2s_ids_orig) - test_r2s_count >= 2 else 0\n",
    "    train_r2s_count = len(r2s_ids_orig) - test_r2s_count - val_r2s_count\n",
    "    if train_r2s_count > 0:\n",
    "      experiment_configs.append({\n",
    "          'name': 'IntraRegime_R2S_Tuned',\n",
    "          'train_battery_ids': r2s_ids_orig[:train_r2s_count],\n",
    "          'val_battery_ids': r2s_ids_orig[train_r2s_count : train_r2s_count + val_r2s_count] if val_r2s_count > 0 else r2s_ids_orig[:train_r2s_count],\n",
    "          'test_battery_ids': r2s_ids_orig[train_r2s_count + val_r2s_count:],\n",
    "      })\n",
    "\n",
    "# --- Scenario 4: Intra-Regime - Recommissioned Three-Stage (R3S) ---\n",
    "r3s_ids_orig = list(all_battery_ids_by_regime.get('recommissioned_three_stage', []))\n",
    "np.random.shuffle(r3s_ids_orig)\n",
    "if len(r3s_ids_orig) >= 3: # Requires 1 for train, 1 for val, 1 for test\n",
    "    experiment_configs.append({\n",
    "        'name': 'IntraRegime_R3S_Tuned',\n",
    "        'train_battery_ids': [r3s_ids_orig[0]],\n",
    "        'val_battery_ids': [r3s_ids_orig[1]],\n",
    "        'test_battery_ids': [r3s_ids_orig[2]],\n",
    "    })\n",
    "elif len(r3s_ids_orig) == 2: # Train/Val are same, Test is different\n",
    "     experiment_configs.append({\n",
    "        'name': 'IntraRegime_R3S_Tuned_TrainValEq_Test',\n",
    "        'train_battery_ids': [r3s_ids_orig[0]],\n",
    "        'val_battery_ids': [r3s_ids_orig[0]], # Use train also as val\n",
    "        'test_battery_ids': [r3s_ids_orig[1]],\n",
    "    })\n",
    "\n",
    "# --- Scenario 5: Cross-Regime - Train ALL Regular, Test ALL Recomm ---\n",
    "all_reg_ids_for_s5 = list(set(all_battery_ids_by_regime.get('regular_constant', []) + all_battery_ids_by_regime.get('regular_variable', [])))\n",
    "all_recomm_ids_for_s5 = list(set(all_battery_ids_by_regime.get('recommissioned_two_stage', []) + all_battery_ids_by_regime.get('recommissioned_three_stage', [])))\n",
    "np.random.shuffle(all_reg_ids_for_s5)\n",
    "\n",
    "if len(all_reg_ids_for_s5) >= 2 and all_recomm_ids_for_s5:\n",
    "    val_count_s5 = max(1, int(0.2 * len(all_reg_ids_for_s5)))\n",
    "    if len(all_reg_ids_for_s5) - val_count_s5 == 0 : val_count_s5 = len(all_reg_ids_for_s5) - 1 # ensure at least 1 for train\n",
    "    \n",
    "    experiment_configs.append({\n",
    "        'name': 'CrossRegime_TrainReg_TestRecomm_Tuned',\n",
    "        'train_battery_ids': all_reg_ids_for_s5[:-val_count_s5],\n",
    "        'val_battery_ids': all_reg_ids_for_s5[-val_count_s5:],\n",
    "        'test_battery_ids': all_recomm_ids_for_s5,\n",
    "    })\n",
    "\n",
    "# --- Scenario 6: Cross-Regime - Train ALL Recomm, Test ALL Regular ---\n",
    "all_recomm_ids_for_s6 = list(set(all_battery_ids_by_regime.get('recommissioned_two_stage', []) + all_battery_ids_by_regime.get('recommissioned_three_stage', [])))\n",
    "all_reg_ids_for_s6 = list(set(all_battery_ids_by_regime.get('regular_constant', []) + all_battery_ids_by_regime.get('regular_variable', [])))\n",
    "np.random.shuffle(all_recomm_ids_for_s6)\n",
    "\n",
    "if len(all_recomm_ids_for_s6) >= 2 and all_reg_ids_for_s6:\n",
    "    val_count_s6 = max(1, int(0.2 * len(all_recomm_ids_for_s6)))\n",
    "    if len(all_recomm_ids_for_s6) - val_count_s6 == 0 : val_count_s6 = len(all_recomm_ids_for_s6) -1\n",
    "    \n",
    "    experiment_configs.append({\n",
    "        'name': 'CrossRegime_TrainRecomm_TestReg_Tuned',\n",
    "        'train_battery_ids': all_recomm_ids_for_s6[:-val_count_s6],\n",
    "        'val_battery_ids': all_recomm_ids_for_s6[-val_count_s6:],\n",
    "        'test_battery_ids': all_reg_ids_for_s6,\n",
    "    })\n",
    "\n",
    "# --- Scenario 7: Train on ALL (Regular + Recommissioned), Test on unseen Regular, Test on unseen Recommissioned ---\n",
    "all_reg_constant_s7 = list(all_battery_ids_by_regime.get('regular_constant', [])); np.random.shuffle(all_reg_constant_s7)\n",
    "all_reg_variable_s7 = list(all_battery_ids_by_regime.get('regular_variable', [])); np.random.shuffle(all_reg_variable_s7)\n",
    "all_recomm_2stage_s7 = list(all_battery_ids_by_regime.get('recommissioned_two_stage', [])); np.random.shuffle(all_recomm_2stage_s7)\n",
    "all_recomm_3stage_s7 = list(all_battery_ids_by_regime.get('recommissioned_three_stage', [])); np.random.shuffle(all_recomm_3stage_s7)\n",
    "\n",
    "# Define number of batteries for specific test sets (ensure they are small enough to leave data for train/val)\n",
    "n_test_reg_c = min(1, len(all_reg_constant_s7) // 3) if len(all_reg_constant_s7) >=3 else (1 if len(all_reg_constant_s7) == 2 else 0)\n",
    "n_test_reg_v = min(1, len(all_reg_variable_s7) // 3) if len(all_reg_variable_s7) >=3 else (1 if len(all_reg_variable_s7) == 2 else 0)\n",
    "n_test_recomm_2s = min(1, len(all_recomm_2stage_s7) // 3) if len(all_recomm_2stage_s7) >=3 else (1 if len(all_recomm_2stage_s7) == 2 else 0)\n",
    "n_test_recomm_3s = min(1, len(all_recomm_3stage_s7) // 3) if len(all_recomm_3stage_s7) >=3 else (1 if len(all_recomm_3stage_s7) == 2 else 0)\n",
    "\n",
    "\n",
    "final_test_regular_batteries_s7 = all_reg_constant_s7[:n_test_reg_c] + all_reg_variable_s7[:n_test_reg_v]\n",
    "final_test_recomm_batteries_s7 = all_recomm_2stage_s7[:n_test_recomm_2s] + all_recomm_3stage_s7[:n_test_recomm_3s]\n",
    "\n",
    "pool_rc_s7 = all_reg_constant_s7[n_test_reg_c:]\n",
    "pool_rv_s7 = all_reg_variable_s7[n_test_reg_v:]\n",
    "pool_r2s_s7 = all_recomm_2stage_s7[n_test_recomm_2s:]\n",
    "pool_r3s_s7 = all_recomm_3stage_s7[n_test_recomm_3s:]\n",
    "\n",
    "training_validation_pool_s7 = pool_rc_s7 + pool_rv_s7 + pool_r2s_s7 + pool_r3s_s7\n",
    "np.random.shuffle(training_validation_pool_s7)\n",
    "\n",
    "if len(training_validation_pool_s7) >= 2:\n",
    "    val_pool_size_s7 = max(1, int(0.20 * len(training_validation_pool_s7)))\n",
    "    if len(training_validation_pool_s7) - val_pool_size_s7 == 0: # Not enough for distinct train & val\n",
    "        train_pool_size_s7 = len(training_validation_pool_s7)\n",
    "        val_combined_batteries_s7 = training_validation_pool_s7 # Use all for val as well\n",
    "    else:\n",
    "        train_pool_size_s7 = len(training_validation_pool_s7) - val_pool_size_s7\n",
    "        val_combined_batteries_s7 = training_validation_pool_s7[train_pool_size_s7:]\n",
    "        \n",
    "    train_combined_batteries_s7 = training_validation_pool_s7[:train_pool_size_s7]\n",
    "\n",
    "    if train_combined_batteries_s7 and val_combined_batteries_s7 and (final_test_regular_batteries_s7 or final_test_recomm_batteries_s7):\n",
    "        experiment_configs.append({\n",
    "            'name': 'TrainCombined_MultiTest_Tuned',\n",
    "            'train_battery_ids': train_combined_batteries_s7,\n",
    "            'val_battery_ids': val_combined_batteries_s7,\n",
    "            'test_battery_ids_regular': final_test_regular_batteries_s7,\n",
    "            'test_battery_ids_recomm': final_test_recomm_batteries_s7,\n",
    "            'test_battery_ids': final_test_regular_batteries_s7 + final_test_recomm_batteries_s7, # Combined for initial pass\n",
    "            'is_multitest': True\n",
    "        })\n",
    "    else:\n",
    "        print(\"Warning: Not enough batteries for 'TrainCombined_MultiTest_Tuned' scenario after reserving test sets.\")\n",
    "else:\n",
    "    print(\"Warning: Training/validation pool too small for 'TrainCombined_MultiTest_Tuned' scenario.\")\n",
    "\n",
    "print(f\"\\nDefined {len(experiment_configs)} experiment scenarios for tuning.\")\n",
    "for i, config in enumerate(experiment_configs):\n",
    "    print(f\"  {i+1}. {config['name']}\")\n",
    "    if config.get('is_multitest'):\n",
    "        print(f\"    Train Batts={len(config['train_battery_ids'])}, Val Batts={len(config['val_battery_ids'])}\")\n",
    "        print(f\"    Test Batts (Regular)={len(config['test_battery_ids_regular'])}\")\n",
    "        print(f\"    Test Batts (Recomm)={len(config['test_battery_ids_recomm'])}\")\n",
    "        print(f\"    Test Batts (Overall Combined for main run)={len(config['test_battery_ids'])}\")\n",
    "    else:\n",
    "        print(f\"    Train Batts={len(config['train_battery_ids'])}, Val Batts={len(config['val_battery_ids'])}, Test Batts={len(config.get('test_battery_ids', 'N/A'))}\")\n",
    "    # Sanity checks for overlaps\n",
    "    if 'test_battery_ids' in config and not set(config['train_battery_ids']).isdisjoint(set(config['test_battery_ids'])):\n",
    "         if not (len(config['test_battery_ids']) == 1 and config['test_battery_ids'][0] in config['train_battery_ids'] and len(config['train_battery_ids'])==1) : # allow if train=test with 1 battery\n",
    "            print(f\"    WARNING: Overlap between train and general test battery IDs for {config['name']}\")\n",
    "    if 'val_battery_ids' in config and not set(config['train_battery_ids']).isdisjoint(set(config['val_battery_ids'])):\n",
    "        if not (config['val_battery_ids'] == config['train_battery_ids']): # Allow val==train\n",
    "            print(f\"    WARNING: Overlap between train and val battery IDs for {config['name']} (and val is not identical to train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6635929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Experiment 1/7: IntraRegime_RC_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: IntraRegime_RC_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (2082, 41)\n",
      "Shape of y: (2082,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (865, 41)\n",
      "Shape of y: (865,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (81, 41)\n",
      "Shape of y: (81,)\n",
      "\n",
      "-- Tuning RF for experiment IntraRegime_RC_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 1.9741, MAE: 0.9968, R2: 0.9195\n",
      "  Test RMSE: 5.6243, MAE: 1.8263, R2: 0.4183\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.638536\n",
      "22          start_voltage_V    0.185795\n",
      "32         voltage_kurtosis    0.048746\n",
      "29            temp_skewness    0.023289\n",
      "14     discharge_duration_s    0.019569\n",
      "38         voltage_skewness    0.010309\n",
      "2             avg_voltage_V    0.008732\n",
      "23            temp_kurtosis    0.007518\n",
      "21             start_temp_C    0.006447\n",
      "33            voltage_p10_V    0.006430\n",
      "34            voltage_p25_V    0.005844\n",
      "16            end_voltage_V    0.005269\n",
      "40      voltage_variance_V2    0.003593\n",
      "36            voltage_p75_V    0.003531\n",
      "13          delta_voltage_V    0.003205\n",
      "\n",
      "-- Tuning GB for experiment IntraRegime_RC_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0, 'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 100}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 2.2845, MAE: 1.2195, R2: 0.8922\n",
      "  Test RMSE: 4.0013, MAE: 1.5292, R2: 0.7056\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.799847\n",
      "22          start_voltage_V    0.186872\n",
      "29            temp_skewness    0.021480\n",
      "13          delta_voltage_V    0.019886\n",
      "32         voltage_kurtosis    0.018751\n",
      "14     discharge_duration_s    0.016090\n",
      "21             start_temp_C    0.008959\n",
      "16            end_voltage_V    0.003100\n",
      "33            voltage_p10_V    0.001088\n",
      "9          current_skewness    0.000597\n",
      "8             current_p90_A    0.000558\n",
      "34            voltage_p25_V    0.000413\n",
      "30               temp_std_C    0.000245\n",
      "39            voltage_std_V    0.000226\n",
      "10            current_std_A    0.000194\n",
      "\n",
      "-- Tuning XGB for experiment IntraRegime_RC_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 200}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 2.2409, MAE: 1.4775, R2: 0.8963\n",
      "  Test RMSE: 4.4920, MAE: 1.0706, R2: 0.6290\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.400247\n",
      "22          start_voltage_V    0.175907\n",
      "32         voltage_kurtosis    0.083124\n",
      "14     discharge_duration_s    0.075753\n",
      "29            temp_skewness    0.040526\n",
      "13          delta_voltage_V    0.035826\n",
      "34            voltage_p25_V    0.024673\n",
      "16            end_voltage_V    0.021517\n",
      "21             start_temp_C    0.014369\n",
      "2             avg_voltage_V    0.013973\n",
      "23            temp_kurtosis    0.013605\n",
      "38         voltage_skewness    0.010554\n",
      "12             delta_temp_C    0.010403\n",
      "3          current_kurtosis    0.009683\n",
      "20             q_initial_Ah    0.008966\n",
      "\n",
      "-- Tuning LR for experiment IntraRegime_RC_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 1.6283, MAE: 0.8780, R2: 0.9452\n",
      "  Test RMSE: 2.6848, MAE: 2.0939, R2: 0.8675\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "8          current_p90_A   45.103845\n",
      "0          avg_current_A   35.903473\n",
      "1             avg_temp_C   24.928781\n",
      "5          current_p25_A   15.040186\n",
      "2          avg_voltage_V   14.637150\n",
      "19            max_temp_C   14.426455\n",
      "14  discharge_duration_s   11.932462\n",
      "6          current_p50_A   10.326133\n",
      "15            end_temp_C    9.911289\n",
      "12          delta_temp_C    9.831093\n",
      "24            temp_p10_C    7.776220\n",
      "34         voltage_p25_V    6.543335\n",
      "28            temp_p90_C    6.149248\n",
      "11   current_variance_A2    6.096433\n",
      "7          current_p75_A    5.586870\n",
      "\n",
      "Scaler and Imputer saved for IntraRegime_RC_Tuned in regime_aware_tuned_experiments/IntraRegime_RC_Tuned\n",
      "Tuned models saved for IntraRegime_RC_Tuned in regime_aware_tuned_experiments/IntraRegime_RC_Tuned\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting Experiment 2/7: IntraRegime_RV_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: IntraRegime_RV_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (985, 41)\n",
      "Shape of y: (985,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (258, 41)\n",
      "Shape of y: (258,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (413, 41)\n",
      "Shape of y: (413,)\n",
      "\n",
      "-- Tuning RF for experiment IntraRegime_RV_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 3.9115, MAE: 3.1552, R2: 0.6876\n",
      "  Test RMSE: 2.3880, MAE: 1.5275, R2: 0.8713\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "37            voltage_p90_V    0.502190\n",
      "22          start_voltage_V    0.102029\n",
      "32         voltage_kurtosis    0.063090\n",
      "33            voltage_p10_V    0.054712\n",
      "36            voltage_p75_V    0.052046\n",
      "2             avg_voltage_V    0.035695\n",
      "14     discharge_duration_s    0.031770\n",
      "35            voltage_p50_V    0.024714\n",
      "13          delta_voltage_V    0.015946\n",
      "40      voltage_variance_V2    0.013022\n",
      "16            end_voltage_V    0.012837\n",
      "25               temp_p25_C    0.012234\n",
      "39            voltage_std_V    0.009333\n",
      "17  internal_resistance_ohm    0.008155\n",
      "34            voltage_p25_V    0.007695\n",
      "\n",
      "-- Tuning GB for experiment IntraRegime_RV_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0.1, 'learning_rate': 0.01, 'max_depth': None, 'max_iter': 200}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 4.7544, MAE: 2.9563, R2: 0.5384\n",
      "  Test RMSE: 3.3162, MAE: 2.1012, R2: 0.7519\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                    feature  importance\n",
      "37            voltage_p90_V    0.412729\n",
      "22          start_voltage_V    0.079628\n",
      "14     discharge_duration_s    0.069841\n",
      "13          delta_voltage_V    0.041709\n",
      "38         voltage_skewness    0.030478\n",
      "39            voltage_std_V    0.028062\n",
      "17  internal_resistance_ohm    0.023418\n",
      "34            voltage_p25_V    0.021440\n",
      "33            voltage_p10_V    0.020523\n",
      "2             avg_voltage_V    0.013758\n",
      "36            voltage_p75_V    0.012995\n",
      "25               temp_p25_C    0.006347\n",
      "21             start_temp_C    0.005018\n",
      "30               temp_std_C    0.004738\n",
      "0             avg_current_A    0.004282\n",
      "\n",
      "-- Tuning XGB for experiment IntraRegime_RV_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 2.3535, MAE: 2.0228, R2: 0.8869\n",
      "  Test RMSE: 3.0362, MAE: 1.9476, R2: 0.7920\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                 feature  importance\n",
      "37         voltage_p90_V    0.315148\n",
      "1             avg_temp_C    0.206919\n",
      "36         voltage_p75_V    0.196314\n",
      "33         voltage_p10_V    0.039407\n",
      "22       start_voltage_V    0.035724\n",
      "13       delta_voltage_V    0.034089\n",
      "32      voltage_kurtosis    0.030140\n",
      "16         end_voltage_V    0.027585\n",
      "2          avg_voltage_V    0.024859\n",
      "14  discharge_duration_s    0.013038\n",
      "39         voltage_std_V    0.010677\n",
      "21          start_temp_C    0.009504\n",
      "7          current_p75_A    0.009453\n",
      "3       current_kurtosis    0.007372\n",
      "38      voltage_skewness    0.006144\n",
      "\n",
      "-- Tuning LR for experiment IntraRegime_RV_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 4.6750, MAE: 4.4871, R2: 0.5537\n",
      "  Test RMSE: 7.5999, MAE: 5.0793, R2: -0.3031\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "0          avg_current_A   47.406193\n",
      "1             avg_temp_C   22.217236\n",
      "4          current_p10_A   18.638339\n",
      "6          current_p50_A   18.518813\n",
      "14  discharge_duration_s   18.518007\n",
      "8          current_p90_A   12.280520\n",
      "10         current_std_A   11.784751\n",
      "2          avg_voltage_V   11.061341\n",
      "31      temp_variance_C2    9.731968\n",
      "18    is_reference_cycle    9.661952\n",
      "11   current_variance_A2    9.064877\n",
      "21          start_temp_C    8.386720\n",
      "5          current_p25_A    7.329617\n",
      "40   voltage_variance_V2    6.347577\n",
      "35         voltage_p50_V    5.769673\n",
      "\n",
      "Scaler and Imputer saved for IntraRegime_RV_Tuned in regime_aware_tuned_experiments/IntraRegime_RV_Tuned\n",
      "Tuned models saved for IntraRegime_RV_Tuned in regime_aware_tuned_experiments/IntraRegime_RV_Tuned\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting Experiment 3/7: IntraRegime_R2S_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: IntraRegime_R2S_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (1659, 41)\n",
      "Shape of y: (1659,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (355, 41)\n",
      "Shape of y: (355,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (209, 41)\n",
      "Shape of y: (209,)\n",
      "\n",
      "-- Tuning RF for experiment IntraRegime_R2S_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 4.7795, MAE: 3.8678, R2: 0.5735\n",
      "  Test RMSE: 5.6327, MAE: 4.6905, R2: -0.2834\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.402873\n",
      "22          start_voltage_V    0.076759\n",
      "14     discharge_duration_s    0.061123\n",
      "26               temp_p50_C    0.050095\n",
      "28               temp_p90_C    0.042566\n",
      "34            voltage_p25_V    0.037725\n",
      "1                avg_temp_C    0.036466\n",
      "32         voltage_kurtosis    0.034962\n",
      "27               temp_p75_C    0.030963\n",
      "5             current_p25_A    0.024006\n",
      "33            voltage_p10_V    0.021554\n",
      "19               max_temp_C    0.018520\n",
      "29            temp_skewness    0.018078\n",
      "31         temp_variance_C2    0.015598\n",
      "30               temp_std_C    0.012910\n",
      "\n",
      "-- Tuning GB for experiment IntraRegime_R2S_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 200}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 4.4148, MAE: 3.8513, R2: 0.6361\n",
      "  Test RMSE: 6.9623, MAE: 5.7863, R2: -0.9608\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                    feature  importance\n",
      "22          start_voltage_V    0.288760\n",
      "17  internal_resistance_ohm    0.178832\n",
      "32         voltage_kurtosis    0.062836\n",
      "14     discharge_duration_s    0.051580\n",
      "29            temp_skewness    0.029556\n",
      "13          delta_voltage_V    0.029192\n",
      "10            current_std_A    0.018871\n",
      "16            end_voltage_V    0.017016\n",
      "33            voltage_p10_V    0.015781\n",
      "23            temp_kurtosis    0.012766\n",
      "26               temp_p50_C    0.007754\n",
      "30               temp_std_C    0.006918\n",
      "36            voltage_p75_V    0.006808\n",
      "3          current_kurtosis    0.004492\n",
      "25               temp_p25_C    0.003457\n",
      "\n",
      "-- Tuning XGB for experiment IntraRegime_R2S_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 2.6351, MAE: 1.7277, R2: 0.8704\n",
      "  Test RMSE: 2.4440, MAE: 1.8911, R2: 0.7584\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                    feature  importance\n",
      "6             current_p50_A    0.462032\n",
      "7             current_p75_A    0.299116\n",
      "17  internal_resistance_ohm    0.060626\n",
      "22          start_voltage_V    0.028105\n",
      "36            voltage_p75_V    0.024694\n",
      "30               temp_std_C    0.023887\n",
      "29            temp_skewness    0.022121\n",
      "14     discharge_duration_s    0.019273\n",
      "1                avg_temp_C    0.016156\n",
      "32         voltage_kurtosis    0.011455\n",
      "39            voltage_std_V    0.005777\n",
      "33            voltage_p10_V    0.004812\n",
      "15               end_temp_C    0.003720\n",
      "34            voltage_p25_V    0.003681\n",
      "2             avg_voltage_V    0.002430\n",
      "\n",
      "-- Tuning LR for experiment IntraRegime_R2S_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 5.6585, MAE: 3.9997, R2: 0.4022\n",
      "  Test RMSE: 7.0786, MAE: 6.2172, R2: -1.0268\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "0          avg_current_A  258.356412\n",
      "5          current_p25_A   86.601840\n",
      "6          current_p50_A   62.376890\n",
      "7          current_p75_A   44.335504\n",
      "4          current_p10_A   42.170875\n",
      "10         current_std_A   21.282781\n",
      "8          current_p90_A   18.761813\n",
      "40   voltage_variance_V2   16.757222\n",
      "39         voltage_std_V   14.457172\n",
      "14  discharge_duration_s   13.016783\n",
      "11   current_variance_A2    9.256784\n",
      "30            temp_std_C    8.471112\n",
      "31      temp_variance_C2    7.107763\n",
      "2          avg_voltage_V    6.265282\n",
      "19            max_temp_C    4.825035\n",
      "\n",
      "Scaler and Imputer saved for IntraRegime_R2S_Tuned in regime_aware_tuned_experiments/IntraRegime_R2S_Tuned\n",
      "Tuned models saved for IntraRegime_R2S_Tuned in regime_aware_tuned_experiments/IntraRegime_R2S_Tuned\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting Experiment 4/7: IntraRegime_R3S_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: IntraRegime_R3S_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (474, 41)\n",
      "Shape of y: (474,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (421, 41)\n",
      "Shape of y: (421,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (418, 41)\n",
      "Shape of y: (418,)\n",
      "\n",
      "-- Tuning RF for experiment IntraRegime_R3S_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 5.6205, MAE: 4.5035, R2: 0.0274\n",
      "  Test RMSE: 5.7643, MAE: 4.9942, R2: 0.3015\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "26               temp_p50_C    0.250888\n",
      "29            temp_skewness    0.208069\n",
      "14     discharge_duration_s    0.125363\n",
      "33            voltage_p10_V    0.118175\n",
      "22          start_voltage_V    0.098620\n",
      "25               temp_p25_C    0.040674\n",
      "28               temp_p90_C    0.028777\n",
      "17  internal_resistance_ohm    0.025845\n",
      "34            voltage_p25_V    0.015772\n",
      "35            voltage_p50_V    0.015371\n",
      "32         voltage_kurtosis    0.007440\n",
      "6             current_p50_A    0.005852\n",
      "5             current_p25_A    0.005005\n",
      "0             avg_current_A    0.004848\n",
      "38         voltage_skewness    0.004810\n",
      "\n",
      "-- Tuning GB for experiment IntraRegime_R3S_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 100}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 6.1832, MAE: 5.2622, R2: -0.1771\n",
      "  Test RMSE: 6.5427, MAE: 5.6715, R2: 0.1001\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                 feature  importance\n",
      "32      voltage_kurtosis    0.591988\n",
      "14  discharge_duration_s    0.243198\n",
      "26            temp_p50_C    0.147754\n",
      "22       start_voltage_V    0.131436\n",
      "0          avg_current_A    0.094725\n",
      "12          delta_temp_C    0.034941\n",
      "2          avg_voltage_V    0.032400\n",
      "21          start_temp_C    0.024366\n",
      "33         voltage_p10_V    0.021691\n",
      "37         voltage_p90_V    0.020654\n",
      "7          current_p75_A    0.017015\n",
      "39         voltage_std_V    0.011808\n",
      "34         voltage_p25_V    0.010499\n",
      "3       current_kurtosis    0.009414\n",
      "9       current_skewness    0.009233\n",
      "\n",
      "-- Tuning XGB for experiment IntraRegime_R3S_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 5.2799, MAE: 4.3378, R2: 0.1417\n",
      "  Test RMSE: 4.3666, MAE: 3.7043, R2: 0.5992\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                 feature  importance\n",
      "26            temp_p50_C    0.439171\n",
      "29         temp_skewness    0.114241\n",
      "22       start_voltage_V    0.072193\n",
      "38      voltage_skewness    0.047711\n",
      "33         voltage_p10_V    0.045334\n",
      "14  discharge_duration_s    0.037147\n",
      "35         voltage_p50_V    0.036613\n",
      "15            end_temp_C    0.031589\n",
      "28            temp_p90_C    0.026292\n",
      "30            temp_std_C    0.019197\n",
      "27            temp_p75_C    0.016691\n",
      "0          avg_current_A    0.015579\n",
      "34         voltage_p25_V    0.014927\n",
      "32      voltage_kurtosis    0.013993\n",
      "39         voltage_std_V    0.013860\n",
      "\n",
      "-- Tuning LR for experiment IntraRegime_R3S_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 6.9500, MAE: 6.0164, R2: -0.4871\n",
      "  Test RMSE: 5.3042, MAE: 5.2460, R2: 0.4086\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "7          current_p75_A  134.649063\n",
      "6          current_p50_A   52.948137\n",
      "8          current_p90_A   48.754575\n",
      "19            max_temp_C   46.558200\n",
      "5          current_p25_A   33.681145\n",
      "4          current_p10_A   24.157686\n",
      "15            end_temp_C   23.520572\n",
      "12          delta_temp_C   22.717484\n",
      "0          avg_current_A   22.233910\n",
      "14  discharge_duration_s   21.588124\n",
      "40   voltage_variance_V2   16.078445\n",
      "1             avg_temp_C   14.036543\n",
      "39         voltage_std_V   13.396327\n",
      "30            temp_std_C   12.452184\n",
      "31      temp_variance_C2   11.825547\n",
      "\n",
      "Scaler and Imputer saved for IntraRegime_R3S_Tuned in regime_aware_tuned_experiments/IntraRegime_R3S_Tuned\n",
      "Tuned models saved for IntraRegime_R3S_Tuned in regime_aware_tuned_experiments/IntraRegime_R3S_Tuned\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting Experiment 5/7: CrossRegime_TrainReg_TestRecomm_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: CrossRegime_TrainReg_TestRecomm_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (3131, 41)\n",
      "Shape of y: (3131,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (1553, 41)\n",
      "Shape of y: (1553,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (3536, 41)\n",
      "Shape of y: (3536,)\n",
      "\n",
      "-- Tuning RF for experiment CrossRegime_TrainReg_TestRecomm_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 2.0901, MAE: 1.6029, R2: 0.9025\n",
      "  Test RMSE: 3.3318, MAE: 2.7085, R2: 0.7795\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.377801\n",
      "22          start_voltage_V    0.265386\n",
      "14     discharge_duration_s    0.081032\n",
      "16            end_voltage_V    0.051627\n",
      "33            voltage_p10_V    0.039402\n",
      "38         voltage_skewness    0.035392\n",
      "13          delta_voltage_V    0.024957\n",
      "29            temp_skewness    0.015135\n",
      "32         voltage_kurtosis    0.013012\n",
      "39            voltage_std_V    0.010508\n",
      "2             avg_voltage_V    0.009358\n",
      "34            voltage_p25_V    0.008279\n",
      "21             start_temp_C    0.007316\n",
      "40      voltage_variance_V2    0.007192\n",
      "35            voltage_p50_V    0.005534\n",
      "\n",
      "-- Tuning GB for experiment CrossRegime_TrainReg_TestRecomm_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0, 'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 200}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 2.1990, MAE: 1.6321, R2: 0.8921\n",
      "  Test RMSE: 2.6662, MAE: 2.1951, R2: 0.8588\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.664969\n",
      "22          start_voltage_V    0.235206\n",
      "21             start_temp_C    0.032117\n",
      "14     discharge_duration_s    0.020056\n",
      "33            voltage_p10_V    0.019430\n",
      "29            temp_skewness    0.017188\n",
      "13          delta_voltage_V    0.013667\n",
      "39            voltage_std_V    0.009510\n",
      "0             avg_current_A    0.008621\n",
      "27               temp_p75_C    0.007829\n",
      "12             delta_temp_C    0.006245\n",
      "30               temp_std_C    0.004077\n",
      "32         voltage_kurtosis    0.002123\n",
      "6             current_p50_A    0.002108\n",
      "15               end_temp_C    0.001634\n",
      "\n",
      "-- Tuning XGB for experiment CrossRegime_TrainReg_TestRecomm_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 2.4135, MAE: 1.5670, R2: 0.8700\n",
      "  Test RMSE: 2.7818, MAE: 2.1529, R2: 0.8463\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.211579\n",
      "22          start_voltage_V    0.162479\n",
      "13          delta_voltage_V    0.153877\n",
      "27               temp_p75_C    0.092682\n",
      "16            end_voltage_V    0.054985\n",
      "38         voltage_skewness    0.047958\n",
      "33            voltage_p10_V    0.041071\n",
      "39            voltage_std_V    0.027743\n",
      "14     discharge_duration_s    0.023226\n",
      "29            temp_skewness    0.022608\n",
      "2             avg_voltage_V    0.019810\n",
      "36            voltage_p75_V    0.016832\n",
      "30               temp_std_C    0.015000\n",
      "0             avg_current_A    0.013730\n",
      "8             current_p90_A    0.011650\n",
      "\n",
      "-- Tuning LR for experiment CrossRegime_TrainReg_TestRecomm_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 2.0667, MAE: 1.5425, R2: 0.9047\n",
      "  Test RMSE: 3.3063, MAE: 2.5575, R2: 0.7828\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "0          avg_current_A   31.561091\n",
      "19            max_temp_C   24.066901\n",
      "14  discharge_duration_s   21.053305\n",
      "8          current_p90_A   16.137999\n",
      "6          current_p50_A   13.071040\n",
      "1             avg_temp_C   12.532570\n",
      "18    is_reference_cycle   11.016688\n",
      "3       current_kurtosis   10.890053\n",
      "15            end_temp_C    9.778919\n",
      "12          delta_temp_C    9.713311\n",
      "9       current_skewness    9.443699\n",
      "31      temp_variance_C2    9.291667\n",
      "26            temp_p50_C    7.605104\n",
      "37         voltage_p90_V    7.148453\n",
      "30            temp_std_C    6.682791\n",
      "\n",
      "Scaler and Imputer saved for CrossRegime_TrainReg_TestRecomm_Tuned in regime_aware_tuned_experiments/CrossRegime_TrainReg_TestRecomm_Tuned\n",
      "Tuned models saved for CrossRegime_TrainReg_TestRecomm_Tuned in regime_aware_tuned_experiments/CrossRegime_TrainReg_TestRecomm_Tuned\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting Experiment 6/7: CrossRegime_TrainRecomm_TestReg_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: CrossRegime_TrainRecomm_TestReg_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (3121, 41)\n",
      "Shape of y: (3121,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (415, 41)\n",
      "Shape of y: (415,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (4684, 41)\n",
      "Shape of y: (4684,)\n",
      "\n",
      "-- Tuning RF for experiment CrossRegime_TrainRecomm_TestReg_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 3.3369, MAE: 1.5628, R2: 0.8161\n",
      "  Test RMSE: 3.3293, MAE: 2.0224, R2: 0.7764\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "22          start_voltage_V    0.605995\n",
      "17  internal_resistance_ohm    0.189889\n",
      "14     discharge_duration_s    0.034490\n",
      "33            voltage_p10_V    0.026593\n",
      "29            temp_skewness    0.025268\n",
      "36            voltage_p75_V    0.021829\n",
      "32         voltage_kurtosis    0.021308\n",
      "37            voltage_p90_V    0.013715\n",
      "13          delta_voltage_V    0.006643\n",
      "34            voltage_p25_V    0.005208\n",
      "4             current_p10_A    0.005135\n",
      "16            end_voltage_V    0.004063\n",
      "9          current_skewness    0.003792\n",
      "21             start_temp_C    0.003767\n",
      "23            temp_kurtosis    0.003583\n",
      "\n",
      "-- Tuning GB for experiment CrossRegime_TrainRecomm_TestReg_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 200}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 2.4372, MAE: 1.1196, R2: 0.9019\n",
      "  Test RMSE: 3.1807, MAE: 1.8472, R2: 0.7959\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                    feature  importance\n",
      "22          start_voltage_V    0.474867\n",
      "17  internal_resistance_ohm    0.071512\n",
      "29            temp_skewness    0.059082\n",
      "33            voltage_p10_V    0.050361\n",
      "23            temp_kurtosis    0.031005\n",
      "13          delta_voltage_V    0.030012\n",
      "39            voltage_std_V    0.027893\n",
      "16            end_voltage_V    0.021801\n",
      "36            voltage_p75_V    0.021554\n",
      "21             start_temp_C    0.015800\n",
      "14     discharge_duration_s    0.015210\n",
      "38         voltage_skewness    0.014784\n",
      "32         voltage_kurtosis    0.013604\n",
      "0             avg_current_A    0.011959\n",
      "10            current_std_A    0.006854\n",
      "\n",
      "-- Tuning XGB for experiment CrossRegime_TrainRecomm_TestReg_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 1.9480, MAE: 1.2784, R2: 0.9373\n",
      "  Test RMSE: 3.8986, MAE: 2.4670, R2: 0.6934\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                    feature  importance\n",
      "22          start_voltage_V    0.359456\n",
      "17  internal_resistance_ohm    0.082950\n",
      "26               temp_p50_C    0.075308\n",
      "36            voltage_p75_V    0.058431\n",
      "37            voltage_p90_V    0.055964\n",
      "5             current_p25_A    0.051714\n",
      "29            temp_skewness    0.047482\n",
      "32         voltage_kurtosis    0.032007\n",
      "33            voltage_p10_V    0.029816\n",
      "14     discharge_duration_s    0.023512\n",
      "3          current_kurtosis    0.019405\n",
      "28               temp_p90_C    0.018543\n",
      "0             avg_current_A    0.016919\n",
      "1                avg_temp_C    0.016229\n",
      "16            end_voltage_V    0.013581\n",
      "\n",
      "-- Tuning LR for experiment CrossRegime_TrainRecomm_TestReg_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 1.7095, MAE: 1.0255, R2: 0.9517\n",
      "  Test RMSE: 5.2108, MAE: 2.3696, R2: 0.4523\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "0          avg_current_A  187.744452\n",
      "7          current_p75_A   83.617995\n",
      "5          current_p25_A   65.161410\n",
      "2          avg_voltage_V   21.042778\n",
      "4          current_p10_A   14.969999\n",
      "40   voltage_variance_V2   14.895622\n",
      "39         voltage_std_V   12.213641\n",
      "19            max_temp_C   12.063868\n",
      "14  discharge_duration_s   10.222787\n",
      "6          current_p50_A    9.415947\n",
      "30            temp_std_C    6.853022\n",
      "36         voltage_p75_V    5.162445\n",
      "15            end_temp_C    4.885507\n",
      "12          delta_temp_C    4.858134\n",
      "31      temp_variance_C2    4.785982\n",
      "\n",
      "Scaler and Imputer saved for CrossRegime_TrainRecomm_TestReg_Tuned in regime_aware_tuned_experiments/CrossRegime_TrainRecomm_TestReg_Tuned\n",
      "Tuned models saved for CrossRegime_TrainRecomm_TestReg_Tuned in regime_aware_tuned_experiments/CrossRegime_TrainRecomm_TestReg_Tuned\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting Experiment 7/7: TrainCombined_MultiTest_Tuned\n",
      "\n",
      "--- Running Experiment with GridSearchCV: TrainCombined_MultiTest_Tuned ---\n",
      "Dropped extra feature columns: ['capacity_Ah', 'energy_Wh', 'avg_power_W']\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (6307, 41)\n",
      "Shape of y: (6307,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (534, 41)\n",
      "Shape of y: (534,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (1379, 41)\n",
      "Shape of y: (1379,)\n",
      "\n",
      "-- Tuning RF for experiment TrainCombined_MultiTest_Tuned --\n",
      "  Using IMPUTED (unscaled) data for RF\n",
      "  Starting GridSearchCV for RF...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for RF: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Evaluating RF...\n",
      "  Validation RMSE: 3.4284, MAE: 2.7055, R2: 0.6764\n",
      "  Test RMSE: 2.8447, MAE: 1.9785, R2: 0.8658\n",
      "\n",
      "  Top 15 Feature Importances for RF:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.507698\n",
      "22          start_voltage_V    0.203655\n",
      "14     discharge_duration_s    0.099260\n",
      "29            temp_skewness    0.052940\n",
      "33            voltage_p10_V    0.030925\n",
      "32         voltage_kurtosis    0.021967\n",
      "38         voltage_skewness    0.010787\n",
      "21             start_temp_C    0.006753\n",
      "20             q_initial_Ah    0.005644\n",
      "8             current_p90_A    0.004907\n",
      "16            end_voltage_V    0.004735\n",
      "34            voltage_p25_V    0.004111\n",
      "27               temp_p75_C    0.004105\n",
      "13          delta_voltage_V    0.003802\n",
      "39            voltage_std_V    0.003234\n",
      "\n",
      "-- Tuning GB for experiment TrainCombined_MultiTest_Tuned --\n",
      "  Using SCALED data for GB\n",
      "  Starting GridSearchCV for GB...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "  Best parameters for GB: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'max_depth': None, 'max_iter': 200}\n",
      "Evaluating GB...\n",
      "  Validation RMSE: 2.4046, MAE: 1.7302, R2: 0.8408\n",
      "  Test RMSE: 2.0785, MAE: 1.3759, R2: 0.9283\n",
      "  Calculating permutation importance for GB on validation data...\n",
      "\n",
      "  Top 15 Feature Importances for GB:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.780527\n",
      "22          start_voltage_V    0.257988\n",
      "14     discharge_duration_s    0.144349\n",
      "33            voltage_p10_V    0.041071\n",
      "29            temp_skewness    0.037844\n",
      "16            end_voltage_V    0.036244\n",
      "21             start_temp_C    0.024816\n",
      "38         voltage_skewness    0.016758\n",
      "39            voltage_std_V    0.007270\n",
      "35            voltage_p50_V    0.004629\n",
      "32         voltage_kurtosis    0.003760\n",
      "13          delta_voltage_V    0.003230\n",
      "34            voltage_p25_V    0.003181\n",
      "36            voltage_p75_V    0.002437\n",
      "20             q_initial_Ah    0.001905\n",
      "\n",
      "-- Tuning XGB for experiment TrainCombined_MultiTest_Tuned --\n",
      "  Using IMPUTED (unscaled) data for XGB\n",
      "  Starting GridSearchCV for XGB...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "  Best parameters for XGB: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}\n",
      "Evaluating XGB...\n",
      "  Validation RMSE: 2.3783, MAE: 1.7956, R2: 0.8443\n",
      "  Test RMSE: 2.1216, MAE: 1.5175, R2: 0.9253\n",
      "\n",
      "  Top 15 Feature Importances for XGB:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.262225\n",
      "22          start_voltage_V    0.178443\n",
      "29            temp_skewness    0.086533\n",
      "15               end_temp_C    0.074476\n",
      "16            end_voltage_V    0.049641\n",
      "32         voltage_kurtosis    0.049314\n",
      "33            voltage_p10_V    0.041784\n",
      "14     discharge_duration_s    0.039249\n",
      "1                avg_temp_C    0.037361\n",
      "13          delta_voltage_V    0.022062\n",
      "21             start_temp_C    0.015402\n",
      "12             delta_temp_C    0.014949\n",
      "10            current_std_A    0.014319\n",
      "0             avg_current_A    0.013866\n",
      "20             q_initial_Ah    0.011465\n",
      "\n",
      "-- Tuning LR for experiment TrainCombined_MultiTest_Tuned --\n",
      "  Using SCALED data for LR\n",
      "  Starting GridSearchCV for LR...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "  Best parameters for LR: {'fit_intercept': True}\n",
      "Evaluating LR...\n",
      "  Validation RMSE: 2.5884, MAE: 2.1276, R2: 0.8155\n",
      "  Test RMSE: 1.7133, MAE: 1.1163, R2: 0.9513\n",
      "\n",
      "  Top 15 Feature Importances for LR:\n",
      "                 feature  importance\n",
      "0          avg_current_A   21.879807\n",
      "19            max_temp_C   21.795945\n",
      "4          current_p10_A   18.930088\n",
      "14  discharge_duration_s   12.907159\n",
      "2          avg_voltage_V   12.211820\n",
      "8          current_p90_A   12.160348\n",
      "12          delta_temp_C    9.234550\n",
      "15            end_temp_C    9.211567\n",
      "3       current_kurtosis    7.019921\n",
      "11   current_variance_A2    6.390891\n",
      "31      temp_variance_C2    6.255346\n",
      "6          current_p50_A    5.793860\n",
      "10         current_std_A    5.434882\n",
      "26            temp_p50_C    5.014600\n",
      "1             avg_temp_C    4.913291\n",
      "\n",
      "Scaler and Imputer saved for TrainCombined_MultiTest_Tuned in regime_aware_tuned_experiments/TrainCombined_MultiTest_Tuned\n",
      "Tuned models saved for TrainCombined_MultiTest_Tuned in regime_aware_tuned_experiments/TrainCombined_MultiTest_Tuned\n",
      "\n",
      "-- Additional Evaluation for MultiTest Experiment: TrainCombined_MultiTest_Tuned --\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (355, 41)\n",
      "Shape of y: (355,)\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (339, 41)\n",
      "Shape of y: (339,)\n",
      "  Model: RF (Evaluating on specific REGULAR Test Set)\n",
      "Evaluating RF_REG_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 4.1802, MAE: 3.1692, R2: 0.6172\n",
      "\n",
      "  Top 15 Feature Importances for RF_REG_TEST:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.507698\n",
      "22          start_voltage_V    0.203655\n",
      "14     discharge_duration_s    0.099260\n",
      "29            temp_skewness    0.052940\n",
      "33            voltage_p10_V    0.030925\n",
      "32         voltage_kurtosis    0.021967\n",
      "38         voltage_skewness    0.010787\n",
      "21             start_temp_C    0.006753\n",
      "20             q_initial_Ah    0.005644\n",
      "8             current_p90_A    0.004907\n",
      "16            end_voltage_V    0.004735\n",
      "34            voltage_p25_V    0.004111\n",
      "27               temp_p75_C    0.004105\n",
      "13          delta_voltage_V    0.003802\n",
      "39            voltage_std_V    0.003234\n",
      "  Model: GB (Evaluating on specific REGULAR Test Set)\n",
      "Evaluating GB_REG_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 2.9160, MAE: 1.9649, R2: 0.8137\n",
      "Warning: Model gb_reg_test has no 'feature_importances_' or 'coef_'.\n",
      "  Model: XGB (Evaluating on specific REGULAR Test Set)\n",
      "Evaluating XGB_REG_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 3.3375, MAE: 2.6205, R2: 0.7560\n",
      "\n",
      "  Top 15 Feature Importances for XGB_REG_TEST:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.262225\n",
      "22          start_voltage_V    0.178443\n",
      "29            temp_skewness    0.086533\n",
      "15               end_temp_C    0.074476\n",
      "16            end_voltage_V    0.049641\n",
      "32         voltage_kurtosis    0.049314\n",
      "33            voltage_p10_V    0.041784\n",
      "14     discharge_duration_s    0.039249\n",
      "1                avg_temp_C    0.037361\n",
      "13          delta_voltage_V    0.022062\n",
      "21             start_temp_C    0.015402\n",
      "12             delta_temp_C    0.014949\n",
      "10            current_std_A    0.014319\n",
      "0             avg_current_A    0.013866\n",
      "20             q_initial_Ah    0.011465\n",
      "  Model: LR (Evaluating on specific REGULAR Test Set)\n",
      "Evaluating LR_REG_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 2.4797, MAE: 1.7422, R2: 0.8653\n",
      "Warning: Model lr_reg_test has no 'feature_importances_' or 'coef_'.\n",
      "Target variable: SOH_cycle_capacity_%\n",
      "Number of features selected: 41\n",
      "Shape of X (features for model): (1040, 41)\n",
      "Shape of y: (1040,)\n",
      "  Model: RF (Evaluating on specific RECOMMISSIONED Test Set)\n",
      "Evaluating RF_RECOMM_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 2.2438, MAE: 1.5904, R2: 0.8953\n",
      "\n",
      "  Top 15 Feature Importances for RF_RECOMM_TEST:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.507698\n",
      "22          start_voltage_V    0.203655\n",
      "14     discharge_duration_s    0.099260\n",
      "29            temp_skewness    0.052940\n",
      "33            voltage_p10_V    0.030925\n",
      "32         voltage_kurtosis    0.021967\n",
      "38         voltage_skewness    0.010787\n",
      "21             start_temp_C    0.006753\n",
      "20             q_initial_Ah    0.005644\n",
      "8             current_p90_A    0.004907\n",
      "16            end_voltage_V    0.004735\n",
      "34            voltage_p25_V    0.004111\n",
      "27               temp_p75_C    0.004105\n",
      "13          delta_voltage_V    0.003802\n",
      "39            voltage_std_V    0.003234\n",
      "  Model: GB (Evaluating on specific RECOMMISSIONED Test Set)\n",
      "Evaluating GB_RECOMM_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 1.7196, MAE: 1.1839, R2: 0.9385\n",
      "Warning: Model gb_recomm_test has no 'feature_importances_' or 'coef_'.\n",
      "  Model: XGB (Evaluating on specific RECOMMISSIONED Test Set)\n",
      "Evaluating XGB_RECOMM_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 1.5290, MAE: 1.1579, R2: 0.9514\n",
      "\n",
      "  Top 15 Feature Importances for XGB_RECOMM_TEST:\n",
      "                    feature  importance\n",
      "17  internal_resistance_ohm    0.262225\n",
      "22          start_voltage_V    0.178443\n",
      "29            temp_skewness    0.086533\n",
      "15               end_temp_C    0.074476\n",
      "16            end_voltage_V    0.049641\n",
      "32         voltage_kurtosis    0.049314\n",
      "33            voltage_p10_V    0.041784\n",
      "14     discharge_duration_s    0.039249\n",
      "1                avg_temp_C    0.037361\n",
      "13          delta_voltage_V    0.022062\n",
      "21             start_temp_C    0.015402\n",
      "12             delta_temp_C    0.014949\n",
      "10            current_std_A    0.014319\n",
      "0             avg_current_A    0.013866\n",
      "20             q_initial_Ah    0.011465\n",
      "  Model: LR (Evaluating on specific RECOMMISSIONED Test Set)\n",
      "Evaluating LR_RECOMM_TEST...\n",
      "  Validation set empty or y_val empty. Skipping validation metrics.\n",
      "  Test RMSE: 1.3740, MAE: 0.9123, R2: 0.9607\n",
      "Warning: Model lr_recomm_test has no 'feature_importances_' or 'coef_'.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--- All Tuned Experiment Runs Completed (including MultiTest if any) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run Experiments and Collect Results\n",
    "all_tuned_experiment_results = {}\n",
    "\n",
    "for config_idx, config in enumerate(experiment_configs):\n",
    "    print(f\"\\nStarting Experiment {config_idx + 1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    train_ids = list(config['train_battery_ids'])\n",
    "    val_ids = list(config['val_battery_ids'])\n",
    "    # Use the general 'test_battery_ids' for the main run (could be combined for multitest)\n",
    "    test_ids_main_run = list(config.get('test_battery_ids', []))\n",
    "\n",
    "    if not train_ids:\n",
    "        print(f\"Skipping experiment {config['name']} due to empty training battery ID list.\")\n",
    "        all_tuned_experiment_results[config['name']] = None # Mark as skipped\n",
    "        continue\n",
    "    if not val_ids: # Ensure val_ids has something, even if it's same as train for tiny sets\n",
    "        print(f\"Warning: Val IDs are empty for {config['name']}, using train IDs for validation.\")\n",
    "        val_ids = train_ids\n",
    "\n",
    "\n",
    "    # Call the experiment runner\n",
    "    # It returns: results (dict of metrics for models), trained_models_dict (dict of model_name: best_estimator)\n",
    "    results_main_run, trained_best_models_dict = run_experiment_with_gridsearch(\n",
    "        exp_name=config['name'],\n",
    "        df_full_data=df_master_full,\n",
    "        train_battery_ids=train_ids,\n",
    "        val_battery_ids=val_ids,\n",
    "        test_battery_ids=test_ids_main_run,\n",
    "        target_col_name=TARGET_COL,\n",
    "        extra_exclude_cols=EXTRA_EXCLUDE_COLS_FROM_FEATURES,\n",
    "        models_to_tune=MODELS_TO_TUNE_AND_TRAIN,\n",
    "        param_grids_dict=PARAM_GRIDS,\n",
    "        cv_folds_gs=CV_FOLDS,\n",
    "        scoring_gs=GRIDSEARCH_SCORING,\n",
    "        save_artifacts=SAVE_MODELS_FLAG\n",
    "    )\n",
    "\n",
    "    if results_main_run:\n",
    "        all_tuned_experiment_results[config['name']] = results_main_run\n",
    "\n",
    "        # --- Special handling for MultiTest scenario ---\n",
    "        if config.get('is_multitest'):\n",
    "            print(f\"\\n-- Additional Evaluation for MultiTest Experiment: {config['name']} --\")\n",
    "            test_ids_regular_s7 = list(config.get('test_battery_ids_regular', []))\n",
    "            test_ids_recomm_s7 = list(config.get('test_battery_ids_recomm', []))\n",
    "            \n",
    "            exp_output_dir_multi = os.path.join(BASE_OUTPUT_DIR, config['name'])\n",
    "            imputer_multi, scaler_multi = None, None\n",
    "            try:\n",
    "                imputer_multi = joblib.load(os.path.join(exp_output_dir_multi, f\"imputer_{config['name']}.joblib\"))\n",
    "                scaler_multi = joblib.load(os.path.join(exp_output_dir_multi, f\"scaler_{config['name']}.joblib\"))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"ERROR: Could not load imputer/scaler for {config['name']} for multi-test. Skipping additional evals.\")\n",
    "                continue # Skip additional evals for this config\n",
    "\n",
    "            # Get original feature names from one of the trained models (assuming they are consistent)\n",
    "            # This assumes run_experiment_with_gridsearch used train_models.preprocess_data which returned actual_feature_names\n",
    "            # And that `evaluate_fitted_model` was called with it.\n",
    "            # For simplicity, we re-derive feature names if needed, or ensure run_experiment_with_gridsearch returns them.\n",
    "            # Let's re-run preprocess_data on a dummy df from train_ids to get feature names if not easily available\n",
    "            # This is a bit inefficient but robust for getting names.\n",
    "            # A better way: ensure run_experiment_with_gridsearch returns the 'actual_feature_names'\n",
    "            # For now:\n",
    "            temp_df_for_names = df_master_full[df_master_full['battery_id'].isin(train_ids[:1])].copy() # Use one train battery\n",
    "            if 'regime' in temp_df_for_names.columns: temp_df_for_names.drop(columns=['regime'], inplace=True)\n",
    "            cols_to_drop_fn = [col for col in EXTRA_EXCLUDE_COLS_FROM_FEATURES if col in temp_df_for_names.columns]\n",
    "            if cols_to_drop_fn: temp_df_for_names.drop(columns=cols_to_drop_fn, inplace=True, errors='ignore')\n",
    "            _, _, _, common_feature_names = train_models.preprocess_data(temp_df_for_names, TARGET_COL)\n",
    "            if not common_feature_names :\n",
    "                print(\"ERROR: Could not derive common feature names for multi-test evaluation. Skipping additional evals.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Evaluate on REGULAR Test Set for MultiTest\n",
    "            if test_ids_regular_s7:\n",
    "                df_test_reg_s7 = df_master_full[df_master_full['battery_id'].isin(test_ids_regular_s7)].copy()\n",
    "                if 'regime' in df_test_reg_s7.columns: df_test_reg_s7.drop(columns=['regime'], inplace=True)\n",
    "                if cols_to_drop_fn: df_test_reg_s7.drop(columns=cols_to_drop_fn, inplace=True, errors='ignore')\n",
    "                \n",
    "                X_test_reg_s7, y_test_reg_s7, _, _ = train_models.preprocess_data(df_test_reg_s7, TARGET_COL)\n",
    "                \n",
    "                if X_test_reg_s7 is not None and not X_test_reg_s7.empty:\n",
    "                    X_test_reg_imputed_s7 = pd.DataFrame(imputer_multi.transform(X_test_reg_s7), columns=X_test_reg_s7.columns, index=X_test_reg_s7.index)\n",
    "                    X_test_reg_scaled_s7 = pd.DataFrame(scaler_multi.transform(X_test_reg_imputed_s7), columns=X_test_reg_imputed_s7.columns, index=X_test_reg_imputed_s7.index)\n",
    "\n",
    "                    for model_name_key, best_model_instance in trained_best_models_dict.items():\n",
    "                        current_X_test_data = X_test_reg_imputed_s7 if model_name_key not in ['lr', 'gb'] else X_test_reg_scaled_s7\n",
    "                        print(f\"  Model: {model_name_key.upper()} (Evaluating on specific REGULAR Test Set)\")\n",
    "                        metrics_specific_reg = evaluate_fitted_model(best_model_instance, f\"{model_name_key}_reg_test\", None, None, current_X_test_data, y_test_reg_s7, common_feature_names)\n",
    "                        if model_name_key not in all_tuned_experiment_results[config['name']]: all_tuned_experiment_results[config['name']][model_name_key] = {}\n",
    "                        all_tuned_experiment_results[config['name']][model_name_key]['test_rmse_regular'] = metrics_specific_reg.get('test_rmse')\n",
    "                        all_tuned_experiment_results[config['name']][model_name_key]['test_mae_regular'] = metrics_specific_reg.get('test_mae')\n",
    "                        all_tuned_experiment_results[config['name']][model_name_key]['test_r2_regular'] = metrics_specific_reg.get('test_r2')\n",
    "                else:\n",
    "                    print(\"  Skipping REGULAR specific test set for MultiTest: No data after preprocessing.\")\n",
    "            \n",
    "            # Evaluate on RECOMMISSIONED Test Set for MultiTest\n",
    "            if test_ids_recomm_s7:\n",
    "                df_test_recomm_s7 = df_master_full[df_master_full['battery_id'].isin(test_ids_recomm_s7)].copy()\n",
    "                if 'regime' in df_test_recomm_s7.columns: df_test_recomm_s7.drop(columns=['regime'], inplace=True)\n",
    "                if cols_to_drop_fn: df_test_recomm_s7.drop(columns=cols_to_drop_fn, inplace=True, errors='ignore')\n",
    "\n",
    "                X_test_recomm_s7, y_test_recomm_s7, _, _ = train_models.preprocess_data(df_test_recomm_s7, TARGET_COL)\n",
    "\n",
    "                if X_test_recomm_s7 is not None and not X_test_recomm_s7.empty:\n",
    "                    X_test_recomm_imputed_s7 = pd.DataFrame(imputer_multi.transform(X_test_recomm_s7), columns=X_test_recomm_s7.columns, index=X_test_recomm_s7.index)\n",
    "                    X_test_recomm_scaled_s7 = pd.DataFrame(scaler_multi.transform(X_test_recomm_imputed_s7), columns=X_test_recomm_imputed_s7.columns, index=X_test_recomm_imputed_s7.index)\n",
    "\n",
    "                    for model_name_key, best_model_instance in trained_best_models_dict.items():\n",
    "                        current_X_test_data = X_test_recomm_imputed_s7 if model_name_key not in ['lr', 'gb'] else X_test_recomm_scaled_s7\n",
    "                        print(f\"  Model: {model_name_key.upper()} (Evaluating on specific RECOMMISSIONED Test Set)\")\n",
    "                        metrics_specific_recomm = evaluate_fitted_model(best_model_instance, f\"{model_name_key}_recomm_test\", None, None, current_X_test_data, y_test_recomm_s7, common_feature_names)\n",
    "                        if model_name_key not in all_tuned_experiment_results[config['name']]: all_tuned_experiment_results[config['name']][model_name_key] = {}\n",
    "                        all_tuned_experiment_results[config['name']][model_name_key]['test_rmse_recomm'] = metrics_specific_recomm.get('test_rmse')\n",
    "                        all_tuned_experiment_results[config['name']][model_name_key]['test_mae_recomm'] = metrics_specific_recomm.get('test_mae')\n",
    "                        all_tuned_experiment_results[config['name']][model_name_key]['test_r2_recomm'] = metrics_specific_recomm.get('test_r2')\n",
    "                else:\n",
    "                    print(\"  Skipping RECOMMISSIONED specific test set for MultiTest: No data after preprocessing.\")\n",
    "    else:\n",
    "         all_tuned_experiment_results[config['name']] = None # Mark as having no results if main run failed\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\\n--- All Tuned Experiment Runs Completed (including MultiTest if any) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750e5239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Consolidated Tuned Results Summary ---\n",
      "\n",
      "Experiment: IntraRegime_RC_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=1.9741, MAE=0.9968, R2=0.9195\n",
      "    Test (Overall): RMSE=5.6243, MAE=1.8263, R2=0.4183\n",
      "  Model: GB\n",
      "    Validation: RMSE=2.2845, MAE=1.2195, R2=0.8922\n",
      "    Test (Overall): RMSE=4.0013, MAE=1.5292, R2=0.7056\n",
      "  Model: XGB\n",
      "    Validation: RMSE=2.2409, MAE=1.4775, R2=0.8963\n",
      "    Test (Overall): RMSE=4.4920, MAE=1.0706, R2=0.6290\n",
      "  Model: LR\n",
      "    Validation: RMSE=1.6283, MAE=0.8780, R2=0.9452\n",
      "    Test (Overall): RMSE=2.6848, MAE=2.0939, R2=0.8675\n",
      "\n",
      "Experiment: IntraRegime_RV_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=3.9115, MAE=3.1552, R2=0.6876\n",
      "    Test (Overall): RMSE=2.3880, MAE=1.5275, R2=0.8713\n",
      "  Model: GB\n",
      "    Validation: RMSE=4.7544, MAE=2.9563, R2=0.5384\n",
      "    Test (Overall): RMSE=3.3162, MAE=2.1012, R2=0.7519\n",
      "  Model: XGB\n",
      "    Validation: RMSE=2.3535, MAE=2.0228, R2=0.8869\n",
      "    Test (Overall): RMSE=3.0362, MAE=1.9476, R2=0.7920\n",
      "  Model: LR\n",
      "    Validation: RMSE=4.6750, MAE=4.4871, R2=0.5537\n",
      "    Test (Overall): RMSE=7.5999, MAE=5.0793, R2=-0.3031\n",
      "\n",
      "Experiment: IntraRegime_R2S_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=4.7795, MAE=3.8678, R2=0.5735\n",
      "    Test (Overall): RMSE=5.6327, MAE=4.6905, R2=-0.2834\n",
      "  Model: GB\n",
      "    Validation: RMSE=4.4148, MAE=3.8513, R2=0.6361\n",
      "    Test (Overall): RMSE=6.9623, MAE=5.7863, R2=-0.9608\n",
      "  Model: XGB\n",
      "    Validation: RMSE=2.6351, MAE=1.7277, R2=0.8704\n",
      "    Test (Overall): RMSE=2.4440, MAE=1.8911, R2=0.7584\n",
      "  Model: LR\n",
      "    Validation: RMSE=5.6585, MAE=3.9997, R2=0.4022\n",
      "    Test (Overall): RMSE=7.0786, MAE=6.2172, R2=-1.0268\n",
      "\n",
      "Experiment: IntraRegime_R3S_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=5.6205, MAE=4.5035, R2=0.0274\n",
      "    Test (Overall): RMSE=5.7643, MAE=4.9942, R2=0.3015\n",
      "  Model: GB\n",
      "    Validation: RMSE=6.1832, MAE=5.2622, R2=-0.1771\n",
      "    Test (Overall): RMSE=6.5427, MAE=5.6715, R2=0.1001\n",
      "  Model: XGB\n",
      "    Validation: RMSE=5.2799, MAE=4.3378, R2=0.1417\n",
      "    Test (Overall): RMSE=4.3666, MAE=3.7043, R2=0.5992\n",
      "  Model: LR\n",
      "    Validation: RMSE=6.9500, MAE=6.0164, R2=-0.4871\n",
      "    Test (Overall): RMSE=5.3042, MAE=5.2460, R2=0.4086\n",
      "\n",
      "Experiment: CrossRegime_TrainReg_TestRecomm_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=2.0901, MAE=1.6029, R2=0.9025\n",
      "    Test (Overall): RMSE=3.3318, MAE=2.7085, R2=0.7795\n",
      "  Model: GB\n",
      "    Validation: RMSE=2.1990, MAE=1.6321, R2=0.8921\n",
      "    Test (Overall): RMSE=2.6662, MAE=2.1951, R2=0.8588\n",
      "  Model: XGB\n",
      "    Validation: RMSE=2.4135, MAE=1.5670, R2=0.8700\n",
      "    Test (Overall): RMSE=2.7818, MAE=2.1529, R2=0.8463\n",
      "  Model: LR\n",
      "    Validation: RMSE=2.0667, MAE=1.5425, R2=0.9047\n",
      "    Test (Overall): RMSE=3.3063, MAE=2.5575, R2=0.7828\n",
      "\n",
      "Experiment: CrossRegime_TrainRecomm_TestReg_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=3.3369, MAE=1.5628, R2=0.8161\n",
      "    Test (Overall): RMSE=3.3293, MAE=2.0224, R2=0.7764\n",
      "  Model: GB\n",
      "    Validation: RMSE=2.4372, MAE=1.1196, R2=0.9019\n",
      "    Test (Overall): RMSE=3.1807, MAE=1.8472, R2=0.7959\n",
      "  Model: XGB\n",
      "    Validation: RMSE=1.9480, MAE=1.2784, R2=0.9373\n",
      "    Test (Overall): RMSE=3.8986, MAE=2.4670, R2=0.6934\n",
      "  Model: LR\n",
      "    Validation: RMSE=1.7095, MAE=1.0255, R2=0.9517\n",
      "    Test (Overall): RMSE=5.2108, MAE=2.3696, R2=0.4523\n",
      "\n",
      "Experiment: TrainCombined_MultiTest_Tuned\n",
      "  Model: RF\n",
      "    Validation: RMSE=3.4284, MAE=2.7055, R2=0.6764\n",
      "    Test (Overall): RMSE=2.8447, MAE=1.9785, R2=0.8658\n",
      "    Test (REGULAR only): RMSE=4.1802, MAE=3.1692, R2=0.6172\n",
      "    Test (RECOMM only): RMSE=2.2438, MAE=1.5904, R2=0.8953\n",
      "  Model: GB\n",
      "    Validation: RMSE=2.4046, MAE=1.7302, R2=0.8408\n",
      "    Test (Overall): RMSE=2.0785, MAE=1.3759, R2=0.9283\n",
      "    Test (REGULAR only): RMSE=2.9160, MAE=1.9649, R2=0.8137\n",
      "    Test (RECOMM only): RMSE=1.7196, MAE=1.1839, R2=0.9385\n",
      "  Model: XGB\n",
      "    Validation: RMSE=2.3783, MAE=1.7956, R2=0.8443\n",
      "    Test (Overall): RMSE=2.1216, MAE=1.5175, R2=0.9253\n",
      "    Test (REGULAR only): RMSE=3.3375, MAE=2.6205, R2=0.7560\n",
      "    Test (RECOMM only): RMSE=1.5290, MAE=1.1579, R2=0.9514\n",
      "  Model: LR\n",
      "    Validation: RMSE=2.5884, MAE=2.1276, R2=0.8155\n",
      "    Test (Overall): RMSE=1.7133, MAE=1.1163, R2=0.9513\n",
      "    Test (REGULAR only): RMSE=2.4797, MAE=1.7422, R2=0.8653\n",
      "    Test (RECOMM only): RMSE=1.3740, MAE=0.9123, R2=0.9607\n",
      "\n",
      "\n",
      "--- Tuned Results Summary DataFrame ---\n",
      "                               Experiment Model  Val RMSE   Val MAE    Val R2  \\\n",
      "0                    IntraRegime_RC_Tuned    RF  1.974071  0.996751  0.919504   \n",
      "1                    IntraRegime_RC_Tuned    GB  2.284463  1.219535  0.892200   \n",
      "2                    IntraRegime_RC_Tuned   XGB  2.240941  1.477492  0.896269   \n",
      "3                    IntraRegime_RC_Tuned    LR  1.628274  0.877975  0.945235   \n",
      "4                    IntraRegime_RV_Tuned    RF  3.911498  3.155233  0.687580   \n",
      "5                    IntraRegime_RV_Tuned    GB  4.754446  2.956252  0.538415   \n",
      "6                    IntraRegime_RV_Tuned   XGB  2.353546  2.022754  0.886891   \n",
      "7                    IntraRegime_RV_Tuned    LR  4.674993  4.487134  0.553713   \n",
      "8                   IntraRegime_R2S_Tuned    RF  4.779460  3.867830  0.573499   \n",
      "9                   IntraRegime_R2S_Tuned    GB  4.414802  3.851268  0.636098   \n",
      "10                  IntraRegime_R2S_Tuned   XGB  2.635081  1.727688  0.870357   \n",
      "11                  IntraRegime_R2S_Tuned    LR  5.658505  3.999653  0.402187   \n",
      "12                  IntraRegime_R3S_Tuned    RF  5.620548  4.503495  0.027406   \n",
      "13                  IntraRegime_R3S_Tuned    GB  6.183219  5.262171 -0.177074   \n",
      "14                  IntraRegime_R3S_Tuned   XGB  5.279870  4.337780  0.141736   \n",
      "15                  IntraRegime_R3S_Tuned    LR  6.949965  6.016385 -0.487098   \n",
      "16  CrossRegime_TrainReg_TestRecomm_Tuned    RF  2.090110  1.602916  0.902530   \n",
      "17  CrossRegime_TrainReg_TestRecomm_Tuned    GB  2.198966  1.632103  0.892113   \n",
      "18  CrossRegime_TrainReg_TestRecomm_Tuned   XGB  2.413499  1.566969  0.870035   \n",
      "19  CrossRegime_TrainReg_TestRecomm_Tuned    LR  2.066736  1.542535  0.904698   \n",
      "20  CrossRegime_TrainRecomm_TestReg_Tuned    RF  3.336936  1.562803  0.816087   \n",
      "21  CrossRegime_TrainRecomm_TestReg_Tuned    GB  2.437160  1.119623  0.901896   \n",
      "22  CrossRegime_TrainRecomm_TestReg_Tuned   XGB  1.947981  1.278443  0.937326   \n",
      "23  CrossRegime_TrainRecomm_TestReg_Tuned    LR  1.709513  1.025496  0.951732   \n",
      "24          TrainCombined_MultiTest_Tuned    RF  3.428357  2.705465  0.676366   \n",
      "25          TrainCombined_MultiTest_Tuned    GB  2.404581  1.730248  0.840793   \n",
      "26          TrainCombined_MultiTest_Tuned   XGB  2.378269  1.795644  0.844258   \n",
      "27          TrainCombined_MultiTest_Tuned    LR  2.588429  2.127561  0.815518   \n",
      "\n",
      "    Test RMSE (Overall)  Test MAE (Overall)  Test R2 (Overall)  \\\n",
      "0              5.624338            1.826293           0.418329   \n",
      "1              4.001252            1.529210           0.705608   \n",
      "2              4.492036            1.070598           0.628960   \n",
      "3              2.684816            2.093862           0.867455   \n",
      "4              2.387979            1.527487           0.871347   \n",
      "5              3.316169            2.101210           0.751898   \n",
      "6              3.036172            1.947619           0.792025   \n",
      "7              7.599904            5.079266          -0.303088   \n",
      "8              5.632655            4.690463          -0.283369   \n",
      "9              6.962259            5.786297          -0.960764   \n",
      "10             2.444001            1.891116           0.758383   \n",
      "11             7.078579            6.217218          -1.026830   \n",
      "12             5.764320            4.994194           0.301489   \n",
      "13             6.542674            5.671503           0.100114   \n",
      "14             4.366643            3.704303           0.599159   \n",
      "15             5.304178            5.246034           0.408556   \n",
      "16             3.331834            2.708508           0.779480   \n",
      "17             2.666214            2.195095           0.858788   \n",
      "18             2.781826            2.152923           0.846276   \n",
      "19             3.306305            2.557497           0.782846   \n",
      "20             3.329342            2.022395           0.776415   \n",
      "21             3.180711            1.847169           0.795933   \n",
      "22             3.898628            2.466996           0.693416   \n",
      "23             5.210807            2.369595           0.452309   \n",
      "24             2.844748            1.978520           0.865778   \n",
      "25             2.078542            1.375905           0.928344   \n",
      "26             2.121642            1.517460           0.925341   \n",
      "27             1.713282            1.116329           0.951315   \n",
      "\n",
      "    Test RMSE (Regular)  Test MAE (Regular)  Test R2 (Regular)  \\\n",
      "0                   NaN                 NaN                NaN   \n",
      "1                   NaN                 NaN                NaN   \n",
      "2                   NaN                 NaN                NaN   \n",
      "3                   NaN                 NaN                NaN   \n",
      "4                   NaN                 NaN                NaN   \n",
      "5                   NaN                 NaN                NaN   \n",
      "6                   NaN                 NaN                NaN   \n",
      "7                   NaN                 NaN                NaN   \n",
      "8                   NaN                 NaN                NaN   \n",
      "9                   NaN                 NaN                NaN   \n",
      "10                  NaN                 NaN                NaN   \n",
      "11                  NaN                 NaN                NaN   \n",
      "12                  NaN                 NaN                NaN   \n",
      "13                  NaN                 NaN                NaN   \n",
      "14                  NaN                 NaN                NaN   \n",
      "15                  NaN                 NaN                NaN   \n",
      "16                  NaN                 NaN                NaN   \n",
      "17                  NaN                 NaN                NaN   \n",
      "18                  NaN                 NaN                NaN   \n",
      "19                  NaN                 NaN                NaN   \n",
      "20                  NaN                 NaN                NaN   \n",
      "21                  NaN                 NaN                NaN   \n",
      "22                  NaN                 NaN                NaN   \n",
      "23                  NaN                 NaN                NaN   \n",
      "24             4.180191            3.169162           0.617154   \n",
      "25             2.915964            1.964892           0.813707   \n",
      "26             3.337506            2.620519           0.755951   \n",
      "27             2.479654            1.742192           0.865285   \n",
      "\n",
      "    Test RMSE (Recomm)  Test MAE (Recomm)  Test R2 (Recomm)  \n",
      "0                  NaN                NaN               NaN  \n",
      "1                  NaN                NaN               NaN  \n",
      "2                  NaN                NaN               NaN  \n",
      "3                  NaN                NaN               NaN  \n",
      "4                  NaN                NaN               NaN  \n",
      "5                  NaN                NaN               NaN  \n",
      "6                  NaN                NaN               NaN  \n",
      "7                  NaN                NaN               NaN  \n",
      "8                  NaN                NaN               NaN  \n",
      "9                  NaN                NaN               NaN  \n",
      "10                 NaN                NaN               NaN  \n",
      "11                 NaN                NaN               NaN  \n",
      "12                 NaN                NaN               NaN  \n",
      "13                 NaN                NaN               NaN  \n",
      "14                 NaN                NaN               NaN  \n",
      "15                 NaN                NaN               NaN  \n",
      "16                 NaN                NaN               NaN  \n",
      "17                 NaN                NaN               NaN  \n",
      "18                 NaN                NaN               NaN  \n",
      "19                 NaN                NaN               NaN  \n",
      "20                 NaN                NaN               NaN  \n",
      "21                 NaN                NaN               NaN  \n",
      "22                 NaN                NaN               NaN  \n",
      "23                 NaN                NaN               NaN  \n",
      "24            2.243794           1.590417          0.895297  \n",
      "25            1.719593           1.183918          0.938504  \n",
      "26            1.528976           1.157905          0.951382  \n",
      "27            1.374011           0.912322          0.960738  \n",
      "\n",
      "Tuned results summary saved to regime_aware_tuned_experiments/all_tuned_experiments_summary_incl_multitest.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Display Consolidated Results\n",
    "print(\"\\n--- Consolidated Tuned Results Summary ---\")\n",
    "# First, a text summary\n",
    "for exp_name_outer, exp_data_outer in all_tuned_experiment_results.items():\n",
    "    print(f\"\\nExperiment: {exp_name_outer}\")\n",
    "    if not exp_data_outer:\n",
    "        print(\"  No results for this experiment (possibly skipped or failed).\")\n",
    "        continue\n",
    "    for model_name_outer, metrics_outer in exp_data_outer.items():\n",
    "        print(f\"  Model: {model_name_outer.upper()}\")\n",
    "        val_rmse = metrics_outer.get('val_rmse', float('nan'))\n",
    "        val_mae = metrics_outer.get('val_mae', float('nan'))\n",
    "        val_r2 = metrics_outer.get('val_r2', float('nan'))\n",
    "        test_rmse = metrics_outer.get('test_rmse', float('nan')) # This is on the 'combined' test set for multitest\n",
    "        test_mae = metrics_outer.get('test_mae', float('nan'))\n",
    "        test_r2 = metrics_outer.get('test_r2', float('nan'))\n",
    "        \n",
    "        print(f\"    Validation: RMSE={val_rmse:.4f}, MAE={val_mae:.4f}, R2={val_r2:.4f}\")\n",
    "        print(f\"    Test (Overall): RMSE={test_rmse:.4f}, MAE={test_mae:.4f}, R2={test_r2:.4f}\")\n",
    "\n",
    "        # Display specific multi-test results if present\n",
    "        if metrics_outer.get('test_r2_regular') is not None:\n",
    "            test_rmse_reg = metrics_outer.get('test_rmse_regular', float('nan'))\n",
    "            test_mae_reg = metrics_outer.get('test_mae_regular', float('nan'))\n",
    "            test_r2_reg = metrics_outer.get('test_r2_regular', float('nan'))\n",
    "            print(f\"    Test (REGULAR only): RMSE={test_rmse_reg:.4f}, MAE={test_mae_reg:.4f}, R2={test_r2_reg:.4f}\")\n",
    "        \n",
    "        if metrics_outer.get('test_r2_recomm') is not None:\n",
    "            test_rmse_recomm = metrics_outer.get('test_rmse_recomm', float('nan'))\n",
    "            test_mae_recomm = metrics_outer.get('test_mae_recomm', float('nan'))\n",
    "            test_r2_recomm = metrics_outer.get('test_r2_recomm', float('nan'))\n",
    "            print(f\"    Test (RECOMM only): RMSE={test_rmse_recomm:.4f}, MAE={test_mae_recomm:.4f}, R2={test_r2_recomm:.4f}\")\n",
    "\n",
    "\n",
    "# DataFrame for easier comparison\n",
    "results_list_tuned_df = []\n",
    "for exp_name, exp_data in all_tuned_experiment_results.items():\n",
    "    if not exp_data: continue # Skip if experiment had no results\n",
    "    for model_name, metrics in exp_data.items():\n",
    "        entry = {\n",
    "            'Experiment': exp_name,\n",
    "            'Model': model_name.upper(),\n",
    "            'Val RMSE': metrics.get('val_rmse'), \n",
    "            'Val MAE': metrics.get('val_mae'), \n",
    "            'Val R2': metrics.get('val_r2'),\n",
    "            'Test RMSE (Overall)': metrics.get('test_rmse'), \n",
    "            'Test MAE (Overall)': metrics.get('test_mae'), \n",
    "            'Test R2 (Overall)': metrics.get('test_r2'),\n",
    "            # Add new multi-test metrics if they exist\n",
    "            'Test RMSE (Regular)': metrics.get('test_rmse_regular'),\n",
    "            'Test MAE (Regular)': metrics.get('test_mae_regular'),\n",
    "            'Test R2 (Regular)': metrics.get('test_r2_regular'),\n",
    "            'Test RMSE (Recomm)': metrics.get('test_rmse_recomm'),\n",
    "            'Test MAE (Recomm)': metrics.get('test_mae_recomm'),\n",
    "            'Test R2 (Recomm)': metrics.get('test_r2_recomm'),\n",
    "        }\n",
    "        results_list_tuned_df.append(entry)\n",
    "\n",
    "if results_list_tuned_df: # Check if list is not empty\n",
    "    df_results_summary_tuned_final = pd.DataFrame(results_list_tuned_df)\n",
    "    # Optionally drop columns that are all NaN if you prefer a cleaner table for general viewing\n",
    "    df_results_summary_tuned_final.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    print(\"\\n\\n--- Tuned Results Summary DataFrame ---\")\n",
    "    print(df_results_summary_tuned_final)\n",
    "\n",
    "    # Save summary to CSV\n",
    "    csv_path = os.path.join(BASE_OUTPUT_DIR, \"all_tuned_experiments_summary_incl_multitest.csv\")\n",
    "    df_results_summary_tuned_final.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nTuned results summary saved to {csv_path}\")\n",
    "else:\n",
    "    print(\"\\nNo results to create a summary DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
