{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24209ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust evaluation base output directory: soh_robust_evaluation_outputs\n",
      "Cross-validation outputs will be saved to: soh_robust_evaluation_outputs/SOH_GroupKFold_CV\n",
      "Final champion model artifacts will be saved to: soh_robust_evaluation_outputs/Final_Champion_Model\n"
     ]
    }
   ],
   "source": [
    "# --- Cell A: Global Configurations for Robust Evaluation ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Assuming these are defined from your original Cell 1 or similar setup ---\n",
    "INPUT_FILE = 'New_Features_Added_ALL.csv'\n",
    "TARGET_COL = 'SOH_cycle_capacity_%'\n",
    "EXTRA_EXCLUDE_COLS_FROM_FEATURES = [\n",
    "    'capacity_Ah', 'energy_Wh',\n",
    "    'capacity_Ah_roll_mean_3', 'capacity_Ah_roll_std_3', 'capacity_Ah_diff_3',\n",
    "    'capacity_Ah_roll_mean_5', 'capacity_Ah_roll_std_5', 'capacity_Ah_diff_5',\n",
    "    'capacity_Ah_roll_mean_10', 'capacity_Ah_roll_std_10', 'capacity_Ah_diff_10',\n",
    "    'energy_Wh_roll_mean_3', 'energy_Wh_roll_std_3', 'energy_Wh_diff_3',\n",
    "    'energy_Wh_roll_mean_5', 'energy_Wh_roll_std_5', 'energy_Wh_diff_5',\n",
    "    'energy_Wh_roll_mean_10', 'energy_Wh_roll_std_10', 'energy_Wh_diff_10',\n",
    "    'SOH_cycle_capacity_%_roll_mean_3', 'SOH_cycle_capacity_%_roll_std_3', 'SOH_cycle_capacity_%_diff_3',\n",
    "    'SOH_cycle_capacity_%_roll_mean_5', 'SOH_cycle_capacity_%_roll_std_5', 'SOH_cycle_capacity_%_diff_5',\n",
    "    'SOH_cycle_capacity_%_roll_mean_10', 'SOH_cycle_capacity_%_roll_std_10', 'SOH_cycle_capacity_%_diff_10'\n",
    "]\n",
    "MODELS_TO_TRAIN = ['rf', 'gb', 'xgb', 'lr']\n",
    "PARAM_GRIDS = { # Using Run 3 grids as an example\n",
    "    'rf': {'n_estimators': [50, 100, 150], 'max_depth': [10, 20, 25], 'min_samples_split': [5, 10]},\n",
    "    'gb': {'learning_rate': [0.05, 0.1], 'max_iter': [150, 200, 250], 'max_depth': [4, 5, 7]},\n",
    "    'xgb': {'n_estimators': [100, 150, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [4, 5, 7]},\n",
    "    'lr': {'fit_intercept': [True]}\n",
    "}\n",
    "CV_FOLDS_GRIDSEARCH = 3\n",
    "GRIDSEARCH_SCORING = 'neg_mean_squared_error'\n",
    "BATTERY_REGIME_MAP = {\n",
    "    'battery00': 'regular_constant', 'battery01': 'regular_constant', 'battery10': 'regular_constant',\n",
    "    'battery11': 'regular_constant', 'battery20': 'regular_constant', 'battery21': 'regular_constant',\n",
    "    'battery30': 'regular_constant', 'battery31': 'regular_constant', 'battery40': 'regular_constant',\n",
    "    'battery50': 'regular_constant', 'battery22': 'regular_variable', 'battery23': 'regular_variable',\n",
    "    'battery41': 'regular_variable', 'battery51': 'regular_variable', 'battery52': 'regular_variable',\n",
    "    'battery02': 'recommissioned_two_stage', 'battery12': 'recommissioned_two_stage',\n",
    "    'battery24': 'recommissioned_two_stage', 'battery32': 'recommissioned_two_stage',\n",
    "    'battery53': 'recommissioned_two_stage', 'battery03': 'recommissioned_three_stage',\n",
    "    'battery25': 'recommissioned_three_stage', 'battery33': 'recommissioned_three_stage',\n",
    "}\n",
    "SPLIT_RANDOM_SEED = 42 # For reproducibility of the initial dev/hold-out split & CV folds\n",
    "# --- End of assumed definitions ---\n",
    "\n",
    "# Parameters for the Robust Evaluation\n",
    "N_FINAL_HOLD_OUT_BATTERIES = 4  # Number of batteries for the sacred final hold-out set\n",
    "N_SPLITS_GROUPKFOLD = 5         # Number of folds for GroupKFold cross-validation\n",
    "\n",
    "BASE_OUTPUT_DIR_ROBUST = \"soh_robust_evaluation_outputs\"\n",
    "CV_EXPERIMENT_NAME = \"SOH_GroupKFold_CV\"\n",
    "CV_EXPERIMENT_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR_ROBUST, CV_EXPERIMENT_NAME)\n",
    "FINAL_MODEL_DIR = os.path.join(BASE_OUTPUT_DIR_ROBUST, \"Final_Champion_Model\")\n",
    "\n",
    "os.makedirs(CV_EXPERIMENT_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Robust evaluation base output directory: {BASE_OUTPUT_DIR_ROBUST}\")\n",
    "print(f\"Cross-validation outputs will be saved to: {CV_EXPERIMENT_OUTPUT_DIR}\")\n",
    "print(f\"Final champion model artifacts will be saved to: {FINAL_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74918f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell B: Preprocessing and Plotting Helper Functions ---\n",
    "\n",
    "# Placeholder: Assume load_data, preprocess_data, scale_features,\n",
    "# plot_actual_vs_predicted_soh, and plot_feature_importances are defined here\n",
    "# or imported correctly. For brevity, I'm not re-pasting them.\n",
    "# Make sure they are available in your notebook's execution context.\n",
    "\n",
    "# Example for preprocess_data (ensure it's suitable)\n",
    "def load_data(filepath):\n",
    "    if not os.path.exists(filepath): print(f\"Error: File not found at {filepath}\"); return None\n",
    "    try: df = pd.read_csv(filepath); print(f\"Data loaded successfully from {filepath}. Shape: {df.shape}\"); return df\n",
    "    except Exception as e: print(f\"Error loading data: {e}\"); return None\n",
    "\n",
    "def preprocess_data(df, target_col_name, current_extra_exclude_cols=None):\n",
    "    if df is None or df.empty: print(\"Error: Input DataFrame empty.\"); return None, None, None\n",
    "    df_cleaned = df.dropna(subset=[target_col_name]).copy()\n",
    "    if df_cleaned.empty: print(f\"Error: No data after dropping NaNs in {target_col_name}.\"); return None, None, None\n",
    "    y = df_cleaned[target_col_name]\n",
    "    base_excluded = ['start_time', 'cycle_number', 'battery_id', 'regime', target_col_name, 'SOH_cycle_capacity_%']\n",
    "    excluded_from_features = list(set(base_excluded + (current_extra_exclude_cols if current_extra_exclude_cols else [])))\n",
    "    potential_features = [col for col in df_cleaned.columns if col not in excluded_from_features]\n",
    "    temp_X_df = df_cleaned[potential_features].copy()\n",
    "    feature_cols_for_X = []\n",
    "    for col in temp_X_df.columns:\n",
    "        if temp_X_df[col].dtype == 'object':\n",
    "            try: temp_X_df[col] = pd.to_numeric(temp_X_df[col]); feature_cols_for_X.append(col)\n",
    "            except ValueError: print(f\"Warning: Skipping non-convertible object column '{col}'.\")\n",
    "        else: feature_cols_for_X.append(col)\n",
    "    feature_cols_for_X = sorted(list(set(feature_cols_for_X)))\n",
    "    X = temp_X_df[feature_cols_for_X].copy()\n",
    "    if X.empty or y.empty: print(\"Error: X or y empty after preprocessing.\"); return None, None, None\n",
    "    return X, y, feature_cols_for_X\n",
    "\n",
    "def scale_features(X_train, X_test, feature_names): # Simplified for CV fold (train/test within fold)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_names, index=X_train.index)\n",
    "    X_test_scaled = None\n",
    "    if X_test is not None and not X_test.empty:\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_names, index=X_test.index)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# --- Make sure your plot_actual_vs_predicted_soh and plot_feature_importances are defined here too ---\n",
    "def plot_actual_vs_predicted_soh(\n",
    "    df_test_eval, battery_id_to_plot, model_name_str, output_dir,\n",
    "    actual_soh_col='y_actual', predicted_soh_col='y_pred'\n",
    "):\n",
    "    battery_data = df_test_eval[df_test_eval['battery_id'] == battery_id_to_plot].sort_values('cycle_number')\n",
    "    if battery_data.empty: print(f\"No data for battery {battery_id_to_plot}. Skipping plot.\"); return\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(battery_data['cycle_number'], battery_data[actual_soh_col], label='Actual SOH', marker='o', linestyle='-')\n",
    "    plt.plot(battery_data['cycle_number'], battery_data[predicted_soh_col], label=f'Predicted SOH ({model_name_str})', marker='x', linestyle='--')\n",
    "    plt.title(f'Actual vs. Predicted SOH: {battery_id_to_plot} ({model_name_str})')\n",
    "    plt.xlabel('Cycle Number'); plt.ylabel('SOH (%)'); plt.legend(); plt.grid(True)\n",
    "    plot_filename = os.path.join(output_dir, f\"soh_comp_{battery_id_to_plot}_{model_name_str}.png\")\n",
    "    try: plt.savefig(plot_filename); # print(f\"  Saved SOH plot to {plot_filename}\")\n",
    "    except Exception as e: print(f\"  Error saving SOH plot {plot_filename}: {e}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importances(\n",
    "    importances_df, model_name_str, output_dir, top_n=15\n",
    "):\n",
    "    if importances_df is None or importances_df.empty: return\n",
    "    importances_df = importances_df.sort_values(by='importance', ascending=False).head(top_n)\n",
    "    plt.figure(figsize=(8, max(5, len(importances_df) * 0.3)))\n",
    "    sns.barplot(x='importance', y='feature', data=importances_df, hue='feature', palette='viridis', legend=False, dodge=False)\n",
    "    plt.title(f'Top {top_n} Feature Importances: {model_name_str}'); plt.xlabel('Importance'); plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(output_dir, f\"fi_{model_name_str}.png\")\n",
    "    try: plt.savefig(plot_filename); # print(f\"  Saved FI plot to {plot_filename}\")\n",
    "    except Exception as e: print(f\"  Error saving FI plot {plot_filename}: {e}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90e3f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from New_Features_Added_ALL.csv. Shape: (8220, 200)\n",
      "--- Data Splitting for Robust Evaluation ---\n",
      "Total unique batteries: 23\n",
      "Development Set: 19 batteries. Shape: (7488, 201)\n",
      "  Regimes: {'regular_constant': 8, 'recommissioned_two_stage': 4, 'regular_variable': 4, 'recommissioned_three_stage': 3}\n",
      "Final Hold-Out Test Set: 4 batteries. Shape: (732, 201)\n",
      "  Regimes: {'regular_constant': 2, 'recommissioned_two_stage': 1, 'regular_variable': 1}\n"
     ]
    }
   ],
   "source": [
    "# --- Cell C: Load Data and Create Final Hold-Out & Development Sets ---\n",
    "df_master_full = load_data(INPUT_FILE)\n",
    "\n",
    "if df_master_full is None:\n",
    "    raise ValueError(\"Failed to load master data.\")\n",
    "\n",
    "# Annotate with 'regime' if not already done (ensure BATTERY_REGIME_MAP is correct)\n",
    "if 'regime' not in df_master_full.columns:\n",
    "    df_master_full['regime'] = df_master_full['battery_id'].map(BATTERY_REGIME_MAP)\n",
    "\n",
    "# Ensure cycle_number is numeric\n",
    "df_master_full['cycle_number'] = pd.to_numeric(df_master_full['cycle_number'], errors='coerce')\n",
    "df_master_full.dropna(subset=['cycle_number', 'regime', TARGET_COL], inplace=True) # Drop vital NaNs\n",
    "df_master_full['cycle_number'] = df_master_full['cycle_number'].astype(int)\n",
    "\n",
    "\n",
    "# Get unique batteries and their regimes for stratified splitting\n",
    "unique_batteries_df = df_master_full[['battery_id', 'regime']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "if len(unique_batteries_df) < N_FINAL_HOLD_OUT_BATTERIES + N_SPLITS_GROUPKFOLD : # Basic check for enough batteries\n",
    "    raise ValueError(f\"Not enough unique batteries ({len(unique_batteries_df)}) for desired hold-out ({N_FINAL_HOLD_OUT_BATTERIES}) and {N_SPLITS_GROUPKFOLD} CV folds.\")\n",
    "\n",
    "# Stratified split of BATTERY IDs into development and final hold-out sets\n",
    "development_battery_ids_df, final_hold_out_battery_ids_df = train_test_split(\n",
    "    unique_batteries_df,\n",
    "    test_size=N_FINAL_HOLD_OUT_BATTERIES,\n",
    "    stratify=unique_batteries_df['regime'], # Stratify by regime\n",
    "    random_state=SPLIT_RANDOM_SEED\n",
    ")\n",
    "\n",
    "development_battery_ids = list(development_battery_ids_df['battery_id'])\n",
    "final_hold_out_battery_ids = list(final_hold_out_battery_ids_df['battery_id'])\n",
    "\n",
    "# Create the actual dataframes\n",
    "df_development = df_master_full[df_master_full['battery_id'].isin(development_battery_ids)].copy()\n",
    "df_final_hold_out_test = df_master_full[df_master_full['battery_id'].isin(final_hold_out_battery_ids)].copy()\n",
    "\n",
    "print(f\"--- Data Splitting for Robust Evaluation ---\")\n",
    "print(f\"Total unique batteries: {df_master_full['battery_id'].nunique()}\")\n",
    "print(f\"Development Set: {len(development_battery_ids)} batteries. Shape: {df_development.shape}\")\n",
    "print(f\"  Regimes: {development_battery_ids_df['regime'].value_counts().to_dict()}\")\n",
    "print(f\"Final Hold-Out Test Set: {len(final_hold_out_battery_ids)} batteries. Shape: {df_final_hold_out_test.shape}\")\n",
    "print(f\"  Regimes: {final_hold_out_battery_ids_df['regime'].value_counts().to_dict()}\")\n",
    "\n",
    "# Sanity check\n",
    "assert set(development_battery_ids).isdisjoint(set(final_hold_out_battery_ids)), \"Overlap between dev and hold-out!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32b729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell D: Define Experiment Runner for a Single CV Fold ---\n",
    "\n",
    "def run_experiment_for_cv_fold(\n",
    "    df_train_data_fold,       # Training data for this fold\n",
    "    df_test_data_fold,        # Test data for this fold (acting as validation for this fold)\n",
    "    target_variable_name,\n",
    "    extra_feature_exclusions,\n",
    "    models_to_run,\n",
    "    model_param_grids,\n",
    "    gridsearch_cv_folds,      # Inner CV for GridSearchCV\n",
    "    gridsearch_metric_name,\n",
    "    fold_output_dir,          # Directory to save artifacts for this specific fold\n",
    "    save_fold_artifacts=False # Control saving for each fold to save space if needed\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs experiment for a single fold of GroupKFold CV.\n",
    "    Hyperparameter tuning is done on df_train_data_fold.\n",
    "    Evaluation is done on df_test_data_fold.\n",
    "    \"\"\"\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Preprocess Data\n",
    "    X_train_raw, y_train, actual_feature_names = preprocess_data(\n",
    "        df_train_data_fold, target_variable_name, extra_feature_exclusions\n",
    "    )\n",
    "    X_test_raw, y_test, _ = preprocess_data(\n",
    "        df_test_data_fold, target_variable_name, extra_feature_exclusions\n",
    "    )\n",
    "\n",
    "    if X_train_raw is None or X_test_raw is None:\n",
    "        print(\"  Error: Preprocessing failed for this fold. Skipping.\")\n",
    "        return {model_code: {'test_rmse': np.nan, 'test_mae': np.nan, 'test_r2': np.nan, 'best_params': {}} for model_code in models_to_run}\n",
    "\n",
    "    # Store original indices if needed for detailed analysis or fetching other columns for plotting\n",
    "    train_indices = X_train_raw.index\n",
    "    test_indices = X_test_raw.index\n",
    "    \n",
    "    # 2. Impute Data\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=actual_feature_names, index=train_indices)\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test_raw), columns=actual_feature_names, index=test_indices)\n",
    "\n",
    "    # 3. Scale Data\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(\n",
    "        X_train_imputed, X_test_imputed, actual_feature_names\n",
    "    )\n",
    "\n",
    "    fold_model_results = {}\n",
    "\n",
    "    for model_code in models_to_run:\n",
    "        # print(f\"    Processing model: {model_code.upper()}\")\n",
    "        current_X_train_for_tuning = X_train_imputed\n",
    "        current_X_test_for_eval = X_test_imputed\n",
    "        \n",
    "        if model_code in ['lr', 'gb']: # gb is HistGradientBoostingRegressor\n",
    "            current_X_train_for_tuning = X_train_scaled\n",
    "            current_X_test_for_eval = X_test_scaled\n",
    "        \n",
    "        base_model = None\n",
    "        if model_code == 'rf': base_model = RandomForestRegressor(random_state=SPLIT_RANDOM_SEED, n_jobs=-1)\n",
    "        elif model_code == 'gb': base_model = HistGradientBoostingRegressor(random_state=SPLIT_RANDOM_SEED)\n",
    "        elif model_code == 'xgb': base_model = xgb.XGBRegressor(random_state=SPLIT_RANDOM_SEED, n_jobs=-1, verbosity=0)\n",
    "        elif model_code == 'lr': base_model = LinearRegression()\n",
    "        else: continue\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model, param_grid=model_param_grids[model_code],\n",
    "            scoring=gridsearch_metric_name, cv=gridsearch_cv_folds, verbose=0, n_jobs=-1 # verbose=0 for less output\n",
    "        )\n",
    "        grid_search.fit(current_X_train_for_tuning, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on the fold's test set\n",
    "        y_pred_test = best_model.predict(current_X_test_for_eval)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        fold_model_results[model_code] = {\n",
    "            'test_rmse': test_rmse, 'test_mae': test_mae, 'test_r2': test_r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "\n",
    "        if save_fold_artifacts:\n",
    "            joblib.dump(best_model, os.path.join(fold_output_dir, f\"model_{model_code}.joblib\"))\n",
    "            # Could save imputer, scaler too if needed for that fold specifically\n",
    "            \n",
    "    if save_fold_artifacts: # Save scaler & imputer once per fold if needed\n",
    "        joblib.dump(scaler, os.path.join(fold_output_dir, f\"scaler_fold.joblib\"))\n",
    "        joblib.dump(imputer, os.path.join(fold_output_dir, f\"imputer_fold.joblib\"))\n",
    "        joblib.dump(actual_feature_names, os.path.join(fold_output_dir, f\"feature_names_fold.joblib\"))\n",
    "\n",
    "\n",
    "    return fold_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bd5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m groups_for_cv = df_development[\u001b[33m'\u001b[39m\u001b[33mbattery_id\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Group by battery ID\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Check if number of splits is feasible\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m N_SPLITS_GROUPKFOLD > \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_development\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbattery_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mN_SPLITS_GROUPKFOLD (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_SPLITS_GROUPKFOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) cannot be greater than the number of unique batteries in the development set (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_development[\u001b[33m'\u001b[39m\u001b[33mbattery_id\u001b[39m\u001b[33m'\u001b[39m].nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m all_fold_results_list = [] \u001b[38;5;66;03m# To store detailed results from each model in each fold\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "# --- Cell E: Perform GroupKFold Cross-Validation ---\n",
    "group_kfold = GroupKFold(n_splits=N_SPLITS_GROUPKFOLD)\n",
    "groups_for_cv = df_development['battery_id'] # Group by battery ID\n",
    "\n",
    "# Check if number of splits is feasible\n",
    "if N_SPLITS_GROUPKFOLD > df_development['battery_id'].nunique():\n",
    "    raise ValueError(f\"N_SPLITS_GROUPKFOLD ({N_SPLITS_GROUPKFOLD}) cannot be greater than the number of unique batteries in the development set ({df_development['battery_id'].nunique()}).\")\n",
    "\n",
    "\n",
    "all_fold_results_list = [] # To store detailed results from each model in each fold\n",
    "\n",
    "print(f\"\\n--- Starting {N_SPLITS_GROUPKFOLD}-Fold Group Cross-Validation ---\")\n",
    "for fold_num, (train_dev_indices, test_dev_indices) in enumerate(group_kfold.split(df_development, y=df_development[TARGET_COL], groups=groups_for_cv)):\n",
    "    fold_experiment_tag = f\"Fold_{fold_num + 1}\"\n",
    "    current_fold_output_dir = os.path.join(CV_EXPERIMENT_OUTPUT_DIR, fold_experiment_tag)\n",
    "    print(f\"\\n--- {fold_experiment_tag}/{N_SPLITS_GROUPKFOLD} ---\")\n",
    "    \n",
    "    df_train_fold = df_development.iloc[train_dev_indices]\n",
    "    df_test_fold = df_development.iloc[test_dev_indices] # This is the test set for THIS FOLD\n",
    "\n",
    "    print(f\"  Train batteries ({len(df_train_fold['battery_id'].unique())}): {sorted(df_train_fold['battery_id'].unique())[:5]}...\")\n",
    "    print(f\"  Test batteries ({len(df_test_fold['battery_id'].unique())}): {sorted(df_test_fold['battery_id'].unique())}\")\n",
    "    \n",
    "    # Run experiment for the current fold\n",
    "    # Set save_fold_artifacts to True if you want to inspect individual fold models/scalers\n",
    "    results_this_fold = run_experiment_for_cv_fold(\n",
    "        df_train_data_fold=df_train_fold,\n",
    "        df_test_data_fold=df_test_fold,\n",
    "        target_variable_name=TARGET_COL,\n",
    "        extra_feature_exclusions=EXTRA_EXCLUDE_COLS_FROM_FEATURES,\n",
    "        models_to_run=MODELS_TO_TRAIN,\n",
    "        model_param_grids=PARAM_GRIDS,\n",
    "        gridsearch_cv_folds=CV_FOLDS_GRIDSEARCH,\n",
    "        gridsearch_metric_name=GRIDSEARCH_SCORING,\n",
    "        fold_output_dir=current_fold_output_dir,\n",
    "        save_fold_artifacts=False # Set to True to save models/scalers for each fold\n",
    "    )\n",
    "    \n",
    "    # Store results for each model in this fold\n",
    "    for model_name, metrics in results_this_fold.items():\n",
    "        all_fold_results_list.append({\n",
    "            'fold': fold_num + 1,\n",
    "            'model': model_name,\n",
    "            'test_rmse': metrics['test_rmse'],\n",
    "            'test_mae': metrics['test_mae'],\n",
    "            'test_r2': metrics['test_r2'],\n",
    "            'best_params': str(metrics['best_params']) # Store as string for DataFrame\n",
    "        })\n",
    "    print(f\"  Fold {fold_num+1} completed.\")\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "df_cv_results_detailed = pd.DataFrame(all_fold_results_list)\n",
    "\n",
    "print(\"\\n--- Cross-Validation Detailed Results (All Folds, All Models) ---\")\n",
    "print(df_cv_results_detailed)\n",
    "\n",
    "# Save detailed CV results\n",
    "cv_detailed_csv_path = os.path.join(CV_EXPERIMENT_OUTPUT_DIR, \"cv_detailed_results.csv\")\n",
    "df_cv_results_detailed.to_csv(cv_detailed_csv_path, index=False)\n",
    "print(f\"\\nDetailed CV results saved to {cv_detailed_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a653e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell F: Analyze Cross-Validation Results and Select Champion Model ---\n",
    "\n",
    "if df_cv_results_detailed.empty:\n",
    "    raise ValueError(\"CV results are empty. Cannot proceed with analysis.\")\n",
    "\n",
    "# Calculate mean and std dev of metrics for each model across folds\n",
    "aggregated_cv_metrics = df_cv_results_detailed.groupby('model').agg(\n",
    "    mean_test_rmse=('test_rmse', 'mean'),\n",
    "    std_test_rmse=('test_rmse', 'std'),\n",
    "    mean_test_mae=('test_mae', 'mean'),\n",
    "    std_test_mae=('test_mae', 'std'),\n",
    "    mean_test_r2=('test_r2', 'mean'),\n",
    "    std_test_r2=('test_r2', 'std')\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\n--- Aggregated Cross-Validation Metrics (Mean & Std Dev) ---\")\n",
    "# Sort by desired metric, e.g., mean_test_r2 descending or mean_test_rmse ascending\n",
    "aggregated_cv_metrics = aggregated_cv_metrics.sort_values(by='mean_test_r2', ascending=False)\n",
    "print(aggregated_cv_metrics)\n",
    "\n",
    "# Save aggregated CV results\n",
    "cv_aggregated_csv_path = os.path.join(CV_EXPERIMENT_OUTPUT_DIR, \"cv_aggregated_metrics.csv\")\n",
    "aggregated_cv_metrics.to_csv(cv_aggregated_csv_path, index=False)\n",
    "print(f\"\\nAggregated CV metrics saved to {cv_aggregated_csv_path}\")\n",
    "\n",
    "# --- Champion Model Selection Logic ---\n",
    "# Example: Select based on highest mean_test_r2.\n",
    "# You might also consider low std_test_r2 (stability) or other metrics.\n",
    "champion_model_series = aggregated_cv_metrics.iloc[0] # Assumes sorted by best R2\n",
    "champion_model_name = champion_model_series['model']\n",
    "print(f\"\\n--- Champion Model Selection ---\")\n",
    "print(f\"Based on CV results (highest mean R2), the champion model architecture is: {champion_model_name.upper()}\")\n",
    "print(f\"  Mean Test R2: {champion_model_series['mean_test_r2']:.4f} (Std: {champion_model_series['std_test_r2']:.4f})\")\n",
    "print(f\"  Mean Test RMSE: {champion_model_series['mean_test_rmse']:.4f} (Std: {champion_model_series['std_test_rmse']:.4f})\")\n",
    "\n",
    "# Get the best hyperparameters for the champion model\n",
    "# Option 1: Find the most frequent best_params for this model from CV folds\n",
    "champion_model_params_list = df_cv_results_detailed[\n",
    "    df_cv_results_detailed['model'] == champion_model_name\n",
    "]['best_params'].tolist()\n",
    "\n",
    "from collections import Counter\n",
    "most_common_params_str = Counter(champion_model_params_list).most_common(1)[0][0]\n",
    "# Convert string back to dict - CAREFUL with this if params are complex\n",
    "import ast\n",
    "try:\n",
    "    final_champion_hyperparameters = ast.literal_eval(most_common_params_str)\n",
    "    print(f\"Most common hyperparameters for {champion_model_name.upper()} from CV: {final_champion_hyperparameters}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not parse hyperparameters for {champion_model_name}, using default from PARAM_GRIDS. Error: {e}\")\n",
    "    final_champion_hyperparameters = PARAM_GRIDS[champion_model_name] # Fallback\n",
    "\n",
    "# Option 2 (Potentially Better for final model): Re-run GridSearchCV on the full development set\n",
    "# For simplicity here, we'll use the most common from CV.\n",
    "# If you want to re-run GridSearchCV:\n",
    "# print(f\"\\nOptional: For even better hyperparameters, re-run GridSearchCV for {champion_model_name.upper()} on the full df_development set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647cdf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell G: Train Final Champion Model and Evaluate on Hold-Out Set ---\n",
    "\n",
    "print(f\"\\n--- Training Final Champion Model: {champion_model_name.upper()} ---\")\n",
    "print(f\"Using hyperparameters: {final_champion_hyperparameters}\")\n",
    "\n",
    "# 1. Preprocess full development set and final hold-out test set\n",
    "X_dev_full_raw, y_dev_full, final_feature_names = preprocess_data(\n",
    "    df_development, TARGET_COL, EXTRA_EXCLUDE_COLS_FROM_FEATURES\n",
    ")\n",
    "X_hold_out_raw, y_hold_out, _ = preprocess_data(\n",
    "    df_final_hold_out_test, TARGET_COL, EXTRA_EXCLUDE_COLS_FROM_FEATURES\n",
    ")\n",
    "\n",
    "if X_dev_full_raw is None or X_hold_out_raw is None:\n",
    "    raise ValueError(\"Preprocessing failed for full development or hold-out set.\")\n",
    "\n",
    "# Impute\n",
    "final_imputer = SimpleImputer(strategy='median')\n",
    "X_dev_full_imputed = pd.DataFrame(final_imputer.fit_transform(X_dev_full_raw), columns=final_feature_names, index=X_dev_full_raw.index)\n",
    "X_hold_out_imputed = pd.DataFrame(final_imputer.transform(X_hold_out_raw), columns=final_feature_names, index=X_hold_out_raw.index)\n",
    "\n",
    "# Scale\n",
    "final_scaler = StandardScaler()\n",
    "X_dev_full_scaled = pd.DataFrame(final_scaler.fit_transform(X_dev_full_imputed), columns=final_feature_names, index=X_dev_full_imputed.index)\n",
    "X_hold_out_scaled = pd.DataFrame(final_scaler.transform(X_hold_out_imputed), columns=final_feature_names, index=X_hold_out_imputed.index)\n",
    "\n",
    "\n",
    "# Determine data for final model training\n",
    "X_train_final = X_dev_full_imputed\n",
    "X_test_final_eval = X_hold_out_imputed\n",
    "if champion_model_name in ['lr', 'gb']:\n",
    "    X_train_final = X_dev_full_scaled\n",
    "    X_test_final_eval = X_hold_out_scaled\n",
    "\n",
    "# Initialize and train the final model\n",
    "final_model = None\n",
    "if champion_model_name == 'rf':\n",
    "    final_model = RandomForestRegressor(random_state=SPLIT_RANDOM_SEED, n_jobs=-1, **final_champion_hyperparameters)\n",
    "elif champion_model_name == 'gb':\n",
    "    final_model = HistGradientBoostingRegressor(random_state=SPLIT_RANDOM_SEED, **final_champion_hyperparameters)\n",
    "elif champion_model_name == 'xgb':\n",
    "    final_model = xgb.XGBRegressor(random_state=SPLIT_RANDOM_SEED, n_jobs=-1, verbosity=0, **final_champion_hyperparameters)\n",
    "elif champion_model_name == 'lr':\n",
    "    final_model = LinearRegression(**final_champion_hyperparameters)\n",
    "\n",
    "if final_model is None:\n",
    "    raise ValueError(f\"Champion model {champion_model_name} could not be initialized.\")\n",
    "\n",
    "print(f\"Training final {champion_model_name.upper()} model on the full development set...\")\n",
    "final_model.fit(X_train_final, y_dev_full)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save the final model, scaler, and imputer\n",
    "joblib.dump(final_model, os.path.join(FINAL_MODEL_DIR, f\"champion_model_{champion_model_name}.joblib\"))\n",
    "joblib.dump(final_scaler, os.path.join(FINAL_MODEL_DIR, \"final_scaler.joblib\"))\n",
    "joblib.dump(final_imputer, os.path.join(FINAL_MODEL_DIR, \"final_imputer.joblib\"))\n",
    "joblib.dump(final_feature_names, os.path.join(FINAL_MODEL_DIR, \"final_feature_names.joblib\"))\n",
    "print(f\"Final champion model and transformers saved to {FINAL_MODEL_DIR}\")\n",
    "\n",
    "\n",
    "# Evaluate on the sacred Final Hold-Out Test Set\n",
    "print(f\"\\n--- Evaluating Champion Model on Final Hold-Out Test Set ---\")\n",
    "y_pred_hold_out = final_model.predict(X_test_final_eval)\n",
    "\n",
    "hold_out_rmse = np.sqrt(mean_squared_error(y_hold_out, y_pred_hold_out))\n",
    "hold_out_mae = mean_absolute_error(y_hold_out, y_pred_hold_out)\n",
    "hold_out_r2 = r2_score(y_hold_out, y_pred_hold_out)\n",
    "\n",
    "print(f\"Hold-Out Test Set Performance for {champion_model_name.upper()}:\")\n",
    "print(f\"  RMSE: {hold_out_rmse:.4f}\")\n",
    "print(f\"  MAE:  {hold_out_mae:.4f}\")\n",
    "print(f\"  R2:   {hold_out_r2:.4f}\")\n",
    "\n",
    "final_results_summary = {\n",
    "    'champion_model': champion_model_name,\n",
    "    'hyperparameters': str(final_champion_hyperparameters),\n",
    "    'hold_out_rmse': hold_out_rmse,\n",
    "    'hold_out_mae': hold_out_mae,\n",
    "    'hold_out_r2': hold_out_r2\n",
    "}\n",
    "df_final_summary = pd.DataFrame([final_results_summary])\n",
    "final_summary_csv_path = os.path.join(FINAL_MODEL_DIR, \"final_hold_out_results_summary.csv\")\n",
    "df_final_summary.to_csv(final_summary_csv_path, index=False)\n",
    "print(f\"Final hold-out results summary saved to {final_summary_csv_path}\")\n",
    "\n",
    "\n",
    "# Generate and save plots for the hold-out set\n",
    "# Feature Importance Plot\n",
    "fi_df_final = None\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    fi_df_final = pd.DataFrame({'feature': final_feature_names, 'importance': final_model.feature_importances_})\n",
    "elif champion_model_name == 'lr' and hasattr(final_model, 'coef_'):\n",
    "    fi_df_final = pd.DataFrame({'feature': final_feature_names, 'importance': np.abs(final_model.coef_)})\n",
    "\n",
    "if fi_df_final is not None:\n",
    "    plot_feature_importances(fi_df_final, f\"champion_{champion_model_name}_hold_out\", FINAL_MODEL_DIR)\n",
    "\n",
    "# SOH Actual vs. Predicted Plots for hold-out batteries\n",
    "df_hold_out_eval_plot = pd.DataFrame({\n",
    "    'battery_id': df_final_hold_out_test.loc[X_hold_out_raw.index, 'battery_id'].values,\n",
    "    'cycle_number': df_final_hold_out_test.loc[X_hold_out_raw.index, 'cycle_number'].values,\n",
    "    'y_actual': y_hold_out.values,\n",
    "    'y_pred': y_pred_hold_out\n",
    "})\n",
    "\n",
    "plots_hold_out_dir = os.path.join(FINAL_MODEL_DIR, \"hold_out_SOH_plots\")\n",
    "os.makedirs(plots_hold_out_dir, exist_ok=True)\n",
    "for batt_id in df_hold_out_eval_plot['battery_id'].unique():\n",
    "    plot_actual_vs_predicted_soh(\n",
    "        df_hold_out_eval_plot, batt_id, f\"champion_{champion_model_name}\", plots_hold_out_dir\n",
    "    )\n",
    "print(f\"SOH comparison plots for hold-out set saved in {plots_hold_out_dir}\")\n",
    "\n",
    "print(\"\\n--- Robust Evaluation Process Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
